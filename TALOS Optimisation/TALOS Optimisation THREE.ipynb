{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS Optimization - THREE\n",
    "\n",
    "An implementation of a hyperparameter grid search using the Talos library.\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check what hardware is available to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11415179865650964799\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12534632907620367887\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5031, 16)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    keep_col = list(range(1,18))\n",
    "\n",
    "    df_day = pd.read_csv('ECL_Clean_Day.csv', \n",
    "                     infer_datetime_format=True,\n",
    "                     parse_dates=['Timestamp'], \n",
    "                     index_col=['Timestamp'],\n",
    "                     usecols = keep_col,\n",
    "                     date_parser=lambda col: pd.to_datetime(col, utc=True).tz_convert('America/New_York'))\n",
    "\n",
    "    #df_min.dtypes\n",
    "    print (df_day.shape)\n",
    "    df_day.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 10 years of data from 2019 - 2010\n",
    "day_data = df_day[df_day.index >= '2010-01-01']\n",
    "\n",
    "#split 7 years train, 3 years test\n",
    "day_train = day_data[day_data.index <= '2017-01-01']\n",
    "day_test = day_data[day_data.index >= '2017-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = day_train['Close'] \n",
    "base_test = day_test['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(base_test)\n",
    "plt.plot(base_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Simple Vanilla Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1703, 30) (1703, 30) (695, 30) (695, 30)\n"
     ]
    }
   ],
   "source": [
    "# Multi-step data preparation\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# define input sequence\n",
    "#raw_seq = list(base_test[:100].values)\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 30, 30\n",
    "# split into samples\n",
    "X_train, y_train = split_sequence(base_train.values, n_steps_in, n_steps_out)\n",
    "X_test, y_test = split_sequence(base_test.values, n_steps_in, n_steps_out) #length must be > n_in + n_out\n",
    "# summarize the data\n",
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 43,830\n",
      "Trainable params: 43,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/jacobscottanthony/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1703 samples, validate on 695 samples\n",
      "Epoch 1/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 2059.3333 - val_loss: 412.4544\n",
      "Epoch 2/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 72.2292 - val_loss: 240.3671\n",
      "Epoch 3/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 42.3224 - val_loss: 220.4168\n",
      "Epoch 4/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 34.5715 - val_loss: 121.0543\n",
      "Epoch 5/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 34.7117 - val_loss: 121.7916\n",
      "Epoch 6/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 29.4226 - val_loss: 101.4682\n",
      "Epoch 7/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 30.6219 - val_loss: 71.3446\n",
      "Epoch 8/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 27.0317 - val_loss: 61.5482\n",
      "Epoch 9/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 27.7336 - val_loss: 133.8741\n",
      "Epoch 10/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 26.6426 - val_loss: 83.1163\n",
      "Epoch 11/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 25.0463 - val_loss: 84.3925\n",
      "Epoch 12/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 24.4943 - val_loss: 58.2418\n",
      "Epoch 13/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 24.3563 - val_loss: 61.9682\n",
      "Epoch 14/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 24.8976 - val_loss: 169.0269\n",
      "Epoch 15/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 25.7642 - val_loss: 171.0588\n",
      "Epoch 16/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 25.1862 - val_loss: 71.6080\n",
      "Epoch 17/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 29.8813 - val_loss: 110.8721\n",
      "Epoch 18/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 25.8632 - val_loss: 190.1743\n",
      "Epoch 19/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 27.4435 - val_loss: 97.4763\n",
      "Epoch 20/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 30.4234 - val_loss: 77.4048\n"
     ]
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "#print (X_train.shape)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "#print (X_test.shape)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# fit model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for grid search\n",
    "\n",
    "p = {'LSTM_n' : [100, 200, 400],\n",
    "    'LSTM_dropout' : [0, 0.1, 0.2],\n",
    "    'batch_size' : [30, 60, 90, 120],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train, y_train, X_test, y_test, params):\n",
    "    model = Sequential()                            \n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu'))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='Adam', \n",
    "                  loss='mse')\n",
    "    print (model.summary())\n",
    "    \n",
    "    #Early Stopping to avoid wasting time on bad hyperparams\n",
    "    es = EarlyStopping(monitor='val_loss', mode='auto', patience=20)\n",
    "\n",
    "    out = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    callbacks=[es],\n",
    "                    validation_data=[X_test, y_test])\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 200, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 1,771,630\n",
      "Trainable params: 1,771,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 12650.4689 - val_loss: 14680.2042\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 16613.1822 - val_loss: 11748.3710\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 84858.8278 - val_loss: 899150.7259\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 714751.5267 - val_loss: 769083.2312\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1321452.0606 - val_loss: 1213085.3816\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 825553.2490 - val_loss: 655777.5194\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1132036.5540 - val_loss: 48791.3177\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1834829.3815 - val_loss: 1377850.0651\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 120891.2090 - val_loss: 8023.1740\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7999.5252 - val_loss: 8022.7017\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7999.0378 - val_loss: 8022.1969\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7998.5210 - val_loss: 8021.6686\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7997.9808 - val_loss: 8021.1146\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7997.4150 - val_loss: 8020.5330\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7996.8237 - val_loss: 8019.9376\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7996.2195 - val_loss: 8019.3157\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7995.5854 - val_loss: 8018.6714\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7994.9340 - val_loss: 8018.0045\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7994.2581 - val_loss: 8017.3216\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7993.5657 - val_loss: 8016.6210\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7992.8547 - val_loss: 8015.8977\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7992.1204 - val_loss: 8015.1523\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7991.3661 - val_loss: 8014.3877\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7990.5949 - val_loss: 8013.6055\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7989.8063 - val_loss: 8012.8046\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7988.9932 - val_loss: 8011.9866\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7988.1510 - val_loss: 8011.0830\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7990.1661 - val_loss: 8010.3340\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 8115.6052 - val_loss: 8009.6382\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7985.8194 - val_loss: 8008.8057\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7984.9681 - val_loss: 8007.9316\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 7984.0864 - val_loss: 8007.0390\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7983.1919 - val_loss: 8006.1328\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7982.2761 - val_loss: 8005.2161\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7981.3509 - val_loss: 8004.2821\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7980.4106 - val_loss: 8003.3341\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7979.4505 - val_loss: 8002.3683\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7978.4772 - val_loss: 8001.3852\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7977.4887 - val_loss: 8000.3890\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 7976.4906 - val_loss: 7999.3791\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7975.4695 - val_loss: 7998.3520\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7974.4295 - val_loss: 7997.3036\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7973.3712 - val_loss: 7996.2375\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7972.2990 - val_loss: 7995.1496\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7971.2054 - val_loss: 7994.0474\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7970.0912 - val_loss: 7992.9292\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7968.9603 - val_loss: 7991.7776\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7967.7945 - val_loss: 7990.5942\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7966.5970 - val_loss: 7989.3834\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7965.3684 - val_loss: 7988.1238\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7964.0845 - val_loss: 7986.8091\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7962.7323 - val_loss: 7985.4029\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7961.2754 - val_loss: 7983.8685\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7959.6528 - val_loss: 7982.0737\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7957.6405 - val_loss: 7979.6902\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7956.6149 - val_loss: 7980.5473\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7957.0607 - val_loss: 7980.1391\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7956.1771 - val_loss: 7978.9897\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7954.9646 - val_loss: 7977.7381\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7953.6983 - val_loss: 7976.4539\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7952.4006 - val_loss: 7975.1480\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7951.0899 - val_loss: 7973.8216\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 7949.7531 - val_loss: 7972.4763\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7948.4006 - val_loss: 7971.1118\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7947.0245 - val_loss: 7969.7301\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7945.6317 - val_loss: 7968.3215\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7944.2141 - val_loss: 7966.8885\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7942.7750 - val_loss: 7965.4415\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7941.3129 - val_loss: 7963.9644\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7939.8232 - val_loss: 7962.4593\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7938.3036 - val_loss: 7960.9261\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7936.7596 - val_loss: 7959.3622\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7935.1782 - val_loss: 7957.7654\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7933.5571 - val_loss: 7956.1178\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 7931.8937 - val_loss: 7954.4236\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7930.1701 - val_loss: 7952.6651\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7928.3820 - val_loss: 7950.8398\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7926.5157 - val_loss: 7948.9294\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7924.5598 - val_loss: 7946.9005\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7922.4689 - val_loss: 7944.7474\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7920.2148 - val_loss: 7942.3439\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7917.5926 - val_loss: 7939.1599\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7473.9323 - val_loss: 6933.2853\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7710.8288 - val_loss: 7934.3486\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7909.2136 - val_loss: 7930.4305\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7904.7437 - val_loss: 7925.0582\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7898.1572 - val_loss: 7916.3194\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7885.3983 - val_loss: 7894.2476\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7809.7326 - val_loss: 6593.2958\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7810.7046 - val_loss: 7936.8366\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7912.6819 - val_loss: 7935.3321\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7911.1672 - val_loss: 7933.8253\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7909.6599 - val_loss: 7932.3131\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7908.1433 - val_loss: 7930.7859\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7906.6111 - val_loss: 7929.2524\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7905.0771 - val_loss: 7927.7153\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7903.5254 - val_loss: 7926.1695\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7901.9807 - val_loss: 7924.6052\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7900.4180 - val_loss: 7923.0473\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7898.8523 - val_loss: 7921.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|         | 1/18 [10:42<3:02:09, 642.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 200, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 1,771,630\n",
      "Trainable params: 1,771,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 17628.0854 - val_loss: 307470.6438\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 215312.4577 - val_loss: 377072.0620\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 111949.5489 - val_loss: 1032922.1403\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 75514.6394 - val_loss: 8018.6669\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 20937.8833 - val_loss: 8012.9541\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 11955.3707 - val_loss: 8006.9873\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 22347.1931 - val_loss: 8001.0665\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 23388.7731 - val_loss: 7994.9832\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 19005.3733 - val_loss: 7988.7733\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 13060.3561 - val_loss: 7982.4993\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 11327.7635 - val_loss: 7976.2417\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 40957.1779 - val_loss: 7970.2375\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 9737.9473 - val_loss: 8079.6765\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 10027.2437 - val_loss: 7957.4553\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 10239.8925 - val_loss: 7951.0560\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 8648.3470 - val_loss: 7944.6156\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 9165.0089 - val_loss: 7938.1841\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8932.4194 - val_loss: 7931.7466\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8076.4315 - val_loss: 7925.2631\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8249.5875 - val_loss: 7918.7456\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 11659.2939 - val_loss: 7912.2008\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8129.4026 - val_loss: 7905.7117\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 10165.2542 - val_loss: 7899.2132\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 10902.8543 - val_loss: 7892.6829\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7974.1975 - val_loss: 7886.1351\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8307.3838 - val_loss: 7879.5846\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7884.0445 - val_loss: 7873.0638\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8054.7669 - val_loss: 7866.4866\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8246.0890 - val_loss: 7859.9485\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 8382.0951 - val_loss: 7853.4274\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 7919.4313 - val_loss: 7846.8617\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7924.3459 - val_loss: 7840.2891\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7927.0360 - val_loss: 7833.7220\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7972.8932 - val_loss: 7827.1453\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7796.3525 - val_loss: 7820.5645\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7869.7305 - val_loss: 7814.0079\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7792.2062 - val_loss: 7807.4313\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7925.7847 - val_loss: 7800.8818\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7866.8553 - val_loss: 7794.3162\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7767.2003 - val_loss: 7787.7649\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7759.3402 - val_loss: 7781.1952\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7778.4053 - val_loss: 7774.6298\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 7964.0009 - val_loss: 7768.0492\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7735.4057 - val_loss: 7761.4877\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7732.4520 - val_loss: 7754.9077\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8045.1518 - val_loss: 7748.3552\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7994.6800 - val_loss: 7741.8113\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7703.6811 - val_loss: 7735.2329\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7827.3641 - val_loss: 7728.6464\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7950.3289 - val_loss: 7722.1652\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7688.5033 - val_loss: 7715.6129\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 7684.5422 - val_loss: 7709.0292\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 8529.0820 - val_loss: 7702.4736\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 14s 12ms/step - loss: 26237.1305 - val_loss: 7696.0375\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 25753.1415 - val_loss: 7689.5636\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 23976.6359 - val_loss: 7683.1321\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 9380.3711 - val_loss: 7676.6140\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 11844.1911 - val_loss: 7670.0997\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 7441.4535 - val_loss: 7291.3012\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 16581.3006 - val_loss: 7637.9507\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 7281.3631 - val_loss: 3953.8612\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 3319.5119 - val_loss: 915.9646\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 1982.0053 - val_loss: 545.5284\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 2002.4467 - val_loss: 286.4225\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 2189.3783 - val_loss: 1372.2615\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 1950.4643 - val_loss: 277.0698\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 2185.7829 - val_loss: 341.3248\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 1905.3595 - val_loss: 1500.4248\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1960.1969 - val_loss: 229.9699\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1813.3012 - val_loss: 340.4072\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1898.1921 - val_loss: 1372.9457\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 3713.4388 - val_loss: 186.7046\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 5226.7725 - val_loss: 2373.7393\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 3777.2937 - val_loss: 795.4607\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 4424.2433 - val_loss: 6811.9616\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 4399.0142 - val_loss: 7345.3109\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 7128.0679 - val_loss: 1604.0697\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 2168.8330 - val_loss: 1811.3818\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1880.8508 - val_loss: 496.0645\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 4507.3276 - val_loss: 348.8724\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1993.3356 - val_loss: 144.9957\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 1809.4546 - val_loss: 63.7861\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1880.8187 - val_loss: 31.4715\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2101.2193 - val_loss: 947.0254\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1847.4872 - val_loss: 61.2714\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 1753.7703 - val_loss: 54.6346\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1743.3299 - val_loss: 42.2202\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1934.5708 - val_loss: 85.0430\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1875.0165 - val_loss: 276.1498\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 1810.7689 - val_loss: 49.6670\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 1960.7012 - val_loss: 330.3101\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1658.0798 - val_loss: 981.8235\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1740.3951 - val_loss: 312.8482\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 2147.5274 - val_loss: 556.3061\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1742.3590 - val_loss: 231.1584\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1739.0799 - val_loss: 293.1232\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1922.1903 - val_loss: 375.1294\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1824.0025 - val_loss: 483.8455\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1555.3035 - val_loss: 438.7755\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 15056.6293 - val_loss: 33300.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|         | 2/18 [34:55<3:56:10, 885.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 100, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 445,830\n",
      "Trainable params: 445,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 11s 10ms/step - loss: 5994.9707 - val_loss: 2593.1916\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 3851.8593 - val_loss: 2878.5866\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 3074.9514 - val_loss: 5646.8906\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 5305.9181 - val_loss: 9249.3344\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 8064.5171 - val_loss: 4941.9725\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 4134.3429 - val_loss: 3982.5722\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 2647.8386 - val_loss: 2009.3256\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1964.9852 - val_loss: 866.7414\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1461.0215 - val_loss: 819.4425\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1158.2822 - val_loss: 617.2221\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 957.3748 - val_loss: 593.9516\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 836.6760 - val_loss: 412.1364\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 760.3553 - val_loss: 481.9196\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 687.5289 - val_loss: 347.2771\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 659.1374 - val_loss: 456.0782\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 663.8653 - val_loss: 299.3244\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 675.8124 - val_loss: 955.1909\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 608.8313 - val_loss: 114.0288\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 589.1155 - val_loss: 413.1824\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 515.6081 - val_loss: 208.3261\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 502.8022 - val_loss: 149.1099\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 539.1329 - val_loss: 225.1140\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 497.6345 - val_loss: 181.2551\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 425.5726 - val_loss: 139.7801\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 417.0998 - val_loss: 123.1604\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 390.6474 - val_loss: 101.0542\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 392.6022 - val_loss: 66.1283\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 430.8264 - val_loss: 211.2298\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 503.0159 - val_loss: 220.9392\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 455.1997 - val_loss: 75.2695\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 563.7325 - val_loss: 98.3783\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 455.8982 - val_loss: 315.7796\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 363.4596 - val_loss: 46.5166\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 313.8059 - val_loss: 44.3188\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 350.5715 - val_loss: 124.5991\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 323.6548 - val_loss: 95.4989\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 374.8927 - val_loss: 122.2883\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 360.0325 - val_loss: 95.7411\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 338.5821 - val_loss: 71.0887\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 326.6098 - val_loss: 61.6364\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 334.6292 - val_loss: 78.4061\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 324.7871 - val_loss: 62.7903\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 325.6594 - val_loss: 113.7214\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 325.1502 - val_loss: 137.1674\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 300.3929 - val_loss: 442.5752\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 333.3077 - val_loss: 403.1540\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 392.6005 - val_loss: 295.3313\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 339.8130 - val_loss: 320.7771\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 290.4944 - val_loss: 171.1373\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 295.9811 - val_loss: 574.7920\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 291.7408 - val_loss: 394.1370\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 271.4398 - val_loss: 222.7254\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 268.6152 - val_loss: 165.3123\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 261.6709 - val_loss: 256.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|        | 3/18 [40:55<3:02:01, 728.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 100, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 445,830\n",
      "Trainable params: 445,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6205.5216 - val_loss: 3373.6053\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2944.1169 - val_loss: 2371.5484\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2190.8010 - val_loss: 2204.7313\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2057.7291 - val_loss: 2135.4788\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1909.7099 - val_loss: 2310.9374\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 3291.9974 - val_loss: 8880.7079\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5667.1709 - val_loss: 8263.9033\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7970.6720 - val_loss: 6695.5903\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 4422.6078 - val_loss: 2800.8983\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 3116.5999 - val_loss: 6852.4649\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4776.3420 - val_loss: 5381.0047\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6188.3865 - val_loss: 3423.1901\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 61162.2006 - val_loss: 15548.2525\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 101070.9565 - val_loss: 118739.1222\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 90917.8769 - val_loss: 8009.3305\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24840.7978 - val_loss: 8006.2215\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29061.6837 - val_loss: 8002.8036\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29769.4872 - val_loss: 7999.4096\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 13337.3119 - val_loss: 7996.0789\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 9650.1835 - val_loss: 7992.8966\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 8216.1497 - val_loss: 7989.7983\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 10892.6947 - val_loss: 7986.7117\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 9160.4135 - val_loss: 7983.6760\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 9586.6559 - val_loss: 7980.6937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|       | 4/18 [43:20<2:09:05, 553.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 400, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 7,063,230\n",
      "Trainable params: 7,063,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 23s 19ms/step - loss: 8622.7835 - val_loss: 116856.5610\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 1042252.9183 - val_loss: 21879302.1370\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 28733121.1007 - val_loss: 4197099.5646\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 33351600.6242 - val_loss: 8187.1199\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 3883004.5215 - val_loss: 8024.9942\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 899016.2603 - val_loss: 8024.8066\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 135245.7983 - val_loss: 8024.6139\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 342789.4675 - val_loss: 8024.3964\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 146991.2717 - val_loss: 8024.1902\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 45091.7545 - val_loss: 8023.9633\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 36552.0940 - val_loss: 8023.7164\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 119824.4355 - val_loss: 8023.4727\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 16201.3343 - val_loss: 8023.1975\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 64487.6201 - val_loss: 8022.9461\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 39116.8115 - val_loss: 8022.6716\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 25393.1875 - val_loss: 8022.3843\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 127869.0616 - val_loss: 8022.1323\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 54148.4286 - val_loss: 8021.8226\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 18739.4762 - val_loss: 8021.5156\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 39464.3361 - val_loss: 8021.1744\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 180390.5505 - val_loss: 8020.8481\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 10304.3253 - val_loss: 8020.5394\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 46452.4586 - val_loss: 8020.1589\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 11972.1893 - val_loss: 8019.8137\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 16986.2639 - val_loss: 8019.4476\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 54114.0067 - val_loss: 8018.9946\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 30733.7590 - val_loss: 8018.5945\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 21506.5700 - val_loss: 8018.1923\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 24178.0880 - val_loss: 8017.7764\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 8264.3626 - val_loss: 8017.3510\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 10572.2245 - val_loss: 8016.9175\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 8172.5160 - val_loss: 8016.4707\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 9371.3396 - val_loss: 8016.0194\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 9446.1616 - val_loss: 8015.5511\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 8827.0393 - val_loss: 8015.0767\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 9706.9788 - val_loss: 8014.5952\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 8267.4654 - val_loss: 8014.0997\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 16176.9052 - val_loss: 8013.6018\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 78274.1612 - val_loss: 8013.2050\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 23836.2119 - val_loss: 8012.7378\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 10973.1154 - val_loss: 8012.2168\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 11123.2110 - val_loss: 8011.6780\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 8786.3289 - val_loss: 8011.1229\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 19s 16ms/step - loss: 7987.3931 - val_loss: 8010.5456\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 15217.3515 - val_loss: 8009.9415\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 72301.5778 - val_loss: 8009.3181\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 11142.8622 - val_loss: 8008.6517\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 10412.2501 - val_loss: 8007.9573\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 17s 15ms/step - loss: 1024282.7511 - val_loss: 8007.2119\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 17s 15ms/step - loss: 9200.6778 - val_loss: 8006.4059\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 17s 15ms/step - loss: 10003.2360 - val_loss: 8005.9224\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 9075.1377 - val_loss: 8005.3444\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 412987.7427 - val_loss: 8005.0099\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 17s 15ms/step - loss: 8415.0154 - val_loss: 8004.8019\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 21s 18ms/step - loss: 8012.2484 - val_loss: 8004.2567\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 19s 16ms/step - loss: 10836.1362 - val_loss: 8003.6576\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 8050.6451 - val_loss: 8003.0405\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 17030.8641 - val_loss: 8002.4216\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 7978.8150 - val_loss: 8001.7883\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 70812.0142 - val_loss: 8001.1470\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 18424.7418 - val_loss: 8000.4971\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 10843.9154 - val_loss: 7999.8387\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 7985.4418 - val_loss: 7999.1735\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 7976.6684 - val_loss: 7998.5012\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 7974.7393 - val_loss: 7997.8236\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 22s 18ms/step - loss: 7974.0886 - val_loss: 7997.1334\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 7973.3543 - val_loss: 7996.4394\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 7989.1817 - val_loss: 7995.7352\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 8670.5291 - val_loss: 7995.0212\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 64597.0077 - val_loss: 7994.2975\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 7970.8997 - val_loss: 7993.5665\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 7991.3870 - val_loss: 7992.8312\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 7969.1717 - val_loss: 7992.0886\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 7968.2261 - val_loss: 7991.3323\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 20510.7161 - val_loss: 7990.5774\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 7966.9098 - val_loss: 7989.8128\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 9832.6404 - val_loss: 7989.0314\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 11128.3924 - val_loss: 7988.2383\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 11233.3582 - val_loss: 7987.4289\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 7963.5614 - val_loss: 7986.6118\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 7981.1966 - val_loss: 7985.7873\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 20s 17ms/step - loss: 8603.3503 - val_loss: 7984.9392\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 20s 16ms/step - loss: 12314.4223 - val_loss: 7984.0933\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 27452.0488 - val_loss: 7983.2301\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 9274.0597 - val_loss: 7982.3429\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 13209.5154 - val_loss: 7981.4173\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 7957.4842 - val_loss: 7980.4599\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 2326659467681114.5000 - val_loss: 84316178263382.6719\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 2836156387073.6045 - val_loss: 7982.0685\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 1221043.0253 - val_loss: 7982.1112\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 358727.0519 - val_loss: 7982.1259\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 61680.6812 - val_loss: 7982.1289\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 1255996.5319 - val_loss: 7982.1292\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 17743.9115 - val_loss: 7982.1292\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 7965.1685 - val_loss: 7982.1292\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 7979.4515 - val_loss: 7982.1292\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 8402.3833 - val_loss: 7982.1291\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 33792.5591 - val_loss: 7982.1301\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 8037.4395 - val_loss: 7982.1300\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 12319.7058 - val_loss: 7982.1311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|       | 5/18 [1:11:50<3:15:03, 900.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 100, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 445,830\n",
      "Trainable params: 445,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7607.4621 - val_loss: 7641.8199\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 17325.9739 - val_loss: 103348.2651\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1581518.1647 - val_loss: 1585982.2498\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4907133.6946 - val_loss: 1530080.4464\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2537397.6192 - val_loss: 216038.8624\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 715436.4170 - val_loss: 565375.8467\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 235542.0739 - val_loss: 392891.9881\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 500938.9186 - val_loss: 74569.3796\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 144309.5582 - val_loss: 9659.0337\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 32327.6212 - val_loss: 8004.3632\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7980.3801 - val_loss: 8003.1667\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7979.1254 - val_loss: 8002.0443\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7977.9517 - val_loss: 8000.8640\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7976.7240 - val_loss: 7999.6262\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7975.4271 - val_loss: 7998.3189\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7974.0398 - val_loss: 7996.8678\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7972.4090 - val_loss: 7994.9677\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7969.6021 - val_loss: 7990.4078\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7949.3211 - val_loss: 7805.7888\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2739965.2801 - val_loss: 8410.7773\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 8507.5037 - val_loss: 7992.6261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|      | 6/18 [1:13:26<2:11:47, 658.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 200, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 1,771,630\n",
      "Trainable params: 1,771,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 5694.5865 - val_loss: 4124.1557\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 4823.2161 - val_loss: 4279.1058\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3280.0107 - val_loss: 3921.1935\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 21378.0787 - val_loss: 39382.6926\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 33311.2820 - val_loss: 24832.2096\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 185381.1779 - val_loss: 1266997.6717\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1465126.2743 - val_loss: 8181729.5616\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 11958464.4908 - val_loss: 77695544.4384\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 33077556.5235 - val_loss: 2456916.4198\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 317971.6865 - val_loss: 7994.7073\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 415344.8854 - val_loss: 88611.7721\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35571.4407 - val_loss: 7958.8547\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7937.5070 - val_loss: 7963.0152\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7968.9190 - val_loss: 8010.6778\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7959.3186 - val_loss: 7920.2001\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7892.0072 - val_loss: 7907.5040\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7880.3732 - val_loss: 7895.9635\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 90535.7002 - val_loss: 666583.1601\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 125395716.2688 - val_loss: 63530421.4638\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 11453025.8446 - val_loss: 7983.4293\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 28783682.3129 - val_loss: 140390344.2975\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 19318288.3890 - val_loss: 8014.8787\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7996.1829 - val_loss: 8022.1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|      | 7/18 [1:15:17<1:30:39, 494.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 200, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 1,771,630\n",
      "Trainable params: 1,771,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 43507.7485 - val_loss: 62184.7983\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 8583312.7378 - val_loss: 9060557.9550\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 19821406.8278 - val_loss: 4568052.4308\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2372484.5393 - val_loss: 8022.3967\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7998.6766 - val_loss: 8021.8399\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7998.1392 - val_loss: 8021.2943\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7997.5648 - val_loss: 8020.6900\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7996.9342 - val_loss: 8020.0341\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7996.2492 - val_loss: 8019.3238\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7995.5078 - val_loss: 8018.5448\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7994.7016 - val_loss: 8017.7154\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7993.8400 - val_loss: 8016.8208\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7992.9163 - val_loss: 8015.8705\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7991.9341 - val_loss: 8014.8534\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7990.8847 - val_loss: 8013.7702\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7989.7647 - val_loss: 8012.6102\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7988.5638 - val_loss: 8011.3665\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7987.2684 - val_loss: 8010.0165\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7985.8571 - val_loss: 8008.5456\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 7984.3071 - val_loss: 8006.9095\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7981.8215 - val_loss: 8005.2295\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 7980.8994 - val_loss: 8003.3444\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7978.8549 - val_loss: 8001.1320\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7976.5035 - val_loss: 7998.5876\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7973.7300 - val_loss: 7995.5007\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 7970.0089 - val_loss: 7990.0287\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 189478.2909 - val_loss: 7980.9081\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 14121.3917 - val_loss: 7986.6868\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 43669.6886 - val_loss: 7994.2997\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7969.9184 - val_loss: 7992.0810\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7967.5289 - val_loss: 7989.5920\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7964.9852 - val_loss: 7986.9746\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7962.3174 - val_loss: 7984.2156\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7959.4968 - val_loss: 7981.3217\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7956.5121 - val_loss: 7978.2254\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7953.3130 - val_loss: 7974.8826\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7949.8296 - val_loss: 7971.2553\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7946.0432 - val_loss: 7967.2172\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7941.8167 - val_loss: 7962.6910\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7936.8703 - val_loss: 7957.0607\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7929.9558 - val_loss: 7947.9107\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 332221895.7931 - val_loss: 15561.6971\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7748270.0753 - val_loss: 7985.4280\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7961.8462 - val_loss: 7984.9785\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7961.1474 - val_loss: 7984.1670\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7960.1087 - val_loss: 7982.7165\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 10565.9279 - val_loss: 7983.5814\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7959.8802 - val_loss: 7983.0491\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7959.2997 - val_loss: 7982.4419\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7958.6682 - val_loss: 7981.7830\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 34841.5374 - val_loss: 7981.0477\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7957.0905 - val_loss: 7980.1149\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7956.2204 - val_loss: 7979.2238\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7955.2690 - val_loss: 7978.2009\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7954.1591 - val_loss: 7976.9826\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7952.8069 - val_loss: 7975.4739\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 7951.0634 - val_loss: 7973.4135\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7948.5434 - val_loss: 7970.2389\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7944.1191 - val_loss: 7963.7680\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 7927.3105 - val_loss: 7865.1274\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2108635.1167 - val_loss: 7978.4241\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 31189.6078 - val_loss: 7978.2899\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7954.5373 - val_loss: 7977.6863\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7953.9315 - val_loss: 7977.0730\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7953.2934 - val_loss: 7976.4016\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7952.5706 - val_loss: 7975.6042\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7951.5931 - val_loss: 7974.2396\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7997.3685 - val_loss: 7975.7919\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7952.0516 - val_loss: 7975.2211\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7951.4765 - val_loss: 7974.6425\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7950.8905 - val_loss: 7974.0508\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7950.2916 - val_loss: 7973.4461\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7949.6801 - val_loss: 7972.8271\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7949.0554 - val_loss: 7972.1973\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7948.4186 - val_loss: 7971.5500\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7947.7665 - val_loss: 7970.8933\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7947.1021 - val_loss: 7970.2208\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7946.4220 - val_loss: 7969.5369\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7945.7144 - val_loss: 7968.8136\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7945.0018 - val_loss: 7968.1059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|     | 8/18 [1:26:11<1:30:25, 542.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 100, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 445,830\n",
      "Trainable params: 445,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6227.9328 - val_loss: 4053.8819\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4770.1401 - val_loss: 22894.1011\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5016.6679 - val_loss: 82871.1620\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 9433.3312 - val_loss: 9078.5017\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 8462.9656 - val_loss: 5145.4388\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4390.0971 - val_loss: 4482.5270\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6232.3987 - val_loss: 13650.3783\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4405.2066 - val_loss: 15695.2145\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3538.5774 - val_loss: 54317.6969\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2839.8768 - val_loss: 5125.1881\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3237.8814 - val_loss: 1544.7565\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2547.6145 - val_loss: 2543.7396\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2204.3984 - val_loss: 2006.1874\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1698.8705 - val_loss: 2416.3985\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2182.1841 - val_loss: 5484.9462\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2165.8942 - val_loss: 1187.1013\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2000.1540 - val_loss: 1695.6284\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2379.8160 - val_loss: 1904.2173\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2543.3392 - val_loss: 2826.2840\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2245.8699 - val_loss: 295.4418\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2007.1362 - val_loss: 296.9871\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1851.7523 - val_loss: 355.9502\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1785.9027 - val_loss: 156.9137\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1541.8869 - val_loss: 1398.9628\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1565.4822 - val_loss: 146.3748\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1319.4874 - val_loss: 237.9308\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1170.4838 - val_loss: 439.8260\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1083.0876 - val_loss: 37.7566\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1015.8940 - val_loss: 549.0928\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1829.1220 - val_loss: 3639.6111\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1379.5005 - val_loss: 630.6899\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1284.8356 - val_loss: 711.4633\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1522.6881 - val_loss: 1424.8702\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2231.7373 - val_loss: 1501.5721\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1799.8133 - val_loss: 2309.0091\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1371.1586 - val_loss: 273.1331\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1451.8735 - val_loss: 366.4840\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1205.6575 - val_loss: 266.0852\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1772.8539 - val_loss: 899.0805\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1015.3025 - val_loss: 1002.9716\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 777.1431 - val_loss: 474.7684\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 738.6003 - val_loss: 337.5262\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 775.9651 - val_loss: 844.4380\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 698.3284 - val_loss: 817.7704\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 692.9161 - val_loss: 503.6774\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 705.8581 - val_loss: 509.4273\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 643.1564 - val_loss: 481.5901\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 724.0745 - val_loss: 254.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|     | 9/18 [1:29:20<1:05:27, 436.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 100, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 445,830\n",
      "Trainable params: 445,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 7177.1045 - val_loss: 5047.7015\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4986.7318 - val_loss: 12538.7105\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4372.0088 - val_loss: 4139.5587\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3546.8134 - val_loss: 2709.7553\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2400.4779 - val_loss: 1805.9710\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1828.2241 - val_loss: 941.3499\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1454.7726 - val_loss: 794.7438\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1323.6821 - val_loss: 913.3545\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1116.0841 - val_loss: 1081.7949\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 981.6656 - val_loss: 926.1481\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 844.0552 - val_loss: 831.3247\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 777.0035 - val_loss: 1064.4323\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 719.8336 - val_loss: 997.4392\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 615.5137 - val_loss: 722.3950\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 661.8553 - val_loss: 1187.7508\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 593.1824 - val_loss: 872.6161\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 527.1240 - val_loss: 1192.5794\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 511.6072 - val_loss: 659.2047\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 472.4110 - val_loss: 530.2537\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 444.3079 - val_loss: 532.1466\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 421.8481 - val_loss: 633.9546\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 391.3200 - val_loss: 401.2054\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 385.7869 - val_loss: 823.8622\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 382.6945 - val_loss: 307.6317\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 380.4144 - val_loss: 810.0840\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 341.7748 - val_loss: 423.0110\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 354.1944 - val_loss: 581.9057\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 449.6242 - val_loss: 554.5144\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 999.7372 - val_loss: 1558.5601\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1242.0221 - val_loss: 336.3910\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 620.0815 - val_loss: 309.2820\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 428.1877 - val_loss: 263.3823\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 357.0016 - val_loss: 107.4565\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 335.3802 - val_loss: 191.8946\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 289.9515 - val_loss: 228.0421\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 296.4627 - val_loss: 310.8230\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 287.1408 - val_loss: 233.4914\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 280.1773 - val_loss: 475.4518\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 315.7339 - val_loss: 821.6045\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 294.3425 - val_loss: 565.2414\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 303.3021 - val_loss: 829.9756\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 273.8329 - val_loss: 863.5731\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 261.9470 - val_loss: 193.7982\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 254.9913 - val_loss: 159.6389\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 253.6720 - val_loss: 303.9684\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 294.0944 - val_loss: 480.7662\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 568.9618 - val_loss: 547.9557\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 638.9798 - val_loss: 200.1898\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 347.3851 - val_loss: 409.9959\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 274.8174 - val_loss: 227.1130\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 271.0530 - val_loss: 384.9095\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 254.8758 - val_loss: 259.9459\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 224.3324 - val_loss: 114.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|    | 10/18 [1:31:45<46:31, 348.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 400, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 7,063,230\n",
      "Trainable params: 7,063,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 6820038391.1384 - val_loss: 33804208338.4110\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 7427838441.1275 - val_loss: 942971755.4286\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 222431556.2500 - val_loss: 15884942.2040\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 5648602.4702 - val_loss: 1371530.1728\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 717663.4732 - val_loss: 329605.0349\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 195449.7323 - val_loss: 106066.5002\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 68648.2197 - val_loss: 41978.8244\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 33069.5966 - val_loss: 26235.1323\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 26642.6869 - val_loss: 23036.1787\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 29816.6023 - val_loss: 46334.8955\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 52772.1148 - val_loss: 49569.8922\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 50928.3172 - val_loss: 47516.4639\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 53174.5916 - val_loss: 42066.2945\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 3075098.0907 - val_loss: 2476758.0586\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 922717.6202 - val_loss: 117631.4753\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 48625.6509 - val_loss: 34671.5836\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 28081.8326 - val_loss: 25039.6715\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 26609.3803 - val_loss: 24227.9238\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 21992.4241 - val_loss: 15135.4215\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 15035.5649 - val_loss: 14828.7227\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 16109.4513 - val_loss: 13399.8228\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 14940.9090 - val_loss: 18319.4913\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 16403.2931 - val_loss: 16982.2691\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 16858.5560 - val_loss: 13663.1947\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 14818.5685 - val_loss: 15229.4658\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 13581.9913 - val_loss: 13210.4534\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 15533.4560 - val_loss: 14020.3128\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 20231.1174 - val_loss: 16770.0071\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 18678.6424 - val_loss: 14575.7792\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 15060.0083 - val_loss: 22818.1363\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 18786.2613 - val_loss: 14853.5713\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 14984.4410 - val_loss: 21826.2217\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 17780.9528 - val_loss: 33340.2168\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 19355.1810 - val_loss: 13280.2670\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 38118.4829 - val_loss: 17684.1882\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 26792.5754 - val_loss: 17459.7309\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 21316.8475 - val_loss: 25628.2335\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 58286.6798 - val_loss: 146499.9406\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 340948.4990 - val_loss: 491353.0015\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 60523592.0494 - val_loss: 27791071.6086\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 6893041.7739 - val_loss: 472055.0609\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 150592.5221 - val_loss: 34025.1131\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 25067027520.6759 - val_loss: 711729277.7456\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 183414624.4208 - val_loss: 8025.0466\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8001.4981 - val_loss: 8024.7591\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 8001.1536 - val_loss: 8024.3752\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.4779 - val_loss: 8023.0234\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 10002478.0541 - val_loss: 8023.6976\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 9546.1853 - val_loss: 8025.2332\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8001.7187 - val_loss: 8025.1043\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8001.5898 - val_loss: 8024.9780\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8001.4670 - val_loss: 8024.8582\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8001.3502 - val_loss: 8024.7441\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8001.2392 - val_loss: 8024.6341\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8001.1323 - val_loss: 8024.5294\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8001.0295 - val_loss: 8024.4284\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.9302 - val_loss: 8024.3294\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.8333 - val_loss: 8024.2347\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.7392 - val_loss: 8024.1411\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.6467 - val_loss: 8024.0496\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 8000.5561 - val_loss: 8023.9590\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 16s 13ms/step - loss: 8000.4666 - val_loss: 8023.8699\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.3779 - val_loss: 8023.7815\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.2901 - val_loss: 8023.6937\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 8000.2023 - val_loss: 8023.6067\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.1149 - val_loss: 8023.5189\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 8000.0269 - val_loss: 8023.4304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|    | 11/18 [1:49:55<1:06:39, 571.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 200, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 1,771,630\n",
      "Trainable params: 1,771,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 5227.5399 - val_loss: 2718.0631\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3917.2714 - val_loss: 5182.2868\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5240.7876 - val_loss: 24961.6551\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6986.2577 - val_loss: 18532.2930\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23780.6978 - val_loss: 25751.9562\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 31759.6169 - val_loss: 169043.9599\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 33765.5129 - val_loss: 9877.9254\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24818.5326 - val_loss: 8013.1470\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 27557.4732 - val_loss: 8012.0973\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 16088.8960 - val_loss: 8012.0471\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 12951.8462 - val_loss: 8009.6761\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 19037.5789 - val_loss: 8007.3747\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 10328.4060 - val_loss: 8005.1271\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 13368.1885 - val_loss: 8002.9391\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 11435.0972 - val_loss: 8000.7907\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 10174.3343 - val_loss: 7998.6804\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 11424.1436 - val_loss: 7996.6154\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 12889.9800 - val_loss: 7994.5901\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 11267.9541 - val_loss: 7992.5848\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 15226.0653 - val_loss: 7990.6025\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 8773.5096 - val_loss: 7988.6228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|   | 12/18 [1:51:45<43:17, 432.85s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 400, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 7,063,230\n",
      "Trainable params: 7,063,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 48154.2932 - val_loss: 1278484.1754\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 51452586.4452 - val_loss: 1534730674.3483\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 4921907603.1141 - val_loss: 7276992054.1057\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 5909577265.6107 - val_loss: 344959656.5793\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 915690475.2752 - val_loss: 9148.3154\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 134671788.3356 - val_loss: 8027.8441\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 33753159.7232 - val_loss: 8025.3878\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 22676787.9945 - val_loss: 661514.6182\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 14431181.0189 - val_loss: 8024.9614\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 10063658.4492 - val_loss: 8024.9667\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 5917389.7177 - val_loss: 8025.3649\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 2999278.2386 - val_loss: 8026.0465\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 6777389.2734 - val_loss: 8027.3943\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 3048608.3241 - val_loss: 8026.6331\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 3586420.8422 - val_loss: 8026.0651\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 920426.4442 - val_loss: 8025.6816\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 2833817.6909 - val_loss: 8025.4394\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 2037124.5389 - val_loss: 8024.8203\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1632778.9893 - val_loss: 8024.8207\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 458323.9271 - val_loss: 8024.7955\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 612409.2884 - val_loss: 8024.7702\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 556008.3654 - val_loss: 8024.7533\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 562707.5548 - val_loss: 8024.7202\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 409833.3915 - val_loss: 8024.7284\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 671937.0347 - val_loss: 8024.6751\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 550101.9482 - val_loss: 8024.6343\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 239165.5346 - val_loss: 8024.5962\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 158148.5992 - val_loss: 8024.5715\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 422811.6466 - val_loss: 8024.5108\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 77466.1498 - val_loss: 8024.4913\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 243516.8422 - val_loss: 8024.4575\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 83067.4844 - val_loss: 8024.4121\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 199617.4270 - val_loss: 8024.3893\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 70672.4643 - val_loss: 8024.3433\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 214931.5865 - val_loss: 8024.2942\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 374651.3770 - val_loss: 8024.2608\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 130034.2339 - val_loss: 8024.2188\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 112871.5019 - val_loss: 8024.1787\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 134006.7340 - val_loss: 8024.1348\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 151150.5471 - val_loss: 8024.0942\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 209473.5558 - val_loss: 8024.0453\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 84679.6543 - val_loss: 8023.9948\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 568332.2420 - val_loss: 8023.9521\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 338692.6066 - val_loss: 8023.9042\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 84775.3636 - val_loss: 8023.8576\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 180768.8468 - val_loss: 8023.8141\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 65818.9883 - val_loss: 8023.7620\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 134042.4950 - val_loss: 8023.7114\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 565681.3947 - val_loss: 8023.6614\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 56612.0509 - val_loss: 8023.6115\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 110484.6491 - val_loss: 8023.5621\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 173146.0739 - val_loss: 8023.5132\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 91144.5720 - val_loss: 8023.4619\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 9s 8ms/step - loss: 62143.9338 - val_loss: 8023.4072\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 108618.2715 - val_loss: 8023.3583\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 65036.5736 - val_loss: 8023.3020\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 201445.9349 - val_loss: 8023.2236\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 247238.6890 - val_loss: 8023.1597\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 39260.8220 - val_loss: 8023.1147\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 147764.6356 - val_loss: 8023.0433\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 18704.6342 - val_loss: 8022.9848\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 33726.7543 - val_loss: 8022.9240\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 95734.9697 - val_loss: 8022.8674\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 666071.2781 - val_loss: 8022.8033\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 914125.7220 - val_loss: 8022.7604\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2320164.6990 - val_loss: 8022.7147\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1081696.8708 - val_loss: 8022.6567\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 119868.1556 - val_loss: 8022.5878\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 152262.2843 - val_loss: 8022.5210\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 29803.0946 - val_loss: 8022.4460\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 20272.0408 - val_loss: 8022.3784\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 66942.9023 - val_loss: 8022.3073\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 103082.0422 - val_loss: 8022.2397\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 26742.9691 - val_loss: 8022.1701\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 133360.6496 - val_loss: 8022.0999\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 15172.0845 - val_loss: 8022.0290\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 52004.4280 - val_loss: 8021.9609\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 25566.3105 - val_loss: 8021.8941\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 9483.5995 - val_loss: 8021.8167\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 14331.4041 - val_loss: 8021.7443\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 176335.1204 - val_loss: 8021.6716\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 185120.0013 - val_loss: 8021.5931\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 69943.7298 - val_loss: 8021.5227\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 26074.9464 - val_loss: 8021.4521\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 15959.3410 - val_loss: 8021.3777\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 17966.1887 - val_loss: 8021.2969\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 9483.1997 - val_loss: 8021.2179\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 32827.0279 - val_loss: 8021.1364\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 22145.7718 - val_loss: 8021.0489\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 43698.0977 - val_loss: 8020.9682\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 13402.6575 - val_loss: 8020.8851\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 36840.7712 - val_loss: 8020.8021\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 32848.1664 - val_loss: 8020.7260\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 20647.9430 - val_loss: 8020.6381\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 19740.1843 - val_loss: 8020.5508\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 20969.6967 - val_loss: 8020.4640\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 9172.5570 - val_loss: 8020.3767\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 17018.9905 - val_loss: 8020.2885\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 14826.0017 - val_loss: 8020.2001\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 7999.4925 - val_loss: 8020.1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|  | 13/18 [2:07:41<49:09, 589.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 200, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 1,771,630\n",
      "Trainable params: 1,771,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 12177.3652 - val_loss: 49302.3976\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 90754.1077 - val_loss: 1142455.0766\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1493247.7716 - val_loss: 607222.1403\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 790060.1199 - val_loss: 542050.7672\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1755606.5904 - val_loss: 130624.0167\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 297226.0326 - val_loss: 18065.5616\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33430.6766 - val_loss: 12080.5278\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 8414.8647 - val_loss: 11488.1699\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 12326.2988 - val_loss: 9058.1484\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 8422.0760 - val_loss: 8049.2292\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7800.2674 - val_loss: 7814.0334\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 9582.0716 - val_loss: 8958.4389\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 9623.9398 - val_loss: 28761.8196\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 8204.8536 - val_loss: 7823.4205\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7773.3956 - val_loss: 7763.1518\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 7810.3892 - val_loss: 7713.7941\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 7574.3507 - val_loss: 7526.3581\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 7463.8235 - val_loss: 7448.3393\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 7393.3987 - val_loss: 7378.8180\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7328.1974 - val_loss: 7301.6745\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7241.8365 - val_loss: 7218.3094\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7140.6236 - val_loss: 7101.3267\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7019.4781 - val_loss: 6975.2553\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6891.7970 - val_loss: 6836.9169\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6748.1366 - val_loss: 6680.2532\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 6582.4200 - val_loss: 6502.5093\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 6396.2816 - val_loss: 6309.1632\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 6188.7219 - val_loss: 6097.3874\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 5962.9948 - val_loss: 5851.2044\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 5725.1460 - val_loss: 5608.2909\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 5491.4330 - val_loss: 5387.9233\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 5231.7583 - val_loss: 5087.0738\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 4938.9590 - val_loss: 4783.3170\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 4639.0370 - val_loss: 4466.2315\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 4299.8702 - val_loss: 4133.5895\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 4025.2540 - val_loss: 3837.7048\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 3689.5284 - val_loss: 3534.2601\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 3349.5183 - val_loss: 3146.5153\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 2367.0046 - val_loss: 1553.7271\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 793.4428 - val_loss: 303.8485\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 186.2937 - val_loss: 108.3338\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 94.2055 - val_loss: 91.1183\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 82.4090 - val_loss: 69.4791\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 75.4119 - val_loss: 71.2890\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 75.1453 - val_loss: 63.4399\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 68.4609 - val_loss: 61.5469\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 62.8614 - val_loss: 55.9059\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 61.5577 - val_loss: 54.9561\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 62.5587 - val_loss: 54.5647\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 57.8736 - val_loss: 52.7179\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 57.0590 - val_loss: 52.5686\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 54.4602 - val_loss: 50.1151\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 54.1313 - val_loss: 55.9547\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 55.0299 - val_loss: 50.8594\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 52.7452 - val_loss: 48.5272\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 5s 5ms/step - loss: 51.4362 - val_loss: 48.2278\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 55.7123 - val_loss: 56.6698\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 65.1638 - val_loss: 55.3695\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 58.3448 - val_loss: 57.0742\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 57.6916 - val_loss: 55.1110\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 55.5989 - val_loss: 56.9386\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 55.4725 - val_loss: 54.1989\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 55.3409 - val_loss: 51.7983\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 52.1976 - val_loss: 51.4351\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 50.6365 - val_loss: 48.7516\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 49.6207 - val_loss: 48.7163\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 49.2585 - val_loss: 47.5119\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 47.9964 - val_loss: 46.9774\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 47.0017 - val_loss: 48.0389\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 53.5540 - val_loss: 63.1156\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 46.7571 - val_loss: 57.3650\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 45.2435 - val_loss: 54.7802\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 45.3790 - val_loss: 42.9266\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 42.9726 - val_loss: 42.5702\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 42.8552 - val_loss: 41.7814\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 41.9173 - val_loss: 41.4593\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 42.5967 - val_loss: 39.7292\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 41.2805 - val_loss: 38.8403\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 40.3668 - val_loss: 37.8206\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 38.6097 - val_loss: 38.5025\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 38.9887 - val_loss: 39.0682\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 44.0122 - val_loss: 39.5687\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 39.6884 - val_loss: 37.3012\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 40.2132 - val_loss: 36.8878\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 37.8117 - val_loss: 37.4142\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 37.4598 - val_loss: 36.3561\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 37.1036 - val_loss: 36.1955\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.7668 - val_loss: 37.3279\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36.5368 - val_loss: 36.5001\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 37.1777 - val_loss: 35.9034\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 35.1004 - val_loss: 35.5412\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.4665 - val_loss: 36.6910\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.1832 - val_loss: 35.0413\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 35.6563 - val_loss: 35.3739\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 34.9217 - val_loss: 36.6487\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 34.4280 - val_loss: 34.3889\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 34.2614 - val_loss: 34.3046\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.8826 - val_loss: 33.4678\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33.8972 - val_loss: 33.2767\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33.8789 - val_loss: 33.3996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|  | 14/18 [2:17:07<38:50, 582.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 100, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 445,830\n",
      "Trainable params: 445,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7891.9032 - val_loss: 6388.7661\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4929.2467 - val_loss: 4996.2196\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4291.2468 - val_loss: 3457.3506\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6541.8966 - val_loss: 7650.9064\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6005.0750 - val_loss: 3559.4189\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3265.4291 - val_loss: 5024.4171\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2793.2571 - val_loss: 3459.3717\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3447.2489 - val_loss: 4699.0143\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 302450.4631 - val_loss: 228470.4864\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 525253.4498 - val_loss: 312506.2432\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 244894.2464 - val_loss: 167057.4254\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 682081.9613 - val_loss: 1054094.3356\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1152540.6900 - val_loss: 586621.9771\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 855313.2269 - val_loss: 700246.4809\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 836424.5008 - val_loss: 360751.0136\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 254751.6011 - val_loss: 170390.1698\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 123905.4712 - val_loss: 72281.7956\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47548.5536 - val_loss: 24933.1164\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 14315.0683 - val_loss: 6298.6773\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3661.0065 - val_loss: 1592.6342\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 959.3372 - val_loss: 467.2462\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 352.6809 - val_loss: 256.4810\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 226.9960 - val_loss: 182.7476\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 166.9449 - val_loss: 138.5576\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 132.5092 - val_loss: 106.3963\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 109.4967 - val_loss: 97.4643\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 101.8642 - val_loss: 90.1312\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 93.1455 - val_loss: 88.2644\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 89.3780 - val_loss: 85.7287\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 86.9097 - val_loss: 80.5686\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 84.2454 - val_loss: 80.3943\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 85.3462 - val_loss: 87.7315\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 90.7431 - val_loss: 85.1270\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 104.7008 - val_loss: 95.4680\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 89.7903 - val_loss: 82.4575\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 82.1116 - val_loss: 73.4811\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 78.8655 - val_loss: 81.6166\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 92.1553 - val_loss: 68.9714\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 74.2002 - val_loss: 67.4432\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 72.5843 - val_loss: 67.9366\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 77.9633 - val_loss: 77.2711\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 99.1909 - val_loss: 112.6883\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 108.9539 - val_loss: 100.6722\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 102.1396 - val_loss: 87.1953\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 95.0335 - val_loss: 86.2831\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 89.3533 - val_loss: 82.3474\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 85.6879 - val_loss: 82.2762\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 83.8359 - val_loss: 77.4733\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 81.5630 - val_loss: 74.8125\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 80.0401 - val_loss: 74.1280\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 79.4259 - val_loss: 76.8769\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 80.7399 - val_loss: 72.4215\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 79.1140 - val_loss: 74.2285\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 81.0121 - val_loss: 78.8975\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 79.6467 - val_loss: 73.9251\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 3s 2ms/step - loss: 78.5702 - val_loss: 76.0309\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 77.4148 - val_loss: 70.2518\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 75.6144 - val_loss: 70.8887\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 75.8408 - val_loss: 71.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%| | 15/18 [2:19:55<22:54, 458.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 400, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 7,063,230\n",
      "Trainable params: 7,063,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 21s 18ms/step - loss: 256536153.9772 - val_loss: 2339344692.2270\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 892030132857.1274 - val_loss: 50376157.7848\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 47844114203.6505 - val_loss: 1689511.0114\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 17s 15ms/step - loss: 1741004.4094 - val_loss: 8049.6199\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 17s 15ms/step - loss: 937812.2321 - val_loss: 8025.0700\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 17s 15ms/step - loss: 3479629.1044 - val_loss: 8025.0623\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 17s 15ms/step - loss: 1750874.8752 - val_loss: 8025.0574\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 1329035.6570 - val_loss: 8025.0497\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 549778.2535 - val_loss: 8025.0427\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 396397.2188 - val_loss: 8025.0346\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 590104.2769 - val_loss: 8025.0253\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 750441.0758 - val_loss: 8025.0146\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 545738.0165 - val_loss: 8025.0048\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 166868.8986 - val_loss: 8024.9978\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 173021.7837 - val_loss: 8024.9874\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 150993.4953 - val_loss: 8024.9773\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 231776.0452 - val_loss: 8024.9663\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 146680.4842 - val_loss: 8024.9545\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 17s 15ms/step - loss: 121953.0487 - val_loss: 8024.9405\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 144609.7964 - val_loss: 8024.9282\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 124866.4516 - val_loss: 8024.9169\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 108943.8630 - val_loss: 8024.9047\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 425433.3793 - val_loss: 8476.8005\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 386671.8201 - val_loss: 8024.8788\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 16s 14ms/step - loss: 171195.1745 - val_loss: 8024.8630\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 809820.3551 - val_loss: 15910.4085\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 281136.4105 - val_loss: 8024.8358\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 304882.8131 - val_loss: 8024.8204\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 523550.6568 - val_loss: 8024.8010\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 196527.5418 - val_loss: 8024.7844\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 96472.4265 - val_loss: 9369.7426\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 49001.6311 - val_loss: 8024.7495\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 40441.1087 - val_loss: 8024.7305\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 39939.7263 - val_loss: 8024.7113\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 39073.2048 - val_loss: 8024.6909\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 25316.6104 - val_loss: 8024.6703\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 33673.1705 - val_loss: 8024.6493\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 20766.6741 - val_loss: 8024.6281\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 47643.3524 - val_loss: 8024.6059\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 46711.1310 - val_loss: 8024.5827\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 18502.2794 - val_loss: 8024.5597\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 33685.6297 - val_loss: 8024.5361\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 17566.0815 - val_loss: 8024.5116\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 111327.8357 - val_loss: 8024.4868\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 14712.1663 - val_loss: 8024.4615\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 19935.3977 - val_loss: 8024.4353\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 12840.6523 - val_loss: 8024.4080\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 10362.2854 - val_loss: 8024.3806\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 24653.3693 - val_loss: 8024.3528\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 17812.2880 - val_loss: 8024.3235\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 31334.9755 - val_loss: 8024.2944\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 30606.7482 - val_loss: 8024.2638\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 17s 14ms/step - loss: 18622.9381 - val_loss: 8024.2324\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 19052.0641 - val_loss: 8024.2005\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 16836.3600 - val_loss: 8024.1681\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 10839.6664 - val_loss: 8024.1349\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 26831.3931 - val_loss: 8024.1010\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 37299.1060 - val_loss: 8024.0660\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 28666.8352 - val_loss: 8024.0303\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 65083.4097 - val_loss: 8023.9934\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 22940.6901 - val_loss: 8023.9573\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 15502.5723 - val_loss: 8023.9197\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 14107.0173 - val_loss: 8023.8809\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 164755.9320 - val_loss: 8023.8270\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 15984.7258 - val_loss: 8023.7937\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 13247.5268 - val_loss: 8023.7358\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 50565.6551 - val_loss: 8023.7174\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 8211.7004 - val_loss: 8023.6741\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 8197.1195 - val_loss: 8023.6301\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 11862.2594 - val_loss: 8023.5854\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 8500.6699 - val_loss: 8023.5393\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 20334.0290 - val_loss: 8023.4926\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 11803.0440 - val_loss: 8023.4451\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 11434.0917 - val_loss: 8023.3960\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 19654.9683 - val_loss: 8023.3458\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 12532.2559 - val_loss: 8023.2947\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 8204.1095 - val_loss: 8023.2422\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 36410.6129 - val_loss: 8023.1879\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 8033.8597 - val_loss: 8023.1334\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 16872.0092 - val_loss: 8023.0786\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 7999.1584 - val_loss: 8023.0219\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 9208.4283 - val_loss: 8022.9641\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 10568.6811 - val_loss: 8022.9050\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 16116.9705 - val_loss: 8022.8440\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 844050.4637 - val_loss: 8022.7828\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 108075.9403 - val_loss: 8022.7179\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 11495447.9456 - val_loss: 8022.6595\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 131043185.8370 - val_loss: 8022.6231\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 15845064.7410 - val_loss: 8022.5620\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 3193032.7943 - val_loss: 8022.4946\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 3960065.5765 - val_loss: 8022.4174\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 2636429.0418 - val_loss: 8022.3488\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2399121.2878 - val_loss: 8022.2777\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 103042.9737 - val_loss: 8022.2042\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1290673.5979 - val_loss: 8022.1299\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 808461.2780 - val_loss: 8022.0501\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 24376.8443 - val_loss: 8021.9687\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 189605.2586 - val_loss: 8021.8880\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 131131.6152 - val_loss: 8021.8059\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 13800.7984 - val_loss: 8021.7227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%| | 16/18 [2:46:12<26:28, 794.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 100, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 445,830\n",
      "Trainable params: 445,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 6543.7269 - val_loss: 3655.9104\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2590.9447 - val_loss: 2051.1666\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1077.8408 - val_loss: 615.8996\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 369.7054 - val_loss: 197.1847\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 185.5585 - val_loss: 198.0591\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 158.4634 - val_loss: 176.7928\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 109.4287 - val_loss: 128.7412\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 100.7372 - val_loss: 68.7454\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 63.2423 - val_loss: 70.8182\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 61.8209 - val_loss: 48.8232\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 71.2091 - val_loss: 69.9964\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 80.0334 - val_loss: 61.8938\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 50.7772 - val_loss: 48.0636\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 39.8417 - val_loss: 42.7237\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 36.0741 - val_loss: 37.4644\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.0885 - val_loss: 58.3490\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 59.5055 - val_loss: 60.9420\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.0131 - val_loss: 31.1068\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 34.0870 - val_loss: 32.3538\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 25.7007 - val_loss: 28.3096\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.1666 - val_loss: 22.1727\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 21.1537 - val_loss: 21.0285\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 22.9248 - val_loss: 28.2351\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 19.6866 - val_loss: 19.9396\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 20.1003 - val_loss: 22.5891\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.5079 - val_loss: 23.8484\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.9811 - val_loss: 23.3085\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21.2334 - val_loss: 22.1760\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21.3506 - val_loss: 26.3847\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 22.4259 - val_loss: 32.8830\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.0191 - val_loss: 20.1629\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 20.0647 - val_loss: 21.5152\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 23.8577 - val_loss: 19.2457\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.0338 - val_loss: 22.6949\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 32.0344 - val_loss: 35.1137\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 37.1708 - val_loss: 45.8055\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 40.2996 - val_loss: 27.7344\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.3225 - val_loss: 23.8728\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.1944 - val_loss: 23.7429\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.9486 - val_loss: 22.7355\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 21.5141 - val_loss: 25.1182\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 19.8283 - val_loss: 20.7923\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 18.1274 - val_loss: 20.7650\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.5272 - val_loss: 26.3280\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 19.0930 - val_loss: 22.1203\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.3919 - val_loss: 21.1718\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 17.2113 - val_loss: 24.3425\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 19.7459 - val_loss: 22.6712\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 17.1599 - val_loss: 22.6628\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.7114 - val_loss: 21.8567\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.9241 - val_loss: 24.6540\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 22.3683 - val_loss: 24.1267\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.9019 - val_loss: 27.2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|| 17/18 [2:48:32<09:57, 597.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 200, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 1,771,630\n",
      "Trainable params: 1,771,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 3562.5981 - val_loss: 4521.9097\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2073.9000 - val_loss: 5436.8100\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 4844.8640 - val_loss: 2771.0417\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 8255.5221 - val_loss: 5919.2620\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 5351.3783 - val_loss: 6546.1455\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 231424.3402 - val_loss: 8001.1295\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 62029.6383 - val_loss: 7993.6621\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 12514.2663 - val_loss: 7985.2278\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 11828.9482 - val_loss: 7976.8823\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 8196.8582 - val_loss: 7905.3105\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 11811.2292 - val_loss: 7960.3275\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7927.2873 - val_loss: 7951.6975\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7920.1240 - val_loss: 7941.4767\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7898.9733 - val_loss: 7929.0113\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7870.7095 - val_loss: 7912.4873\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 8657.1279 - val_loss: 4717.5307\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 93886.5548 - val_loss: 7903.7172\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 8095.3807 - val_loss: 7908.8569\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 12685.3660 - val_loss: 7901.2872\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 8747.3752 - val_loss: 7665.3766\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 10247.8337 - val_loss: 7732.6655\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 11309.5857 - val_loss: 7883.2167\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 9245.0603 - val_loss: 7877.3148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [2:51:32<00:00, 571.80s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "\n",
    "scan_object = talos.Scan(X_train,\n",
    "                         y_train, \n",
    "                         params=p,\n",
    "                         model=LSTM_model,\n",
    "                         experiment_name='LSTM',\n",
    "                         fraction_limit=0.5,\n",
    "                         print_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug (link below) in the Talos library that means it sorts the columns and values from parameter dictionary incorrectly in the ```talos.Analze``` object. That is why each hyperparameter has unique values so I can easily understand the results.\n",
    "\n",
    "[https://github.com/autonomio/talos/issues/439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>LSTM_dropout</th>\n",
       "      <th>LSTM_n</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>53</td>\n",
       "      <td>27.232586</td>\n",
       "      <td>19.901883</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100</td>\n",
       "      <td>33.399596</td>\n",
       "      <td>33.878929</td>\n",
       "      <td>60</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>59</td>\n",
       "      <td>71.236593</td>\n",
       "      <td>75.840805</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53</td>\n",
       "      <td>114.846235</td>\n",
       "      <td>224.332397</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48</td>\n",
       "      <td>254.929426</td>\n",
       "      <td>724.074489</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>256.996254</td>\n",
       "      <td>261.670937</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23</td>\n",
       "      <td>7877.314776</td>\n",
       "      <td>9245.060340</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>7921.480580</td>\n",
       "      <td>7898.852284</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80</td>\n",
       "      <td>7968.105866</td>\n",
       "      <td>7945.001771</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>7980.693736</td>\n",
       "      <td>9586.655944</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>7982.131144</td>\n",
       "      <td>12319.705792</td>\n",
       "      <td>90</td>\n",
       "      <td>400</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>7988.622800</td>\n",
       "      <td>8773.509585</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>7992.626135</td>\n",
       "      <td>8507.503703</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100</td>\n",
       "      <td>8020.109193</td>\n",
       "      <td>7999.492469</td>\n",
       "      <td>90</td>\n",
       "      <td>400</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100</td>\n",
       "      <td>8021.722695</td>\n",
       "      <td>13800.798435</td>\n",
       "      <td>30</td>\n",
       "      <td>400</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>8022.153768</td>\n",
       "      <td>7996.182863</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>67</td>\n",
       "      <td>8023.430448</td>\n",
       "      <td>8000.026928</td>\n",
       "      <td>30</td>\n",
       "      <td>400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>33300.675021</td>\n",
       "      <td>15056.629315</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      val_loss          loss  LSTM_dropout  LSTM_n  batch_size\n",
       "16            53     27.232586     19.901883            90     100         0.0\n",
       "13           100     33.399596     33.878929            60     200         0.0\n",
       "14            59     71.236593     75.840805           120     100         0.0\n",
       "9             53    114.846235    224.332397           120     100         0.2\n",
       "8             48    254.929426    724.074489            60     100         0.2\n",
       "2             54    256.996254    261.670937            60     100         0.1\n",
       "17            23   7877.314776   9245.060340            30     200         0.1\n",
       "0            100   7921.480580   7898.852284            90     200         0.0\n",
       "7             80   7968.105866   7945.001771            30     200         0.0\n",
       "3             24   7980.693736   9586.655944            90     100         0.1\n",
       "4            100   7982.131144  12319.705792            90     400         0.1\n",
       "11            21   7988.622800   8773.509585           120     200         0.2\n",
       "5             21   7992.626135   8507.503703            60     100         0.0\n",
       "12           100   8020.109193   7999.492469            90     400         0.2\n",
       "15           100   8021.722695  13800.798435            30     400         0.2\n",
       "6             23   8022.153768   7996.182863           120     200         0.0\n",
       "10            67   8023.430448   8000.026928            30     400         0.0\n",
       "1            100  33300.675021  15056.629315            30     200         0.2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_object = talos.Analyze(scan_object)\n",
    "df = analyze_object.data\n",
    "df.sort_values(by=['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "* The top three results are interesting. They all feature:\n",
    "  * 100 or 200 neurons in LSTM layers\n",
    "  * No dropout for LSTM layers\n",
    "  * Large batch size > 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
