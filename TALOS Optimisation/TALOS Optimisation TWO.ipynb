{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS Optimization - TWO\n",
    "\n",
    "An implementation of a hyperparameter grid search using the Talos library.\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check what hardware is available to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12047915752496629882\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8451138988614149474\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5031, 16)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    keep_col = list(range(1,18))\n",
    "\n",
    "    df_day = pd.read_csv('ECL_Clean_Day.csv', \n",
    "                     infer_datetime_format=True,\n",
    "                     parse_dates=['Timestamp'], \n",
    "                     index_col=['Timestamp'],\n",
    "                     usecols = keep_col,\n",
    "                     date_parser=lambda col: pd.to_datetime(col, utc=True).tz_convert('America/New_York'))\n",
    "\n",
    "    #df_min.dtypes\n",
    "    print (df_day.shape)\n",
    "    df_day.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 10 years of data from 2019 - 2010\n",
    "day_data = df_day[df_day.index >= '2010-01-01']\n",
    "\n",
    "#split 7 years train, 3 years test\n",
    "day_train = day_data[day_data.index <= '2017-01-01']\n",
    "day_test = day_data[day_data.index >= '2017-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = day_train['Close'] \n",
    "base_test = day_test['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(base_test)\n",
    "plt.plot(base_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Simple Vanilla Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1703, 30) (1703, 30) (695, 30) (695, 30)\n"
     ]
    }
   ],
   "source": [
    "# Multi-step data preparation\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# define input sequence\n",
    "#raw_seq = list(base_test[:100].values)\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 30, 30\n",
    "# split into samples\n",
    "X_train, y_train = split_sequence(base_train.values, n_steps_in, n_steps_out)\n",
    "X_test, y_test = split_sequence(base_test.values, n_steps_in, n_steps_out) #length must be > n_in + n_out\n",
    "# summarize the data\n",
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 43,830\n",
      "Trainable params: 43,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/jacobscottanthony/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1703 samples, validate on 695 samples\n",
      "Epoch 1/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 10019.1586 - val_loss: 16243.2162\n",
      "Epoch 2/20\n",
      "1703/1703 [==============================] - 1s 801us/step - loss: 4184.2281 - val_loss: 22997.2938\n",
      "Epoch 3/20\n",
      "1703/1703 [==============================] - 1s 802us/step - loss: 1149.7130 - val_loss: 2016.9637\n",
      "Epoch 4/20\n",
      "1703/1703 [==============================] - 1s 789us/step - loss: 2828.2662 - val_loss: 70688.4714\n",
      "Epoch 5/20\n",
      "1703/1703 [==============================] - 1s 794us/step - loss: 87415.7203 - val_loss: 6450.1141\n",
      "Epoch 6/20\n",
      "1703/1703 [==============================] - 1s 794us/step - loss: 1118.2156 - val_loss: 803.1168\n",
      "Epoch 7/20\n",
      "1703/1703 [==============================] - 1s 807us/step - loss: 358.7226 - val_loss: 159.7322\n",
      "Epoch 8/20\n",
      "1703/1703 [==============================] - 1s 803us/step - loss: 289.0814 - val_loss: 155.1185\n",
      "Epoch 9/20\n",
      "1703/1703 [==============================] - 1s 794us/step - loss: 138.0895 - val_loss: 134.0615\n",
      "Epoch 10/20\n",
      "1703/1703 [==============================] - 1s 792us/step - loss: 123.6688 - val_loss: 128.8112\n",
      "Epoch 11/20\n",
      "1703/1703 [==============================] - 1s 798us/step - loss: 89.8606 - val_loss: 126.7266\n",
      "Epoch 12/20\n",
      "1703/1703 [==============================] - 1s 802us/step - loss: 76.5442 - val_loss: 120.0634\n",
      "Epoch 13/20\n",
      "1703/1703 [==============================] - 1s 797us/step - loss: 64.1791 - val_loss: 105.1389\n",
      "Epoch 14/20\n",
      "1703/1703 [==============================] - 1s 799us/step - loss: 54.4675 - val_loss: 100.8206\n",
      "Epoch 15/20\n",
      "1703/1703 [==============================] - 1s 798us/step - loss: 49.6770 - val_loss: 98.9480\n",
      "Epoch 16/20\n",
      "1703/1703 [==============================] - 1s 799us/step - loss: 41.8893 - val_loss: 97.9036\n",
      "Epoch 17/20\n",
      "1703/1703 [==============================] - 1s 797us/step - loss: 38.9295 - val_loss: 100.8099\n",
      "Epoch 18/20\n",
      "1703/1703 [==============================] - 1s 799us/step - loss: 35.7903 - val_loss: 103.1519\n",
      "Epoch 19/20\n",
      "1703/1703 [==============================] - 1s 795us/step - loss: 34.1113 - val_loss: 103.7914\n",
      "Epoch 20/20\n",
      "1703/1703 [==============================] - 1s 798us/step - loss: 31.9772 - val_loss: 95.4668\n"
     ]
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "#print (X_train.shape)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "#print (X_test.shape)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# fit model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for grid search\n",
    "\n",
    "p = {'LSTM_n' : [100, 200, 400],\n",
    "    'LSTM_dropout' : [0, 0.1, 0.2],\n",
    "    'batch_size' : [30, 60, 90, 120],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train, y_train, X_test, y_test, params):\n",
    "    model = Sequential()                            \n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu'))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='Adam', \n",
    "                  loss='mse')\n",
    "    print (model.summary())\n",
    "    \n",
    "    #Early Stopping to avoid wasting time on bad hyperparams\n",
    "    es = EarlyStopping(monitor='val_loss', mode='auto', patience=20)\n",
    "\n",
    "    out = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    callbacks=[es],\n",
    "                    validation_data=[X_test, y_test])\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 200, 'batch_size': 90, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 809,230\n",
      "Trainable params: 809,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 11520.3349 - val_loss: 61068.7524\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 28764.8960 - val_loss: 15065.4808\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 24726.9315 - val_loss: 88975.4854\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 86487.2300 - val_loss: 57991.3808\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 146059.0500 - val_loss: 429726.2452\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2256526.5391 - val_loss: 2815092.0411\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3788132.9388 - val_loss: 4329567.9364\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7411481.3914 - val_loss: 8173811.8376\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6807179.9614 - val_loss: 3906253.2872\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1316489.0653 - val_loss: 528992.7477\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 441177.9484 - val_loss: 14575.6412\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 8671.7138 - val_loss: 7659.3899\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7519.6184 - val_loss: 7665.5733\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7426.5608 - val_loss: 7499.9667\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7317.4997 - val_loss: 7430.1585\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7242.2735 - val_loss: 7378.1129\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7157.2371 - val_loss: 7258.6656\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7077.1013 - val_loss: 7332.9262\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7215.1448 - val_loss: 7631.3851\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7536.5359 - val_loss: 7728.0836\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7520.8557 - val_loss: 7578.4123\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7066.7027 - val_loss: 6973.4840\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6689.7430 - val_loss: 6728.3662\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6479.3994 - val_loss: 6490.9677\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6244.9785 - val_loss: 6241.3021\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5985.9171 - val_loss: 5961.8155\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5703.0113 - val_loss: 5661.3315\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5403.1262 - val_loss: 5348.8716\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5091.3926 - val_loss: 5035.2456\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4780.6883 - val_loss: 4707.8897\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4457.0655 - val_loss: 4368.1635\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4128.9390 - val_loss: 4031.5918\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4111.3313 - val_loss: 3752.8913\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3559.6513 - val_loss: 3468.5778\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3237.1882 - val_loss: 3140.7692\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2944.4171 - val_loss: 2867.0351\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2673.9762 - val_loss: 2583.9272\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2415.9959 - val_loss: 2365.5798\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2188.0283 - val_loss: 2094.9897\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1929.7361 - val_loss: 1891.5136\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1693.9068 - val_loss: 1593.0766\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1503.2357 - val_loss: 1414.3408\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1310.3530 - val_loss: 1257.9652\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1166.0665 - val_loss: 1096.8921\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1008.4846 - val_loss: 947.3715\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 873.7042 - val_loss: 826.5036\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 766.7647 - val_loss: 768.0787\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 757.4920 - val_loss: 1436.3195\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 481723.0751 - val_loss: 12250.4618\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 10285.9796 - val_loss: 8096.0113\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7916.1784 - val_loss: 8086.2750\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7908.7494 - val_loss: 8084.5517\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7906.9160 - val_loss: 8082.5094\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7905.7889 - val_loss: 8081.2962\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7904.7338 - val_loss: 8080.2385\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7903.7279 - val_loss: 8079.2389\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7902.7340 - val_loss: 8078.2197\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7901.7248 - val_loss: 8077.1918\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7900.7160 - val_loss: 8076.1485\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7899.6823 - val_loss: 8075.0910\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7898.6364 - val_loss: 8074.0238\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7897.5863 - val_loss: 8072.9381\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7896.5095 - val_loss: 8071.8384\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7895.4293 - val_loss: 8070.7133\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7894.3201 - val_loss: 8069.5869\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7893.2072 - val_loss: 8068.4318\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7892.0715 - val_loss: 8067.2591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 1/18 [03:04<52:23, 184.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 400, 'batch_size': 120, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,218,430\n",
      "Trainable params: 3,218,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 8153.3549 - val_loss: 91072.0989\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 42728.1010 - val_loss: 94033.7083\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 86447.6925 - val_loss: 459124.3949\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1069793.8161 - val_loss: 515708.4909\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 935347.7273 - val_loss: 1607531.6688\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3351557.8213 - val_loss: 2484908.3567\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2378261.5629 - val_loss: 571477.5774\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1021792.5474 - val_loss: 192823.9859\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 549614.7852 - val_loss: 77128.4897\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 326562.3559 - val_loss: 85574.5664\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 214293.5819 - val_loss: 80647.9088\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 135343.1229 - val_loss: 31004.4805\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 123505.0051 - val_loss: 128861.3345\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 110107.0639 - val_loss: 39018.5459\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 65282.5347 - val_loss: 32644.6939\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35980.4213 - val_loss: 13442.9961\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 18139.0103 - val_loss: 10214.1797\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 13193.1261 - val_loss: 5424.8534\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 9331.8915 - val_loss: 2797.2697\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7556.0491 - val_loss: 2213.5063\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5795.7212 - val_loss: 1058.5205\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5749.1671 - val_loss: 421.7291\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4553.9012 - val_loss: 516.3373\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 3434.2082 - val_loss: 605.5679\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 2825.0417 - val_loss: 149.4276\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 2395.2188 - val_loss: 119.8888\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1921.9961 - val_loss: 266.1018\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1763.6950 - val_loss: 188.6916\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1565.6821 - val_loss: 227.1436\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1468.1052 - val_loss: 258.1089\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1430.9840 - val_loss: 261.4732\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1422.7188 - val_loss: 256.6772\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1288.4717 - val_loss: 199.5122\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1325.2662 - val_loss: 299.5286\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1291.5241 - val_loss: 355.3418\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1234.9359 - val_loss: 271.7245\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1255.5982 - val_loss: 492.4788\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1205.2433 - val_loss: 292.2750\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1122.4560 - val_loss: 438.8988\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1083.4853 - val_loss: 458.6786\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1059.1784 - val_loss: 392.9513\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1067.4871 - val_loss: 176.2879\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1139.3126 - val_loss: 249.7158\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1033.2637 - val_loss: 509.2536\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1009.8195 - val_loss: 415.2461\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 996.9469 - val_loss: 677.0939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 2/18 [07:08<54:02, 202.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 400, 'batch_size': 120, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,218,430\n",
      "Trainable params: 3,218,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 33786.0188 - val_loss: 319937.5028\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 4144031.7397 - val_loss: 25458606.6888\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 944078456.0134 - val_loss: 1194341752.8611\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3144986937.5570 - val_loss: 2992180941.4012\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 4030380664.2685 - val_loss: 938544914.1605\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 3405555090.0403 - val_loss: 1129297286.7632\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 1692209706.0940 - val_loss: 277491231.4677\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 759105374.9262 - val_loss: 69913798.8102\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 477129231.2483 - val_loss: 67266464.5088\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 358291787.8121 - val_loss: 47430627.3973\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 278197374.0671 - val_loss: 22134225.1663\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 204585254.7651 - val_loss: 24688271.5695\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 162496544.7517 - val_loss: 4858067.8415\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 127238351.0872 - val_loss: 8267574.9971\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 110245612.0805 - val_loss: 3498656.3723\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 93451274.8456 - val_loss: 3844182.5157\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 85810328.9128 - val_loss: 4983964.4344\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 50797112.6711 - val_loss: 10646359.0431\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 17749293.7987 - val_loss: 5234695.8493\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 5944912.4262 - val_loss: 1626437.7089\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 3287723.7785 - val_loss: 1028010.5709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 3/18 [09:12<44:46, 179.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 400, 'batch_size': 120, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,218,430\n",
      "Trainable params: 3,218,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 83109.8189 - val_loss: 583961.5802\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1219159.0956 - val_loss: 1753130.9354\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3592822.3372 - val_loss: 5732218.7309\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 10988144.8188 - val_loss: 29636758.6341\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 19404154.8893 - val_loss: 3570145.6673\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2155177.8372 - val_loss: 1129615.3471\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 509903.3162 - val_loss: 171204.0243\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 138903.0350 - val_loss: 220005.0808\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 493069.0108 - val_loss: 1056239.0121\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 707706.7731 - val_loss: 313504.4314\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 142156.7636 - val_loss: 41149.7989\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35866.5105 - val_loss: 25146.7373\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 16802.1598 - val_loss: 8764.8938\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5545.3567 - val_loss: 3344.6477\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2327.2965 - val_loss: 1528.9551\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1192.7263 - val_loss: 724.8154\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 536.1588 - val_loss: 423.5194\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 317.2920 - val_loss: 220.7338\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 180.1673 - val_loss: 173.5544\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 152.4845 - val_loss: 166.8871\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 129.7048 - val_loss: 102.1239\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 82.3473 - val_loss: 74.1951\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 64.3840 - val_loss: 64.1064\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 55.6301 - val_loss: 55.4252\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 49.1451 - val_loss: 50.1675\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 45.2526 - val_loss: 44.3703\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 40.0393 - val_loss: 41.2861\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 37.2916 - val_loss: 37.9655\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 34.8711 - val_loss: 35.6291\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 32.5896 - val_loss: 32.9478\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 30.6958 - val_loss: 31.8970\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29.2082 - val_loss: 30.0913\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 28.2240 - val_loss: 28.8879\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 27.3409 - val_loss: 28.2746\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.4640 - val_loss: 27.8194\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.1808 - val_loss: 27.6975\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.9389 - val_loss: 27.3507\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.5189 - val_loss: 26.9866\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.2518 - val_loss: 26.4535\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 25.0409 - val_loss: 26.0510\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 25.7510 - val_loss: 26.6863\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24.5768 - val_loss: 25.5933\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24.1891 - val_loss: 25.2004\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24.0549 - val_loss: 25.9968\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 29.4226 - val_loss: 30.5110\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 28.4820 - val_loss: 27.9281\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 25.7805 - val_loss: 26.1725\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 24.6210 - val_loss: 25.0063\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24.1028 - val_loss: 24.8819\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.8575 - val_loss: 24.7431\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.4964 - val_loss: 24.3132\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.2729 - val_loss: 24.1454\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.1659 - val_loss: 24.8848\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.4871 - val_loss: 28.1718\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.7717 - val_loss: 28.4877\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24.8397 - val_loss: 25.7234\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24.2163 - val_loss: 24.7264\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.2936 - val_loss: 24.5007\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.6604 - val_loss: 24.1864\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.4480 - val_loss: 23.6511\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.6074 - val_loss: 25.9000\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.4793 - val_loss: 24.6071\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.7110 - val_loss: 23.8377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.2883 - val_loss: 23.3776\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.1940 - val_loss: 23.1266\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.0562 - val_loss: 23.0420\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.0881 - val_loss: 23.1485\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.9483 - val_loss: 23.0951\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.8024 - val_loss: 22.9416\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.0205 - val_loss: 22.6167\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.6203 - val_loss: 22.7293\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.5094 - val_loss: 22.5359\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.4374 - val_loss: 22.4983\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.4236 - val_loss: 22.4410\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.3498 - val_loss: 22.9516\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 21.4388 - val_loss: 22.3991\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.5367 - val_loss: 22.5215\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 21.3986 - val_loss: 22.3277\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 21.0869 - val_loss: 22.0823\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 21.2931 - val_loss: 22.4058\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.5050 - val_loss: 24.0671\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 22.5297 - val_loss: 23.5107\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 22.0028 - val_loss: 22.8744\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 21.7810 - val_loss: 23.6747\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 21.6817 - val_loss: 22.8317\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 21.5093 - val_loss: 22.8727\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 21.5056 - val_loss: 22.7819\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 21.3683 - val_loss: 22.4777\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 21.1279 - val_loss: 22.3051\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.8403 - val_loss: 22.1062\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.8983 - val_loss: 21.7731\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 21.1110 - val_loss: 22.3666\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.8309 - val_loss: 22.0509\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.6674 - val_loss: 21.9113\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.5900 - val_loss: 21.7643\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.5525 - val_loss: 21.5472\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.4883 - val_loss: 21.5565\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.6122 - val_loss: 21.3598\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 20.6835 - val_loss: 21.4396\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 20.3015 - val_loss: 21.0653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 4/18 [17:53<1:05:39, 281.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 100, 'batch_size': 90, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 204,630\n",
      "Trainable params: 204,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7917.0576 - val_loss: 5727.0941\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6688.4674 - val_loss: 5253.9059\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 8162.6573 - val_loss: 6833.5072\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4823.3783 - val_loss: 2868.7583\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4041.7285 - val_loss: 3592.3795\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3955.9112 - val_loss: 1897.2364\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2343.9319 - val_loss: 1468.0526\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1575.9732 - val_loss: 504.7044\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1111.5059 - val_loss: 353.8286\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1078.6810 - val_loss: 3232.5844\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 858.8132 - val_loss: 559.2185\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 694.3349 - val_loss: 1110.7115\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 589.5616 - val_loss: 684.1809\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 535.7365 - val_loss: 535.2182\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 516.0719 - val_loss: 634.5314\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 457.8279 - val_loss: 531.1184\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 403.3728 - val_loss: 622.6137\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 427.0009 - val_loss: 466.1375\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 380.9429 - val_loss: 692.6755\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 375.3687 - val_loss: 431.3595\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 345.4199 - val_loss: 505.7618\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 334.5615 - val_loss: 510.6711\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 307.7861 - val_loss: 564.1634\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 305.1096 - val_loss: 521.3052\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 297.9787 - val_loss: 487.3576\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 286.0373 - val_loss: 468.6617\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 283.6670 - val_loss: 652.0627\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 264.3728 - val_loss: 712.5814\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 267.7506 - val_loss: 536.3426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 5/18 [19:07<47:31, 219.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 400, 'batch_size': 60, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,218,430\n",
      "Trainable params: 3,218,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 92690.7427 - val_loss: 220327.4472\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 5201757.0638 - val_loss: 6355209.0969\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 77140201.9413 - val_loss: 26526889.0724\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 24550545.9631 - val_loss: 5121045.7192\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 9431846.7768 - val_loss: 901319.2707\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 4499126.2953 - val_loss: 841027.9143\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 2231341.1380 - val_loss: 863760.6378\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1340245.7076 - val_loss: 101003.7581\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1125466.9253 - val_loss: 210426.5140\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 757229.6350 - val_loss: 249511.8763\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 243207.3946 - val_loss: 46152.4958\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 102256.0759 - val_loss: 24192.7988\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 62594.1444 - val_loss: 14775.8283\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 50332.7229 - val_loss: 9580.3967\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 38523.6810 - val_loss: 7232.9002\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 27485.4058 - val_loss: 5067.2503\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 21896.2471 - val_loss: 6285.7719\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 17360.7749 - val_loss: 3682.3438\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 12216.8663 - val_loss: 1271.0250\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 7968.4907 - val_loss: 1237.3185\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 5662.2027 - val_loss: 584.3314\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 3572.7492 - val_loss: 552.7366\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 2868.0829 - val_loss: 381.9718\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1917.4344 - val_loss: 226.8822\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1502.7620 - val_loss: 170.8808\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1127.7430 - val_loss: 105.7137\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1018.0028 - val_loss: 136.9492\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 820.6012 - val_loss: 140.2189\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 816.3345 - val_loss: 89.7984\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 668.3191 - val_loss: 71.8009\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 608.3407 - val_loss: 63.1979\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 597.7176 - val_loss: 79.7618\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 607.1878 - val_loss: 91.4470\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 565.9989 - val_loss: 79.5718\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 532.9365 - val_loss: 94.4762\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 520.6740 - val_loss: 60.3446\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 513.6372 - val_loss: 53.5661\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 499.6011 - val_loss: 60.5107\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 429.1356 - val_loss: 51.2313\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 423.9211 - val_loss: 71.8765\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 453.8244 - val_loss: 63.2152\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 402.7916 - val_loss: 73.5351\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 456.2097 - val_loss: 60.8017\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 395.1106 - val_loss: 87.4958\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 425.9853 - val_loss: 56.2240\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 404.6306 - val_loss: 67.2671\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 407.3797 - val_loss: 55.3395\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 396.4168 - val_loss: 53.1196\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 361.3995 - val_loss: 80.5596\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 417.7965 - val_loss: 80.7970\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 380.7487 - val_loss: 53.1229\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 402.9018 - val_loss: 64.7926\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 387.2814 - val_loss: 63.3542\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 354.7829 - val_loss: 56.8404\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 399.3810 - val_loss: 63.7486\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 382.9007 - val_loss: 63.9094\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 358.9584 - val_loss: 84.5576\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 399.2335 - val_loss: 61.9069\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 385.0774 - val_loss: 90.1448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 6/18 [28:28<1:04:20, 321.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 100, 'batch_size': 90, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 204,630\n",
      "Trainable params: 204,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 39937.6189 - val_loss: 112030.3994\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20136.1408 - val_loss: 17982.8765\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 9272.2567 - val_loss: 7924.2670\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7076.7990 - val_loss: 6025.7662\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5932.8083 - val_loss: 5796.5555\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4932.6848 - val_loss: 7568.7116\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3798.9787 - val_loss: 2683.0041\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2774.5841 - val_loss: 1323.7705\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2323.4910 - val_loss: 873.8438\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1913.0854 - val_loss: 708.7176\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1399.6532 - val_loss: 582.7834\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1153.8292 - val_loss: 692.8031\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 957.6316 - val_loss: 584.0204\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 893.9484 - val_loss: 317.2030\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 755.7025 - val_loss: 447.8420\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 708.7137 - val_loss: 376.8040\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 663.7529 - val_loss: 318.0272\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 597.2802 - val_loss: 356.8854\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 566.4566 - val_loss: 396.0487\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 543.3056 - val_loss: 353.3317\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 523.8810 - val_loss: 466.4406\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 527.7781 - val_loss: 601.4283\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 494.9196 - val_loss: 407.7528\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 517.8833 - val_loss: 616.9024\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 554.9755 - val_loss: 899.7738\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 685.1665 - val_loss: 422.9613\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1074.5634 - val_loss: 464.0614\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1037.3014 - val_loss: 895.1533\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1049.7077 - val_loss: 284.0355\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 780.2779 - val_loss: 327.6714\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 622.3475 - val_loss: 238.5005\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 503.0203 - val_loss: 476.3192\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 478.8327 - val_loss: 497.8609\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 465.0297 - val_loss: 506.7032\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 431.6534 - val_loss: 616.3861\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 399.9497 - val_loss: 545.1585\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 373.7668 - val_loss: 617.0657\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 367.5280 - val_loss: 774.9200\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 368.0839 - val_loss: 676.8249\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 376.3477 - val_loss: 592.8253\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 320.5162 - val_loss: 745.6667\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 358.3242 - val_loss: 584.6769\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 339.1073 - val_loss: 679.1968\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 322.0389 - val_loss: 632.4021\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 326.5090 - val_loss: 815.9289\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 304.2663 - val_loss: 906.1551\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 301.7250 - val_loss: 792.8386\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 283.9854 - val_loss: 808.1332\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 288.9019 - val_loss: 828.4842\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 308.1023 - val_loss: 967.2736\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 294.6589 - val_loss: 873.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 7/18 [30:44<48:45, 265.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 400, 'batch_size': 60, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,218,430\n",
      "Trainable params: 3,218,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 11s 10ms/step - loss: 283871.2545 - val_loss: 1268111.9154\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 23156055.2093 - val_loss: 31493497.8356\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 55934113.7852 - val_loss: 6442557.0157\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 22044959.4732 - val_loss: 3550777.6419\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 8204598.0856 - val_loss: 639708.5165\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 2382979.1858 - val_loss: 1754671.4114\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 2841625.6275 - val_loss: 336213.7971\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 1738719.4451 - val_loss: 372268.1262\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 1111248.1363 - val_loss: 642882.0767\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 1077379.8628 - val_loss: 176728.2833\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 477664.7112 - val_loss: 64887.9528\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 130728.7170 - val_loss: 24245.0608\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 64583.1882 - val_loss: 10469.4517\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 16014.4172 - val_loss: 3189.2025\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7899.9199 - val_loss: 1017.7229\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 5438.0323 - val_loss: 749.6303\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 3647.6583 - val_loss: 977.2648\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 2711.3021 - val_loss: 597.2409\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2021.4229 - val_loss: 543.5011\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 1651.7320 - val_loss: 680.8208\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 1486.7430 - val_loss: 372.9407\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 1232.7538 - val_loss: 372.1793\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 1046.3634 - val_loss: 650.1234\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 1004.8438 - val_loss: 364.8971\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 928.1398 - val_loss: 526.1823\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 710.4760 - val_loss: 789.4527\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 600.5138 - val_loss: 510.6743\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 568.9274 - val_loss: 674.8688\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 539.8206 - val_loss: 746.2838\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 506.3850 - val_loss: 698.6289\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 495.0133 - val_loss: 558.9206\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 472.3497 - val_loss: 727.3937\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 480.0056 - val_loss: 743.7936\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 429.1521 - val_loss: 555.1891\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 494.4997 - val_loss: 693.2951\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 479.3224 - val_loss: 500.8465\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 384.8136 - val_loss: 651.5852\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 424.1059 - val_loss: 563.4465\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 628.6719 - val_loss: 942.6212\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 543.9966 - val_loss: 471.4548\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 475.6048 - val_loss: 465.3060\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 384.9193 - val_loss: 616.2486\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 412.1037 - val_loss: 536.0781\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 415.8893 - val_loss: 469.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 8/18 [37:05<50:04, 300.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 200, 'batch_size': 60, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 809,230\n",
      "Trainable params: 809,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 45880.1407 - val_loss: 111080.3967\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 218007.8503 - val_loss: 480614.4407\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 314513.2988 - val_loss: 199441.1799\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 160434.8154 - val_loss: 103458.4692\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 581301.6010 - val_loss: 1217981.1673\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 551892.3532 - val_loss: 544546.3305\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 284554.5327 - val_loss: 32421.4560\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 105492.9843 - val_loss: 59845.1611\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 25825.1342 - val_loss: 12449.6130\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 4875.7892 - val_loss: 890.2170\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 468.5073 - val_loss: 210.1731\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 92.0001 - val_loss: 42.5991\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 34.3974 - val_loss: 32.1279\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 27.9896 - val_loss: 29.9471\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 27.2980 - val_loss: 29.7375\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 27.2090 - val_loss: 29.6730\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 27.2073 - val_loss: 29.6941\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 27.1109 - val_loss: 29.5833\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 27.1176 - val_loss: 29.6501\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 27.0369 - val_loss: 29.8020\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.9545 - val_loss: 29.3096\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.7669 - val_loss: 29.2906\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.7833 - val_loss: 29.1361\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.7279 - val_loss: 29.0938\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.4899 - val_loss: 28.9206\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.4696 - val_loss: 28.8097\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3881 - val_loss: 28.8253\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.4026 - val_loss: 28.7428\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3993 - val_loss: 28.9830\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3570 - val_loss: 28.7179\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3828 - val_loss: 28.8032\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3430 - val_loss: 29.2419\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 26.5800 - val_loss: 28.9271\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.3379 - val_loss: 28.8604\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.3800 - val_loss: 28.6731\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.2543 - val_loss: 28.6325\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.2588 - val_loss: 28.7118\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 26.2927 - val_loss: 28.8991\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.4484 - val_loss: 29.1524\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.3814 - val_loss: 28.6038\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.2939 - val_loss: 28.5783\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2251 - val_loss: 28.6176\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2125 - val_loss: 28.5968\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.2217 - val_loss: 28.5510\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1977 - val_loss: 28.6175\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2544 - val_loss: 28.5542\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3003 - val_loss: 28.6472\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3426 - val_loss: 28.7068\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2876 - val_loss: 28.5928\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2232 - val_loss: 28.5533\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2214 - val_loss: 28.5223\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.2093 - val_loss: 28.7416\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2178 - val_loss: 28.7441\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3976 - val_loss: 28.5686\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3829 - val_loss: 28.8918\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.4786 - val_loss: 28.5407\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2307 - val_loss: 28.6726\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1899 - val_loss: 28.5373\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1637 - val_loss: 28.4524\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1703 - val_loss: 28.5704\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3750 - val_loss: 28.4618\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2072 - val_loss: 28.6953\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.5317 - val_loss: 29.0099\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 6s 5ms/step - loss: 26.4553 - val_loss: 29.1972\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.2992 - val_loss: 28.6345\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 26.3670 - val_loss: 29.1362\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3686 - val_loss: 28.4269\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2137 - val_loss: 28.4097\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3392 - val_loss: 28.6461\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1647 - val_loss: 28.5585\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.2721 - val_loss: 28.5426\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3304 - val_loss: 28.6469\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.8847 - val_loss: 30.0063\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.6271 - val_loss: 28.4487\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 27.1310 - val_loss: 30.2389\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.4244 - val_loss: 28.3682\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1654 - val_loss: 28.3860\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1199 - val_loss: 28.7217\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1786 - val_loss: 28.6484\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.0798 - val_loss: 28.5317\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.1428 - val_loss: 28.3576\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2075 - val_loss: 28.4201\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2321 - val_loss: 28.5353\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3820 - val_loss: 28.3734\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.2054 - val_loss: 28.3504\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.6115 - val_loss: 28.4154\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2713 - val_loss: 28.2909\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3066 - val_loss: 28.3262\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.6489 - val_loss: 28.8902\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2652 - val_loss: 28.2393\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.3879 - val_loss: 28.8749\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2221 - val_loss: 28.2165\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.0677 - val_loss: 28.3491\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.0170 - val_loss: 28.6880\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2656 - val_loss: 30.0118\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.2931 - val_loss: 28.2002\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.0143 - val_loss: 28.2432\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.9860 - val_loss: 28.7835\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.0535 - val_loss: 28.5001\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 26.0551 - val_loss: 28.6494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 9/18 [44:32<51:41, 344.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 200, 'batch_size': 60, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 809,230\n",
      "Trainable params: 809,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 9567.7522 - val_loss: 12660.5955\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 41419.5947 - val_loss: 98622.0078\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 252639.0079 - val_loss: 143989.6089\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 272825.2828 - val_loss: 296094.7457\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 294488.8263 - val_loss: 344024.2306\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4762072.4909 - val_loss: 11474424.8121\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 147192791.9933 - val_loss: 32623422.3718\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 16577197.9983 - val_loss: 2188070.6710\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4559771.5545 - val_loss: 4564607.6013\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1952348.5524 - val_loss: 708135.0185\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 982620.7494 - val_loss: 324451.7274\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 580958.7308 - val_loss: 585934.6496\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 507020.9554 - val_loss: 516489.1804\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 428665.3430 - val_loss: 172615.6769\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 313805.5177 - val_loss: 86058.8229\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 263981.1096 - val_loss: 77384.1479\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 217707.6368 - val_loss: 74882.3412\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 188724.6633 - val_loss: 86649.0030\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 137238.5371 - val_loss: 43580.1722\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 149495.0576 - val_loss: 32102.9353\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 122392.2568 - val_loss: 75981.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 10/18 [46:21<36:31, 273.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 200, 'batch_size': 120, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 809,230\n",
      "Trainable params: 809,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 87842.4321 - val_loss: 91122.5194\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 74555.9575 - val_loss: 41126.9294\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 32893.1074 - val_loss: 93408.1751\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 139890.2722 - val_loss: 637647.8485\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1114927.1581 - val_loss: 2668542.4697\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5901824.5822 - val_loss: 3098032.9873\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 3235089.5923 - val_loss: 1827339.3048\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3966661.3859 - val_loss: 7487840.6399\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6131503.6376 - val_loss: 4539023.5039\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6556870.3993 - val_loss: 2195842.1771\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6592527.7651 - val_loss: 1310904.8253\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5301166.1812 - val_loss: 3489836.7480\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 11877881.7550 - val_loss: 16233496.7280\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 10721845.4631 - val_loss: 11218588.3953\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 8102513.6040 - val_loss: 938156.3914\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 2388236.0529 - val_loss: 443471.0971\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1592987.1921 - val_loss: 419744.3422\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1394348.9933 - val_loss: 350406.2671\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1213988.3297 - val_loss: 271268.0054\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1025957.8343 - val_loss: 175135.4354\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 887150.7945 - val_loss: 143790.6294\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 757975.8691 - val_loss: 119827.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 11/18 [47:50<25:28, 218.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 200, 'batch_size': 120, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 809,230\n",
      "Trainable params: 809,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 12481.7925 - val_loss: 9858.3399\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 12597.2669 - val_loss: 12267.6418\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 14361.0620 - val_loss: 20985.6363\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 12986.2497 - val_loss: 23323.9901\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 155090.1359 - val_loss: 141202.4588\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 307272.9048 - val_loss: 320044.7051\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 176601.9010 - val_loss: 97458.8075\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 71132.6182 - val_loss: 19776.2011\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14848.7887 - val_loss: 9069.1968\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16505.4253 - val_loss: 11781.4919\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7394.3739 - val_loss: 3865.2360\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2905.4816 - val_loss: 800.8199\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 912.8828 - val_loss: 564.5080\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 447.8502 - val_loss: 394.8034\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 304.1476 - val_loss: 910.8083\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1158.6423 - val_loss: 785.0673\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 505.9106 - val_loss: 333.7092\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 292.1507 - val_loss: 233.8014\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 219.8777 - val_loss: 186.8120\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 182.4056 - val_loss: 175.2547\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 168.2619 - val_loss: 160.3257\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 157.4325 - val_loss: 151.5630\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 221.8727 - val_loss: 285.2221\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 268.6864 - val_loss: 195.0397\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 171.7015 - val_loss: 160.5182\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 145.3834 - val_loss: 130.9622\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 130.3140 - val_loss: 119.2614\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 115.9928 - val_loss: 109.5048\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 107.4437 - val_loss: 105.5444\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 113.0540 - val_loss: 107.1468\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 106.8226 - val_loss: 100.5914\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 97.6903 - val_loss: 92.5015\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 82.6933 - val_loss: 74.7823\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 70.6360 - val_loss: 70.4905\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 67.6307 - val_loss: 65.8215\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 62.2107 - val_loss: 61.6931\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 58.2257 - val_loss: 58.3019\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 55.4714 - val_loss: 56.1854\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 53.2545 - val_loss: 53.1817\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 50.1801 - val_loss: 50.9114\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 47.8866 - val_loss: 48.5047\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 45.0168 - val_loss: 46.0898\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 42.1538 - val_loss: 43.9001\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 40.7158 - val_loss: 42.6038\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 39.8004 - val_loss: 41.9314\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 39.2727 - val_loss: 43.0144\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 44.6719 - val_loss: 52.3610\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 41.9584 - val_loss: 44.8311\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 38.7659 - val_loss: 41.7419\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 38.1199 - val_loss: 41.6902\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 37.9024 - val_loss: 41.9488\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 37.1487 - val_loss: 39.9762\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 37.2825 - val_loss: 42.1335\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 41.2712 - val_loss: 46.2161\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 42.3473 - val_loss: 51.1050\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 45.0686 - val_loss: 47.1383\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 46.5271 - val_loss: 46.4958\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 41.5710 - val_loss: 45.0953\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 37.9730 - val_loss: 37.7101\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 36.3270 - val_loss: 39.9847\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 36.2409 - val_loss: 38.5124\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 35.9929 - val_loss: 38.0415\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 35.5477 - val_loss: 37.8185\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 4s 3ms/step - loss: 50.6680 - val_loss: 47.4682\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 51.5427 - val_loss: 42.5554\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 44.2846 - val_loss: 47.2594\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 42.6475 - val_loss: 39.4680\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 37.5585 - val_loss: 38.6784\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 35.1612 - val_loss: 36.8225\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 36.2261 - val_loss: 39.2355\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 38.4855 - val_loss: 42.8962\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 35.9199 - val_loss: 35.4730\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 33.8618 - val_loss: 35.0423\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 38.6914 - val_loss: 42.0003\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 36.5293 - val_loss: 40.8242\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 34.9136 - val_loss: 34.9190\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 32.2660 - val_loss: 34.4603\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 31.6214 - val_loss: 36.5846\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 31.8091 - val_loss: 33.7322\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 31.1915 - val_loss: 37.0510\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 37.0919 - val_loss: 48.7861\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 38.4862 - val_loss: 38.6925\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 34.9266 - val_loss: 34.5033\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 47.3880 - val_loss: 54.9728\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 40.8904 - val_loss: 37.9466\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 33.9636 - val_loss: 38.5858\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 36.0742 - val_loss: 44.3130\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 36.1041 - val_loss: 38.3278\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 33.8982 - val_loss: 33.7151\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 31.5707 - val_loss: 33.2318\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 31.1824 - val_loss: 45.6324\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 50.6427 - val_loss: 60.1748\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 51.1207 - val_loss: 44.8010\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 38.2222 - val_loss: 35.8711\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 32.7239 - val_loss: 33.6910\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 30.5884 - val_loss: 34.3182\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 30.7936 - val_loss: 34.4920\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 30.3807 - val_loss: 32.1399\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 30.0541 - val_loss: 31.4202\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 30.6635 - val_loss: 33.5830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 12/18 [54:02<26:26, 264.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 100, 'batch_size': 120, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 204,630\n",
      "Trainable params: 204,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7147.0240 - val_loss: 4625.4783\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4643.3120 - val_loss: 10611.7490\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6913.0121 - val_loss: 9325.9213\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6866.1257 - val_loss: 14490.0060\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 13123.5477 - val_loss: 19114.1036\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12448.6732 - val_loss: 8322.1175\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4702.1700 - val_loss: 2794.6839\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2696.5690 - val_loss: 1407.7332\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1882.1852 - val_loss: 833.2984\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1253.2881 - val_loss: 573.7179\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 969.4284 - val_loss: 432.8095\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1211.6443 - val_loss: 305.0708\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1476.7735 - val_loss: 1809.9106\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1327.2653 - val_loss: 1342.2839\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1085.8498 - val_loss: 1199.5632\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 922.7246 - val_loss: 1301.6451\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 892.7771 - val_loss: 890.1824\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 871.9248 - val_loss: 697.0564\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 795.7703 - val_loss: 813.6813\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 742.1160 - val_loss: 764.3405\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 9936.1035 - val_loss: 87547.7972\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 251150.3743 - val_loss: 350017.7657\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 150929.4398 - val_loss: 741521.9993\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 179194.9417 - val_loss: 8755.0836\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 15343.6362 - val_loss: 8086.6280\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 8836.3293 - val_loss: 7501.7598\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7000.4987 - val_loss: 7110.9076\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6094.1855 - val_loss: 5941.2473\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5111.5093 - val_loss: 4599.0389\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4326.6213 - val_loss: 4137.0036\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3490.4039 - val_loss: 2797.2812\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3166.0167 - val_loss: 2595.2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 13/18 [55:23<17:27, 209.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 100, 'batch_size': 30, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 204,630\n",
      "Trainable params: 204,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 48467.2327 - val_loss: 69786.8960\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 106048.7189 - val_loss: 74524.7636\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 89462.2787 - val_loss: 625359.0024\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36294792.0836 - val_loss: 96849655.1859\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 838492905.4362 - val_loss: 64536592.0939\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 169811229.5235 - val_loss: 21613479.1429\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24643650.2324 - val_loss: 91254.5643\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 242359.6351 - val_loss: 28789.9199\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 166086.6535 - val_loss: 19036.7023\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 155733.8709 - val_loss: 203708.8153\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 161947.3866 - val_loss: 43381.2757\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 170115.3074 - val_loss: 28802.1502\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 99802.5926 - val_loss: 18149.0971\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 87485.0318 - val_loss: 15310.1577\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 77106.7626 - val_loss: 10902.5788\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 66291.1356 - val_loss: 9365.2798\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 59423.5767 - val_loss: 8410.4714\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 52596.0614 - val_loss: 6939.4072\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 47796.6774 - val_loss: 6179.4701\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38648.5965 - val_loss: 5827.7063\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 34215.3354 - val_loss: 4707.5694\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 20685.8555 - val_loss: 1903.1163\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4111.7977 - val_loss: 233.8277\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1726.3428 - val_loss: 156.8955\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1685.4617 - val_loss: 159.7677\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1412.7704 - val_loss: 174.7041\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1107.7568 - val_loss: 84.1181\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1625.2731 - val_loss: 261.0238\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1791.6103 - val_loss: 173.9010\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2148.2078 - val_loss: 339.7281\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1423.2591 - val_loss: 159.4146\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1232.2583 - val_loss: 134.9658\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1145.0210 - val_loss: 67.1437\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 2583.5381 - val_loss: 188.7013\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1104.0120 - val_loss: 109.4386\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1285.6619 - val_loss: 237.9511\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1084.9611 - val_loss: 81.3315\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 894.1255 - val_loss: 127.5219\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1187.3370 - val_loss: 77.2386\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1141.4656 - val_loss: 64.6634\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 948.1615 - val_loss: 218.7526\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 917.3413 - val_loss: 92.8516\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 968.0788 - val_loss: 335.5821\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 847.8213 - val_loss: 221.0769\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 869.3409 - val_loss: 195.5794\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1046.2093 - val_loss: 179.8534\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 851.7491 - val_loss: 76.5406\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 789.7258 - val_loss: 97.5477\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 869.2932 - val_loss: 134.6963\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 751.5927 - val_loss: 80.6007\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 984.3182 - val_loss: 79.8360\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 985.5397 - val_loss: 107.8886\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 899.5036 - val_loss: 179.6456\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 961.9427 - val_loss: 141.9187\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 905.5588 - val_loss: 88.2194\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 935.4565 - val_loss: 135.9613\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 894.4210 - val_loss: 62.8342\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 836.5408 - val_loss: 165.6265\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 953.9569 - val_loss: 96.8550\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 943.7178 - val_loss: 50.4082\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1005.6614 - val_loss: 154.8423\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 732.3518 - val_loss: 91.9000\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 5s 4ms/step - loss: 927.8575 - val_loss: 151.2286\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 791.1205 - val_loss: 101.5364\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 872.7437 - val_loss: 119.4891\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 917.3524 - val_loss: 75.8046\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1048.7003 - val_loss: 78.3636\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 843.8801 - val_loss: 60.4882\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 825.3505 - val_loss: 107.2501\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 960.9212 - val_loss: 90.2958\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 783.5351 - val_loss: 106.6483\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 788.9980 - val_loss: 283.4423\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 810.8132 - val_loss: 46.8635\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 862.4033 - val_loss: 143.3827\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 855.5352 - val_loss: 170.3090\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 737.0939 - val_loss: 242.4607\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 896.8569 - val_loss: 153.9024\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 953.3560 - val_loss: 135.4903\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 807.6131 - val_loss: 120.9733\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 828.8581 - val_loss: 127.5004\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 824.3065 - val_loss: 279.1679\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1011.8786 - val_loss: 228.7906\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 947.8393 - val_loss: 212.5968\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 826.7416 - val_loss: 144.3107\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 766.3204 - val_loss: 161.2209\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 911.1194 - val_loss: 232.6085\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1049.9303 - val_loss: 194.8669\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 946.7399 - val_loss: 180.3210\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 720.5477 - val_loss: 271.9417\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 938.3114 - val_loss: 250.3533\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 985.7562 - val_loss: 179.7449\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 845.9966 - val_loss: 124.3419\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 726.0686 - val_loss: 150.8708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 14/18 [1:03:14<19:11, 287.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 400, 'batch_size': 90, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,218,430\n",
      "Trainable params: 3,218,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 45385.9627 - val_loss: 2396534.8728\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 37976272.6846 - val_loss: 106376748.8689\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 529028577.4631 - val_loss: 859069741.3386\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1330444346.8456 - val_loss: 1864780442.5519\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1285294746.6309 - val_loss: 311383113.4560\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 442510033.1812 - val_loss: 39389561.6673\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 224937021.7718 - val_loss: 37681230.4031\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 105470859.0738 - val_loss: 44249385.5108\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 65106925.7315 - val_loss: 19857268.4932\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 51656271.5268 - val_loss: 5082842.5558\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 24748108.6544 - val_loss: 1553594.4887\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 5607922.1921 - val_loss: 1053348.7868\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 4711797.4228 - val_loss: 545300.4227\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1841364.0033 - val_loss: 146611.6713\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 488110.4736 - val_loss: 46953.5354\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 234924.7528 - val_loss: 103717.4884\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 57466.2091 - val_loss: 96829.7834\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 322561.9559 - val_loss: 1192438.7874\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 418293.0857 - val_loss: 125658.4146\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 133452.7983 - val_loss: 21365.7539\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 11s 10ms/step - loss: 52040.3047 - val_loss: 18562.5888\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 28885.5390 - val_loss: 5328.0746\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 17456.5920 - val_loss: 3454.4175\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 10774.7914 - val_loss: 3096.8198\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 7814.3404 - val_loss: 2429.9692\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 6107.1302 - val_loss: 1951.5194\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6169.4599 - val_loss: 1369.1250\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 5850.7980 - val_loss: 1744.8595\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 5095.9929 - val_loss: 1609.5307\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 4686.7873 - val_loss: 1193.9687\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 4674.7115 - val_loss: 1122.6875\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 4115.9759 - val_loss: 1113.2078\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 3790.3808 - val_loss: 1082.8341\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 3380.7768 - val_loss: 1010.1351\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 3586.5640 - val_loss: 1104.5985\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 3465.1017 - val_loss: 747.4356\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 3483.4498 - val_loss: 844.9786\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 3061.3969 - val_loss: 1088.3503\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 2772.1923 - val_loss: 1046.9037\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2585.6476 - val_loss: 1116.4035\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2790.5158 - val_loss: 1051.9701\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2881.6873 - val_loss: 1111.7835\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 2754.1172 - val_loss: 1377.8860\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2633.0906 - val_loss: 1249.2772\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2516.5286 - val_loss: 1269.2460\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2348.1228 - val_loss: 1128.2945\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2187.9483 - val_loss: 1266.0389\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2377.2202 - val_loss: 1338.9207\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2409.4844 - val_loss: 1825.6563\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2486.1625 - val_loss: 1263.6026\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 2070.1985 - val_loss: 1327.8889\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2218.1450 - val_loss: 1453.4471\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1836.5640 - val_loss: 1510.3217\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1590.0682 - val_loss: 1514.4946\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1835.6782 - val_loss: 1384.5722\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1569.7775 - val_loss: 1846.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 15/18 [1:12:29<18:24, 368.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 100, 'batch_size': 120, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 204,630\n",
      "Trainable params: 204,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 8085.5587 - val_loss: 5705.7586\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10117.8217 - val_loss: 7083.2717\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7164.3416 - val_loss: 4153.3543\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5071.3059 - val_loss: 3562.8503\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3631.9115 - val_loss: 4069.3776\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 24620.8358 - val_loss: 18994.2528\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 64286.3586 - val_loss: 304974.8100\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 253802.2449 - val_loss: 68567.3430\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 50121.7397 - val_loss: 75757.5631\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 45839.1011 - val_loss: 25889.1825\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 26082.9288 - val_loss: 16411.7916\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21653.2214 - val_loss: 16782.5016\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 56606.4734 - val_loss: 355708.0898\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 356126.6267 - val_loss: 50986.5007\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 147483.7692 - val_loss: 107500.2038\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 108832.2657 - val_loss: 80572.2371\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 198052.8979 - val_loss: 151283.2922\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 91460.3234 - val_loss: 55372.2346\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 71922.9261 - val_loss: 30671.2658\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21828.9947 - val_loss: 12992.2725\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7909.1670 - val_loss: 3550.6850\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1891.9188 - val_loss: 1023.0584\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 804.9717 - val_loss: 743.4292\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 679.5693 - val_loss: 621.3859\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 616.0355 - val_loss: 625.1897\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 447.6585 - val_loss: 359.3861\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 317.0752 - val_loss: 397.8839\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 307.0839 - val_loss: 302.2235\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 270.4898 - val_loss: 240.1300\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 213.7114 - val_loss: 200.1421\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 225.5805 - val_loss: 268.4161\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 237.6852 - val_loss: 274.0236\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 252.8001 - val_loss: 236.8547\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 235.1662 - val_loss: 237.7127\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 311.4010 - val_loss: 309.0119\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 297.0820 - val_loss: 262.0655\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 226.6134 - val_loss: 207.8416\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 221.3502 - val_loss: 199.1752\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 176.4169 - val_loss: 178.9230\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 169.5287 - val_loss: 174.6201\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 162.8167 - val_loss: 160.4661\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 170.3987 - val_loss: 235.1763\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 233.9984 - val_loss: 244.5323\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 250.2954 - val_loss: 280.5152\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 243.7070 - val_loss: 242.7408\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 229.5690 - val_loss: 246.5572\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 240.8567 - val_loss: 233.8793\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 215.0636 - val_loss: 199.7753\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 212.0432 - val_loss: 201.8922\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 218.7711 - val_loss: 237.0982\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 222.2436 - val_loss: 217.8650\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 198.3259 - val_loss: 195.9908\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 182.8335 - val_loss: 191.1747\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 167.7357 - val_loss: 156.5177\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 153.8363 - val_loss: 138.0564\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 161.3839 - val_loss: 178.8104\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 183.4787 - val_loss: 215.5679\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 256.7989 - val_loss: 352.2933\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 349.5114 - val_loss: 225.1540\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 134.9711 - val_loss: 101.6763\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 87.9870 - val_loss: 77.0635\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 69.0139 - val_loss: 63.5128\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 2s 1ms/step - loss: 58.2744 - val_loss: 59.6348\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 55.5366 - val_loss: 57.3307\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 53.9620 - val_loss: 56.0613\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 53.1222 - val_loss: 56.0248\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 52.5427 - val_loss: 55.5385\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 52.1685 - val_loss: 55.2396\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 51.7924 - val_loss: 54.9889\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 51.6642 - val_loss: 54.6407\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 51.3086 - val_loss: 53.3169\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 50.2718 - val_loss: 52.6770\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 50.6272 - val_loss: 53.9078\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 50.7915 - val_loss: 54.8442\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 50.5357 - val_loss: 54.6195\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 50.1332 - val_loss: 54.5529\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 49.8960 - val_loss: 53.8354\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 49.6168 - val_loss: 54.3364\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 49.2827 - val_loss: 53.1191\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 49.1470 - val_loss: 52.5868\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 48.6628 - val_loss: 52.6210\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 48.0773 - val_loss: 51.5492\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 48.2248 - val_loss: 51.2016\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 48.1028 - val_loss: 50.4479\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 48.3662 - val_loss: 50.6009\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 48.2878 - val_loss: 51.0770\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 48.0735 - val_loss: 51.1551\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 48.0059 - val_loss: 52.1385\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 47.7292 - val_loss: 52.4752\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 47.5209 - val_loss: 52.3119\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 47.5763 - val_loss: 51.7315\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 47.2908 - val_loss: 49.1960\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 47.1159 - val_loss: 48.8206\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 46.9943 - val_loss: 48.5458\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 46.8087 - val_loss: 48.2671\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 46.6000 - val_loss: 48.0535\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 46.3081 - val_loss: 47.6907\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 46.0083 - val_loss: 47.6402\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 45.8340 - val_loss: 48.1745\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 45.6573 - val_loss: 48.7044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 16/18 [1:15:46<10:33, 316.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 200, 'batch_size': 120, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 809,230\n",
      "Trainable params: 809,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6847.1718 - val_loss: 4403.0071\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4939.4115 - val_loss: 6852.4960\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6510.7341 - val_loss: 5896.6948\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 8748.0229 - val_loss: 4156.8109\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5027.5815 - val_loss: 8299.2458\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4855.6833 - val_loss: 3670.5130\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4370.4121 - val_loss: 5617.2948\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5808.1974 - val_loss: 4233.0755\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5377.1725 - val_loss: 8087.0284\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5330.9669 - val_loss: 4913.8398\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3506.9215 - val_loss: 2702.0661\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2396.5498 - val_loss: 3740.4430\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1722.1539 - val_loss: 2063.6610\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1390.8729 - val_loss: 2266.7645\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1183.8985 - val_loss: 2434.4006\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 992.9685 - val_loss: 1441.7859\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 963.4427 - val_loss: 1512.0043\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 929.4036 - val_loss: 1057.5422\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 814.9218 - val_loss: 1193.1298\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 719.9499 - val_loss: 1176.4626\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 697.2358 - val_loss: 1110.8985\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 685.0220 - val_loss: 779.8335\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 666.9987 - val_loss: 989.7993\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 602.7634 - val_loss: 658.3723\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 539.3708 - val_loss: 651.2826\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 547.9183 - val_loss: 659.4774\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 510.8415 - val_loss: 598.0385\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 510.8167 - val_loss: 528.3470\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 513.6523 - val_loss: 906.9642\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 484.2892 - val_loss: 360.5037\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 432.1454 - val_loss: 426.5047\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 457.1688 - val_loss: 426.9449\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 410.8709 - val_loss: 427.2038\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 403.6287 - val_loss: 357.5531\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 429.1859 - val_loss: 420.9555\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 392.9268 - val_loss: 757.1467\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 397.9599 - val_loss: 559.8112\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 358.4824 - val_loss: 593.8331\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 357.8709 - val_loss: 511.5948\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 330.1896 - val_loss: 695.2474\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 355.5319 - val_loss: 525.5300\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 335.1471 - val_loss: 344.9722\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 300.0477 - val_loss: 502.1100\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 289.6897 - val_loss: 516.7331\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 271.9909 - val_loss: 300.7027\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 300.3695 - val_loss: 427.0558\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 298.7952 - val_loss: 633.2949\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 267.5035 - val_loss: 496.9175\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 246.6430 - val_loss: 314.5314\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 277.2500 - val_loss: 423.9824\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 248.2035 - val_loss: 593.5878\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 222.5349 - val_loss: 545.6077\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 224.9483 - val_loss: 596.5903\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 247.4106 - val_loss: 458.3544\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 229.0614 - val_loss: 491.0568\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 226.6879 - val_loss: 600.9340\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 210.3803 - val_loss: 646.0393\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 230.6710 - val_loss: 614.5961\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 233.2509 - val_loss: 632.6195\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 213.0128 - val_loss: 616.8016\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 215.6885 - val_loss: 566.0428\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 251.0561 - val_loss: 610.9406\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 2s 2ms/step - loss: 219.6733 - val_loss: 471.6683\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 225.9546 - val_loss: 368.7301\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 217.0810 - val_loss: 406.4689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 17/18 [1:18:30<04:30, 270.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_n': 200, 'batch_size': 90, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 809,230\n",
      "Trainable params: 809,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 8087.9184 - val_loss: 10434.5095\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 16305.3380 - val_loss: 37310.8204\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 84026.9910 - val_loss: 1565511.2603\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1107500.3920 - val_loss: 10531899.8258\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 31597403.6116 - val_loss: 18221043.3386\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 35723755.5738 - val_loss: 48955769.5734\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47615719.2383 - val_loss: 5789051.1438\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 17312232.0805 - val_loss: 12295129.9941\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 10436504.7282 - val_loss: 3166910.6634\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 8012003.1518 - val_loss: 4708589.2965\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5473972.6720 - val_loss: 1947066.8124\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2515280.0195 - val_loss: 470665.4154\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1161568.0908 - val_loss: 400132.7032\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 588953.8054 - val_loss: 1091498.4500\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 421488.8708 - val_loss: 77372.3699\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 327012.8064 - val_loss: 55773.4784\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 265495.8404 - val_loss: 49114.3323\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 220019.4945 - val_loss: 74595.4966\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 190715.8978 - val_loss: 73520.8685\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 168481.9544 - val_loss: 63700.3768\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 172578.8523 - val_loss: 116807.2819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [1:19:32<00:00, 265.13s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "\n",
    "scan_object = talos.Scan(X_train,\n",
    "                         y_train, \n",
    "                         params=p,\n",
    "                         model=LSTM_model,\n",
    "                         experiment_name='LSTM',\n",
    "                         fraction_limit=0.5,\n",
    "                         print_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug (link below) in the Talos library that means it sorts the columns and values from parameter dictionary incorrectly in the ```talos.Analze``` object. That is why each hyperparameter has unique values so I can easily understand the results.\n",
    "\n",
    "[https://github.com/autonomio/talos/issues/439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>LSTM_dropout</th>\n",
       "      <th>LSTM_n</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>2.106531e+01</td>\n",
       "      <td>2.030146e+01</td>\n",
       "      <td>400</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>2.864940e+01</td>\n",
       "      <td>2.605506e+01</td>\n",
       "      <td>200</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>3.358297e+01</td>\n",
       "      <td>3.066354e+01</td>\n",
       "      <td>200</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100</td>\n",
       "      <td>4.870437e+01</td>\n",
       "      <td>4.565728e+01</td>\n",
       "      <td>100</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59</td>\n",
       "      <td>9.014478e+01</td>\n",
       "      <td>3.850774e+02</td>\n",
       "      <td>400</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>93</td>\n",
       "      <td>1.508708e+02</td>\n",
       "      <td>7.260686e+02</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>65</td>\n",
       "      <td>4.064689e+02</td>\n",
       "      <td>2.170810e+02</td>\n",
       "      <td>200</td>\n",
       "      <td>120</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>4.691741e+02</td>\n",
       "      <td>4.158893e+02</td>\n",
       "      <td>400</td>\n",
       "      <td>60</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>5.363426e+02</td>\n",
       "      <td>2.677506e+02</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>6.770939e+02</td>\n",
       "      <td>9.969469e+02</td>\n",
       "      <td>400</td>\n",
       "      <td>120</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51</td>\n",
       "      <td>8.737528e+02</td>\n",
       "      <td>2.946589e+02</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56</td>\n",
       "      <td>1.846025e+03</td>\n",
       "      <td>1.569778e+03</td>\n",
       "      <td>400</td>\n",
       "      <td>90</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>2.595271e+03</td>\n",
       "      <td>3.166017e+03</td>\n",
       "      <td>100</td>\n",
       "      <td>120</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>8.067259e+03</td>\n",
       "      <td>7.892072e+03</td>\n",
       "      <td>200</td>\n",
       "      <td>90</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>7.598191e+04</td>\n",
       "      <td>1.223923e+05</td>\n",
       "      <td>200</td>\n",
       "      <td>60</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21</td>\n",
       "      <td>1.168073e+05</td>\n",
       "      <td>1.725789e+05</td>\n",
       "      <td>200</td>\n",
       "      <td>90</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22</td>\n",
       "      <td>1.198270e+05</td>\n",
       "      <td>7.579759e+05</td>\n",
       "      <td>200</td>\n",
       "      <td>120</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>1.028011e+06</td>\n",
       "      <td>3.287724e+06</td>\n",
       "      <td>400</td>\n",
       "      <td>120</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs      val_loss          loss  LSTM_dropout  LSTM_n  batch_size\n",
       "3            100  2.106531e+01  2.030146e+01           400     120         0.0\n",
       "8            100  2.864940e+01  2.605506e+01           200      60         0.0\n",
       "11           100  3.358297e+01  3.066354e+01           200     120         0.0\n",
       "15           100  4.870437e+01  4.565728e+01           100     120         0.0\n",
       "5             59  9.014478e+01  3.850774e+02           400      60         0.1\n",
       "13            93  1.508708e+02  7.260686e+02           100      30         0.1\n",
       "16            65  4.064689e+02  2.170810e+02           200     120         0.2\n",
       "7             44  4.691741e+02  4.158893e+02           400      60         0.2\n",
       "4             29  5.363426e+02  2.677506e+02           100      90         0.1\n",
       "1             46  6.770939e+02  9.969469e+02           400     120         0.2\n",
       "6             51  8.737528e+02  2.946589e+02           100      90         0.2\n",
       "14            56  1.846025e+03  1.569778e+03           400      90         0.2\n",
       "12            32  2.595271e+03  3.166017e+03           100     120         0.1\n",
       "0             67  8.067259e+03  7.892072e+03           200      90         0.0\n",
       "9             21  7.598191e+04  1.223923e+05           200      60         0.1\n",
       "17            21  1.168073e+05  1.725789e+05           200      90         0.2\n",
       "10            22  1.198270e+05  7.579759e+05           200     120         0.1\n",
       "2             21  1.028011e+06  3.287724e+06           400     120         0.1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_object = talos.Analyze(scan_object)\n",
    "df = analyze_object.data\n",
    "df.sort_values(by=['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "* The top 4 results here are promising. They all achieve a val_loss and loss of < 100. Their architectures feature:\n",
    "  * LSTM layer size of 200 - 400\n",
    "  * No dropout on LSTMs\n",
    "  * High batch sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
