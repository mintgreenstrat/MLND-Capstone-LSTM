{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS Optimization - FIVE\n",
    "\n",
    "An implementation of a hyperparameter grid search using the Talos library.\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jacobscottanthony/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check what hardware is available to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9614274374224652803\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 18147710963669015094\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5031, 16)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    keep_col = list(range(1,18))\n",
    "\n",
    "    df_day = pd.read_csv('ECL_Clean_Day.csv', \n",
    "                     infer_datetime_format=True,\n",
    "                     parse_dates=['Timestamp'], \n",
    "                     index_col=['Timestamp'],\n",
    "                     usecols = keep_col,\n",
    "                     date_parser=lambda col: pd.to_datetime(col, utc=True).tz_convert('America/New_York'))\n",
    "\n",
    "    #df_min.dtypes\n",
    "    print (df_day.shape)\n",
    "    df_day.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 10 years of data from 2019 - 2010\n",
    "day_data = df_day[df_day.index >= '2010-01-01']\n",
    "\n",
    "#split 7 years train, 3 years test\n",
    "day_train = day_data[day_data.index <= '2017-01-01']\n",
    "day_test = day_data[day_data.index >= '2017-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = day_train['Close'] \n",
    "base_test = day_test['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(base_test)\n",
    "plt.plot(base_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Simple Vanilla Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1703, 30) (1703, 30) (695, 30) (695, 30)\n"
     ]
    }
   ],
   "source": [
    "# Multi-step data preparation\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# define input sequence\n",
    "#raw_seq = list(base_test[:100].values)\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 30, 30\n",
    "# split into samples\n",
    "X_train, y_train = split_sequence(base_train.values, n_steps_in, n_steps_out)\n",
    "X_test, y_test = split_sequence(base_test.values, n_steps_in, n_steps_out) #length must be > n_in + n_out\n",
    "# summarize the data\n",
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 43,830\n",
      "Trainable params: 43,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/jacobscottanthony/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1703 samples, validate on 695 samples\n",
      "Epoch 1/20\n",
      "1703/1703 [==============================] - 3s 2ms/step - loss: 3093.6669 - val_loss: 5058.6756\n",
      "Epoch 2/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 650.7626 - val_loss: 694.1269\n",
      "Epoch 3/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 265.6385 - val_loss: 267.4794\n",
      "Epoch 4/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 93.7279 - val_loss: 289.7618\n",
      "Epoch 5/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 39.3132 - val_loss: 259.8009\n",
      "Epoch 6/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 33.5372 - val_loss: 195.7178\n",
      "Epoch 7/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 30.6333 - val_loss: 195.2754\n",
      "Epoch 8/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 28.2415 - val_loss: 201.7942\n",
      "Epoch 9/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 26.3319 - val_loss: 196.9452\n",
      "Epoch 10/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 25.0413 - val_loss: 199.0951\n",
      "Epoch 11/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 24.6257 - val_loss: 171.4928\n",
      "Epoch 12/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 23.0540 - val_loss: 171.7984\n",
      "Epoch 13/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 22.7362 - val_loss: 151.5324\n",
      "Epoch 14/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.7822 - val_loss: 150.1065\n",
      "Epoch 15/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.3263 - val_loss: 135.9062\n",
      "Epoch 16/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.0721 - val_loss: 130.3624\n",
      "Epoch 17/20\n",
      "1703/1703 [==============================] - 2s 992us/step - loss: 20.2335 - val_loss: 121.3672\n",
      "Epoch 18/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 19.6043 - val_loss: 110.7109\n",
      "Epoch 19/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 19.6437 - val_loss: 111.0684\n",
      "Epoch 20/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 19.4928 - val_loss: 112.7279\n"
     ]
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "#print (X_train.shape)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "#print (X_test.shape)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# fit model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for grid search\n",
    "\n",
    "p = {'LSTM_n' : [100, 200, 400],\n",
    "    'LSTM_dropout' : [0, 0.1, 0.2],\n",
    "    'dense_n' : [101, 201, 401],\n",
    "    'dense_dropout' : [0, 0.11, 0.22],\n",
    "    'batch_size' : [30, 60, 90, 120],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train, y_train, X_test, y_test, params):\n",
    "    model = Sequential()                            \n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu'))\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(Dense(params['dense_n']))\n",
    "    model.add(Dropout(params['dense_dropout']))\n",
    "    model.add(Dense(params['dense_n']))\n",
    "    model.add(Dropout(params['dense_dropout']))\n",
    "    model.add(Dense(params['dense_n']))\n",
    "    model.add(Dropout(params['dense_dropout']))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='Adam', \n",
    "                  loss='mse')\n",
    "    print (model.summary())\n",
    "    \n",
    "    #Early Stopping to avoid wasting time on bad hyperparams\n",
    "    es = EarlyStopping(monitor='val_loss', mode='auto', patience=20)\n",
    "\n",
    "    out = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    callbacks=[es],\n",
    "                    validation_data=[X_test, y_test])\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0, 'batch_size': 60, 'dense_dropout': 0.22, 'dense_n': 401, 'LSTM_n': 200}\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 401)               80601     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 401)               161202    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 401)               161202    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                12060     \n",
      "=================================================================\n",
      "Total params: 1,218,265\n",
      "Trainable params: 1,218,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3728.7925 - val_loss: 3972.7108\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14835.5276 - val_loss: 30973.0255\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 34457.7060 - val_loss: 2950.9216\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 5733.5363 - val_loss: 2083.5787\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1568.2861 - val_loss: 135.8386\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 561.9414 - val_loss: 40.8686\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 434.9616 - val_loss: 90.3619\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 419.3758 - val_loss: 28.4611\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 470.2043 - val_loss: 432.6280\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 459.1778 - val_loss: 43.7508\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1050.1208 - val_loss: 99.2185\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 472.8041 - val_loss: 62.8943\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 344.6678 - val_loss: 39.5048\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 299.7618 - val_loss: 30.7288\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 269.8046 - val_loss: 22.6610\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 253.7989 - val_loss: 19.8429\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 247.8242 - val_loss: 19.9929\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 238.3191 - val_loss: 30.2997\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 232.2004 - val_loss: 19.8652\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 219.4532 - val_loss: 41.7565\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 244.0494 - val_loss: 21.8719\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 212.8159 - val_loss: 19.5514\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 206.7439 - val_loss: 19.2765\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 191.5248 - val_loss: 18.4254\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 193.4753 - val_loss: 25.1793\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 184.7530 - val_loss: 18.0611\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 179.1575 - val_loss: 17.6294\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 176.7658 - val_loss: 22.9827\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 733250.4445 - val_loss: 266382.7405\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3673384.0447 - val_loss: 344706.0352\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4120191.7054 - val_loss: 7987.8159\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 43731.7083 - val_loss: 7702.7300\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7823.8552 - val_loss: 7433.9659\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 6773.8318 - val_loss: 4222.5469\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4347.2018 - val_loss: 1895.5937\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3281.2418 - val_loss: 1646.3010\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3022.0691 - val_loss: 1257.7828\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2794.6261 - val_loss: 1161.6266\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2681.1552 - val_loss: 886.3712\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2477.6948 - val_loss: 917.2677\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2378.5123 - val_loss: 854.9489\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2342.9574 - val_loss: 727.5241\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2302.2421 - val_loss: 749.2335\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2224.7749 - val_loss: 697.0012\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2132.5905 - val_loss: 689.2336\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 2121.6654 - val_loss: 602.5656\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1979.4474 - val_loss: 537.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|         | 1/9 [03:40<29:24, 220.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0, 'batch_size': 90, 'dense_dropout': 0, 'dense_n': 401, 'LSTM_n': 400}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 401)               160801    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 401)               161202    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 401)               161202    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                12060     \n",
      "=================================================================\n",
      "Total params: 3,701,665\n",
      "Trainable params: 3,701,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 225179.1946 - val_loss: 509848.5176\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 38542349.0229 - val_loss: 47589099.2564\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 33313433.8842 - val_loss: 35169733.9335\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 65947663.3490 - val_loss: 43489988.8611\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 18586009.1804 - val_loss: 7030514.3757\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 3144120.0612 - val_loss: 1285802.7838\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 586340.2920 - val_loss: 260369.9842\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 129663.5078 - val_loss: 63301.4178\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 29116.9579 - val_loss: 14950.9290\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7421.2830 - val_loss: 4074.4676\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 2200.5911 - val_loss: 1477.8838\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 898.4705 - val_loss: 761.6631\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 571.0820 - val_loss: 518.4324\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 468.1022 - val_loss: 514.2124\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 448.0959 - val_loss: 477.8523\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 998.1935 - val_loss: 1267.5931\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 1352.6542 - val_loss: 727.9951\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 645.7886 - val_loss: 530.5730\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 460.0880 - val_loss: 462.5011\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 421.0022 - val_loss: 420.6423\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 399.0909 - val_loss: 421.9189\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 410.4422 - val_loss: 477.0991\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 422.0079 - val_loss: 411.5956\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 389.3754 - val_loss: 416.0267\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 395.9632 - val_loss: 431.2285\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 387.2374 - val_loss: 412.8356\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 394.9756 - val_loss: 482.3868\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 405.1222 - val_loss: 421.0031\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 382.1755 - val_loss: 394.2292\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 388.5300 - val_loss: 400.4720\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 381.5538 - val_loss: 406.0442\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 392.3390 - val_loss: 439.4735\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 384.9271 - val_loss: 414.7301\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 404.7133 - val_loss: 430.8179\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 398.3553 - val_loss: 487.4506\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 402.2043 - val_loss: 400.5214\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 389.5016 - val_loss: 489.4464\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 410.5503 - val_loss: 415.1602\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 391.6697 - val_loss: 464.3069\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 421.2944 - val_loss: 422.3719\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 417.1745 - val_loss: 471.2443\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 416.1943 - val_loss: 404.7626\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 400.6142 - val_loss: 403.0741\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 381.4138 - val_loss: 407.7953\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 373.9388 - val_loss: 382.4932\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 372.9787 - val_loss: 390.7631\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 389.0627 - val_loss: 402.2927\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 575.1597 - val_loss: 536.9341\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 426.4569 - val_loss: 460.6131\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 402.3327 - val_loss: 398.4509\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 429.3597 - val_loss: 496.2593\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 435.2065 - val_loss: 459.6841\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 406.5923 - val_loss: 480.0064\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 427.0076 - val_loss: 473.4211\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 382.5017 - val_loss: 404.7620\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 368.6307 - val_loss: 401.5502\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 393.0075 - val_loss: 414.9073\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 391.8872 - val_loss: 420.7572\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 363.9421 - val_loss: 394.1164\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 348.7305 - val_loss: 373.8898\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 334.8375 - val_loss: 342.8399\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 342.7404 - val_loss: 340.6403\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 344.3733 - val_loss: 372.4167\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 332.4948 - val_loss: 320.9553\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 297.7924 - val_loss: 301.5601\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 273.0110 - val_loss: 304.2956\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 277.6375 - val_loss: 339.9403\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 292.0609 - val_loss: 290.4331\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 251.3944 - val_loss: 244.3810\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 235.6634 - val_loss: 262.8362\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 230.8620 - val_loss: 300.6212\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 250.3344 - val_loss: 373.2316\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 275.5582 - val_loss: 324.4244\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 258.3783 - val_loss: 357.6567\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 251.9674 - val_loss: 305.0807\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 244.7006 - val_loss: 250.4032\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 243.0754 - val_loss: 367.6609\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 261.8206 - val_loss: 307.5246\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 228.5796 - val_loss: 294.7748\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 237.8856 - val_loss: 321.5231\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 225.1057 - val_loss: 314.0675\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 255.3869 - val_loss: 308.9312\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 298.0329 - val_loss: 276.6378\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 236.9705 - val_loss: 282.5346\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 246.6274 - val_loss: 452.4941\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 417.7296 - val_loss: 277.5930\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 441.1083 - val_loss: 489.3274\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 360.6978 - val_loss: 270.2378\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 241.0181 - val_loss: 224.5621\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 209.7312 - val_loss: 215.9920\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 231.5011 - val_loss: 390.9629\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 282.4399 - val_loss: 259.8784\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 275.3414 - val_loss: 473.3654\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 377.0992 - val_loss: 324.4142\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 465.4833 - val_loss: 313.7368\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 377.6272 - val_loss: 403.7999\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 397.1649 - val_loss: 452.6052\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 298.9821 - val_loss: 490.7619\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 311.1116 - val_loss: 334.5041\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 249.2666 - val_loss: 229.0862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|       | 2/9 [16:29<44:55, 385.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0.2, 'batch_size': 90, 'dense_dropout': 0.22, 'dense_n': 201, 'LSTM_n': 100}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 201)               20301     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 201)               40602     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 201)               40602     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                6060      \n",
      "=================================================================\n",
      "Total params: 309,165\n",
      "Trainable params: 309,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5751.9747 - val_loss: 48332.9020\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10011.4746 - val_loss: 3065.1609\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5785.9056 - val_loss: 1366.1464\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4608.0487 - val_loss: 2540.4423\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2907.2535 - val_loss: 1078.8437\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2793.0184 - val_loss: 654.5996\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3113.8198 - val_loss: 2054.4145\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2310.8196 - val_loss: 3379.1788\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1884.5333 - val_loss: 2426.2712\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1633.7183 - val_loss: 2116.9566\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1403.1159 - val_loss: 1453.7463\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1330.8503 - val_loss: 2692.1969\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1256.9591 - val_loss: 2704.9869\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1285.6217 - val_loss: 510.3673\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1837.4841 - val_loss: 2450.6730\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1599.8213 - val_loss: 462.4227\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4518.1041 - val_loss: 525.3214\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2642.6422 - val_loss: 1986.7345\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2037.8710 - val_loss: 1116.6608\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1353.2054 - val_loss: 967.7372\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1032.7008 - val_loss: 1430.9048\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1016.2746 - val_loss: 1089.4018\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 980.5481 - val_loss: 737.8671\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 960.6445 - val_loss: 424.5298\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 848.9722 - val_loss: 715.4467\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 813.5456 - val_loss: 851.3693\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 764.8511 - val_loss: 732.8591\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 770.6327 - val_loss: 476.3871\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 748.4734 - val_loss: 415.2975\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 715.2784 - val_loss: 536.9640\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 719.6394 - val_loss: 635.9889\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 690.0485 - val_loss: 628.3015\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 647.4249 - val_loss: 716.1639\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 648.4868 - val_loss: 989.4450\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 616.4251 - val_loss: 619.1636\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 599.9499 - val_loss: 517.9334\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 578.5147 - val_loss: 754.2447\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 623.3589 - val_loss: 713.9685\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 563.6630 - val_loss: 577.9786\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 553.2376 - val_loss: 841.5727\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 558.2132 - val_loss: 687.8177\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 539.1674 - val_loss: 909.2857\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 543.3086 - val_loss: 547.7887\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 537.0388 - val_loss: 909.5635\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 522.8268 - val_loss: 537.4754\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 495.4327 - val_loss: 447.6281\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 518.9308 - val_loss: 626.0985\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 522.2581 - val_loss: 663.6068\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 471.7656 - val_loss: 596.3521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|      | 3/9 [18:39<30:51, 308.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0, 'batch_size': 90, 'dense_dropout': 0, 'dense_n': 401, 'LSTM_n': 100}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 401)               40501     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 401)               161202    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 401)               161202    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                12060     \n",
      "=================================================================\n",
      "Total params: 576,565\n",
      "Trainable params: 576,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3742.0810 - val_loss: 861.4219\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 572.7084 - val_loss: 479.7441\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 139.4214 - val_loss: 52.3343\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 43.3117 - val_loss: 27.6166\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 22.5373 - val_loss: 23.0274\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.4866 - val_loss: 21.9881\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 16.9114 - val_loss: 14.2221\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 13.8484 - val_loss: 15.0870\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 14.0989 - val_loss: 13.1053\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 14.1487 - val_loss: 15.2755\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 13.6467 - val_loss: 12.1121\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.8349 - val_loss: 13.1348\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.6262 - val_loss: 13.0469\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 14.4657 - val_loss: 12.5000\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.9940 - val_loss: 11.3184\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.7535 - val_loss: 12.7516\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.5433 - val_loss: 15.3183\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.9104 - val_loss: 11.3781\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.2156 - val_loss: 12.5055\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.1217 - val_loss: 11.0793\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.6561 - val_loss: 13.1019\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.7428 - val_loss: 11.3402\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.6780 - val_loss: 12.1051\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.1979 - val_loss: 15.9817\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.7404 - val_loss: 12.3739\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 13.5410 - val_loss: 11.4165\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.2645 - val_loss: 11.3645\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.3619 - val_loss: 10.9650\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.8306 - val_loss: 10.9617\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.9601 - val_loss: 11.4893\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.5555 - val_loss: 10.9468\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.3693 - val_loss: 10.8585\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.6029 - val_loss: 11.1781\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.6921 - val_loss: 11.8184\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.5471 - val_loss: 13.4362\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.7457 - val_loss: 18.0503\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 14.8172 - val_loss: 10.9645\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.2435 - val_loss: 11.1360\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.5142 - val_loss: 11.6828\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.5365 - val_loss: 11.9339\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 12.4976 - val_loss: 12.9783\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 12.1778 - val_loss: 14.4261\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 13.3161 - val_loss: 12.9608\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.9380 - val_loss: 10.4471\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.7134 - val_loss: 10.3817\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.2917 - val_loss: 11.3754\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 10.4806 - val_loss: 12.8131\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 10.6784 - val_loss: 14.0008\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 10.5686 - val_loss: 10.4031\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 10.2887 - val_loss: 10.2509\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.8117 - val_loss: 10.4968\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.9025 - val_loss: 21.7861\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 15.2518 - val_loss: 11.7306\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 11.9948 - val_loss: 11.5094\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.7591 - val_loss: 10.9316\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 11.4634 - val_loss: 15.6092\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.4467 - val_loss: 10.7196\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 11.1122 - val_loss: 12.3013\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 10.6549 - val_loss: 15.3582\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 13.8716 - val_loss: 11.5840\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.4912 - val_loss: 11.5597\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.1216 - val_loss: 12.6918\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.0773 - val_loss: 10.4270\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.6499 - val_loss: 10.5233\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11.4837 - val_loss: 16.9092\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 11.9206 - val_loss: 14.1332\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 12.4310 - val_loss: 10.5938\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10.7340 - val_loss: 15.9089\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.1612 - val_loss: 13.2345\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12.2767 - val_loss: 18.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|     | 4/9 [21:39<22:29, 269.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0.1, 'batch_size': 60, 'dense_dropout': 0.22, 'dense_n': 201, 'LSTM_n': 200}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 201)               40401     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 201)               40602     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 201)               40602     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                6060      \n",
      "=================================================================\n",
      "Total params: 930,865\n",
      "Trainable params: 930,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 29709.7488 - val_loss: 13371.2117\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 28930.1685 - val_loss: 2880.6630\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 150212.2730 - val_loss: 20489.1450\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 192052.3940 - val_loss: 7447.3729\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 74134.6357 - val_loss: 11506.5851\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 67660.3652 - val_loss: 9016.2387\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 59907.8119 - val_loss: 1113.8961\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 12127.5174 - val_loss: 900.5725\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6335.7866 - val_loss: 498.9702\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4535.9074 - val_loss: 505.8416\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4055.9734 - val_loss: 378.3323\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3473.5030 - val_loss: 404.8332\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3006.2432 - val_loss: 326.8692\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2666.1679 - val_loss: 444.5863\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2392.9102 - val_loss: 384.6106\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2368.9643 - val_loss: 338.8138\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2312.3762 - val_loss: 252.3666\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 2089.7140 - val_loss: 198.2068\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1924.1338 - val_loss: 207.9634\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1836.4163 - val_loss: 240.4930\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1920.1479 - val_loss: 332.6533\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1853.3655 - val_loss: 213.5779\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1616.8131 - val_loss: 367.3139\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1545.7099 - val_loss: 503.6467\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1497.9216 - val_loss: 479.9411\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1627.4931 - val_loss: 522.7871\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1468.8790 - val_loss: 476.1809\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1392.0184 - val_loss: 399.0560\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1416.5773 - val_loss: 333.9303\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1314.4375 - val_loss: 270.9300\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1330.7534 - val_loss: 304.4294\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1212.6374 - val_loss: 206.4389\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1164.1822 - val_loss: 344.9726\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1244.1907 - val_loss: 468.0147\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1145.3799 - val_loss: 210.9148\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1124.0592 - val_loss: 137.1154\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1044.7427 - val_loss: 220.4592\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1051.4136 - val_loss: 426.6753\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 975.2246 - val_loss: 285.1053\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1013.8404 - val_loss: 310.9209\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 994.4937 - val_loss: 295.0435\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1004.0305 - val_loss: 187.6675\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 940.1196 - val_loss: 184.8692\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 889.7364 - val_loss: 309.1272\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 895.7506 - val_loss: 313.9921\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 875.4891 - val_loss: 221.4367\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 968.3417 - val_loss: 253.5757\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 862.8133 - val_loss: 199.4211\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 854.6064 - val_loss: 201.9057\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 830.7237 - val_loss: 189.3486\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 814.8794 - val_loss: 245.6580\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 777.2933 - val_loss: 338.7602\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 794.6028 - val_loss: 314.0744\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 782.7039 - val_loss: 353.9554\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 788.5517 - val_loss: 239.1970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 720.6591 - val_loss: 344.5506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|    | 5/9 [26:06<17:56, 269.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0.2, 'batch_size': 30, 'dense_dropout': 0.11, 'dense_n': 101, 'LSTM_n': 400}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 101)               40501     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 101)               10302     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 101)               10302     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                3060      \n",
      "=================================================================\n",
      "Total params: 3,270,565\n",
      "Trainable params: 3,270,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 15s 13ms/step - loss: 738281.9721 - val_loss: 254213.1702\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 10893729.0245 - val_loss: 804798.8366\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 6154552.3067 - val_loss: 26485.2544\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 187226.1808 - val_loss: 10107.0690\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 207258.7645 - val_loss: 3012.7984\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 47784.9512 - val_loss: 5282.5452\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 27883.4063 - val_loss: 11984.3961\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 56949.9739 - val_loss: 22042.4570\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 1879630.8600 - val_loss: 176345.3073\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 652069.0349 - val_loss: 8176.8699\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 447754.4093 - val_loss: 7641.2900\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 244947.7334 - val_loss: 9751.8344\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 27090.7161 - val_loss: 7890.7597\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 10299.4603 - val_loss: 7886.6272\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 28832.1743 - val_loss: 18061.3754\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 32569.3987 - val_loss: 6457.7130\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 16638.2225 - val_loss: 7873.1661\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 9098.7092 - val_loss: 7804.7836\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 7691.5968 - val_loss: 6723.3597\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 6401.9800 - val_loss: 3067.4254\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 4609.8771 - val_loss: 2093.2786\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 3870.7977 - val_loss: 2487.5746\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 3771.0468 - val_loss: 1281.2129\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 3442.5283 - val_loss: 1776.4887\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 3013.6121 - val_loss: 2102.2505\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2726.4718 - val_loss: 1698.4603\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2676.2374 - val_loss: 1266.4257\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2588.8030 - val_loss: 1395.7042\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 3204.8912 - val_loss: 1707.4039\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 2939.2916 - val_loss: 2493.3988\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 2639.3156 - val_loss: 1391.5189\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 2380.2026 - val_loss: 1871.3135\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 2303.9964 - val_loss: 957.4482\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2107.6110 - val_loss: 799.6540\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2083.1030 - val_loss: 1324.6229\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1999.0442 - val_loss: 627.0696\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1997.3757 - val_loss: 380.3563\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1961.9052 - val_loss: 1242.8489\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2040.5791 - val_loss: 154.6056\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 3031.0813 - val_loss: 3713.0446\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 3143.8224 - val_loss: 758.4350\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2230.3965 - val_loss: 739.5963\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2002.1514 - val_loss: 1165.9243\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1882.1006 - val_loss: 665.5111\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1759.7461 - val_loss: 1021.3996\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 1707.1542 - val_loss: 945.3930\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1718.8741 - val_loss: 780.6287\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1700.4967 - val_loss: 1068.7497\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1602.9221 - val_loss: 903.2778\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1584.3549 - val_loss: 460.5612\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1489.7357 - val_loss: 711.1072\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1497.1799 - val_loss: 662.0520\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1449.0417 - val_loss: 668.7190\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 17s 14ms/step - loss: 1399.6303 - val_loss: 813.2672\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1369.9758 - val_loss: 481.7765\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1322.6516 - val_loss: 810.2408\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 14s 12ms/step - loss: 1358.7675 - val_loss: 528.6402\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 1291.7564 - val_loss: 460.7859\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 1305.1468 - val_loss: 610.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|   | 6/9 [39:18<21:17, 425.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0, 'batch_size': 120, 'dense_dropout': 0, 'dense_n': 401, 'LSTM_n': 100}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 401)               40501     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 401)               161202    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 401)               161202    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 401)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                12060     \n",
      "=================================================================\n",
      "Total params: 576,565\n",
      "Trainable params: 576,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 8705.1994 - val_loss: 6402.0027\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3972.6963 - val_loss: 3823.8180\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7181.4251 - val_loss: 5024.9745\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 11506.7952 - val_loss: 8500.6594\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 8374.4918 - val_loss: 6570.3605\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4206.7199 - val_loss: 3540.0281\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2556.8458 - val_loss: 1705.6835\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1738.0786 - val_loss: 1227.1152\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1200.5586 - val_loss: 675.3382\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 588.6686 - val_loss: 357.0270\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 289.1729 - val_loss: 164.7185\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 128.6204 - val_loss: 84.2376\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 71.7545 - val_loss: 57.0784\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 53.0176 - val_loss: 48.5706\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 45.6472 - val_loss: 39.6298\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 38.9447 - val_loss: 40.2246\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 38.2995 - val_loss: 37.5222\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 33.2749 - val_loss: 32.4403\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 31.1098 - val_loss: 32.6615\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 30.5372 - val_loss: 30.6265\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 31.1841 - val_loss: 30.4190\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 29.8097 - val_loss: 30.0292\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 28.7984 - val_loss: 28.3925\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 27.7262 - val_loss: 27.3263\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 27.1642 - val_loss: 26.7222\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 26.2087 - val_loss: 26.0103\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 25.6327 - val_loss: 25.8763\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 25.3791 - val_loss: 24.6652\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 24.9875 - val_loss: 24.5708\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 24.3837 - val_loss: 24.0349\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 24.1647 - val_loss: 25.1938\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 24.2273 - val_loss: 24.0840\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.9167 - val_loss: 24.3677\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.4499 - val_loss: 23.9733\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.7503 - val_loss: 23.2123\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23.4847 - val_loss: 22.8057\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 22.7128 - val_loss: 22.7257\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 22.4463 - val_loss: 22.4552\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21.9714 - val_loss: 22.1696\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 22.6604 - val_loss: 23.0432\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 22.1750 - val_loss: 21.8172\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21.2567 - val_loss: 21.7946\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.9651 - val_loss: 21.5671\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.9593 - val_loss: 21.6329\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21.3514 - val_loss: 22.2268\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21.1412 - val_loss: 21.2908\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 21.0369 - val_loss: 21.8101\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.4808 - val_loss: 20.6419\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.4531 - val_loss: 21.3886\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.1749 - val_loss: 20.8504\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.5930 - val_loss: 21.0236\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 20.2503 - val_loss: 20.6239\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.6855 - val_loss: 20.4808\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.0001 - val_loss: 19.1219\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.1792 - val_loss: 20.1881\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.2123 - val_loss: 21.1145\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.3522 - val_loss: 19.5695\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.8254 - val_loss: 18.7759\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.5754 - val_loss: 18.3615\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.2093 - val_loss: 18.7248\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.0814 - val_loss: 18.8294\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.1290 - val_loss: 20.1408\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.5887 - val_loss: 19.2695\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.2591 - val_loss: 19.8213\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.7049 - val_loss: 19.0528\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.2044 - val_loss: 17.9194\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.9050 - val_loss: 19.2341\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.5559 - val_loss: 17.9641\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.6037 - val_loss: 18.4251\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.3038 - val_loss: 18.9123\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.9822 - val_loss: 18.9479\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.5610 - val_loss: 19.0661\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.1746 - val_loss: 18.7308\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.4123 - val_loss: 18.0288\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.9203 - val_loss: 20.7468\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.4460 - val_loss: 19.6361\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.0234 - val_loss: 18.4923\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.8673 - val_loss: 19.0467\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.7217 - val_loss: 18.9258\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.6267 - val_loss: 18.0063\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 16.9655 - val_loss: 17.2927\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.1125 - val_loss: 17.8836\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 16.9344 - val_loss: 17.4491\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 16.9398 - val_loss: 19.0195\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.0157 - val_loss: 19.0073\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.6503 - val_loss: 17.1505\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.1004 - val_loss: 18.1685\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.0369 - val_loss: 19.2945\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.3379 - val_loss: 19.1089\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.6351 - val_loss: 17.6107\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.7195 - val_loss: 18.8993\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.2206 - val_loss: 18.6160\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.4107 - val_loss: 19.8926\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.9479 - val_loss: 19.6371\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.5362 - val_loss: 19.1297\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 18.3736 - val_loss: 17.5065\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 19.7564 - val_loss: 19.4750\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.2599 - val_loss: 19.3983\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.1531 - val_loss: 17.7422\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 17.0634 - val_loss: 17.7479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|  | 7/9 [42:49<12:02, 361.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0.1, 'batch_size': 120, 'dense_dropout': 0.11, 'dense_n': 201, 'LSTM_n': 100}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 201)               20301     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 201)               40602     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 201)               40602     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 201)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                6060      \n",
      "=================================================================\n",
      "Total params: 309,165\n",
      "Trainable params: 309,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 31436.8839 - val_loss: 15056.3557\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 15430.0474 - val_loss: 4650.9643\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23253.4215 - val_loss: 4429.1442\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 33907.7753 - val_loss: 7091.3385\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 51065.8681 - val_loss: 8595.8561\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 46664.5189 - val_loss: 2668.9614\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 23630.8074 - val_loss: 2199.4879\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 14634.2034 - val_loss: 1089.0438\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 12860.4987 - val_loss: 767.5035\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10110.8933 - val_loss: 245.5558\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 10246.0814 - val_loss: 471.1609\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 13064.0677 - val_loss: 495.9517\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 9361.6693 - val_loss: 232.9736\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6930.8204 - val_loss: 132.0220\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4778.7479 - val_loss: 189.0418\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5405.0433 - val_loss: 232.2977\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5222.2361 - val_loss: 450.4488\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3458.3895 - val_loss: 74.9246\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2718.9901 - val_loss: 50.2083\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2436.0369 - val_loss: 44.7118\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2337.7793 - val_loss: 41.5394\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2271.8192 - val_loss: 51.2177\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2236.7520 - val_loss: 70.8403\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2135.8811 - val_loss: 88.0243\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2204.1861 - val_loss: 51.8892\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2232.3740 - val_loss: 112.4349\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2799.6996 - val_loss: 217.8407\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3510.3771 - val_loss: 175.5419\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4300.6007 - val_loss: 89.3465\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1855.6658 - val_loss: 52.8093\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1559.2716 - val_loss: 205.8574\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1447.0123 - val_loss: 194.7506\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1391.5315 - val_loss: 66.3801\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1418.7436 - val_loss: 381.5890\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 963.3552 - val_loss: 1362.3020\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6611.7447 - val_loss: 776.6734\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5142.6715 - val_loss: 270.0070\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2348.2672 - val_loss: 186.3802\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2961.6034 - val_loss: 878.0470\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3485.8988 - val_loss: 387.3683\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1501.8913 - val_loss: 118.7815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%| | 8/9 [44:27<04:42, 282.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_dropout': 0, 'batch_size': 120, 'dense_dropout': 0.22, 'dense_n': 101, 'LSTM_n': 400}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 101)               40501     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 101)               10302     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 101)               10302     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                3060      \n",
      "=================================================================\n",
      "Total params: 3,270,565\n",
      "Trainable params: 3,270,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 30544.3690 - val_loss: 12853.9087\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 27634.4885 - val_loss: 13877.1634\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 393496.4202 - val_loss: 133925.8829\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 15745531.3372 - val_loss: 6107818.8268\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 32110404.1611 - val_loss: 1112957.4107\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 9403068.3322 - val_loss: 589398.9616\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2528264.9715 - val_loss: 305525.9821\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 1798004.7848 - val_loss: 270725.5706\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 525191.7029 - val_loss: 23819.5193\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 76429.4907 - val_loss: 6743.5956\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7770.7468 - val_loss: 4813.8974\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 5649.8506 - val_loss: 2233.8882\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3831.4754 - val_loss: 1606.6910\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3521.2938 - val_loss: 1663.6821\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3382.0270 - val_loss: 1591.0116\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 3261.8781 - val_loss: 1406.9411\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3235.8710 - val_loss: 1378.0316\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3237.7260 - val_loss: 1360.5953\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3220.3877 - val_loss: 1477.8335\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3200.2137 - val_loss: 1391.5832\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3143.1164 - val_loss: 1315.4457\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3170.4167 - val_loss: 1349.0691\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3153.4030 - val_loss: 1224.1052\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 3136.9506 - val_loss: 1344.9489\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 3123.2109 - val_loss: 1378.5490\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3151.0019 - val_loss: 1395.1620\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3164.1633 - val_loss: 1308.0840\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3068.6484 - val_loss: 1407.3483\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3175.8010 - val_loss: 1347.4698\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3133.7117 - val_loss: 1311.4387\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3158.5046 - val_loss: 1334.6044\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3111.4747 - val_loss: 1355.1764\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3110.4569 - val_loss: 1414.0359\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3067.0014 - val_loss: 1311.7896\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3049.8021 - val_loss: 1294.6690\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3072.0980 - val_loss: 1246.0778\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3006.5519 - val_loss: 1320.5807\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3051.1252 - val_loss: 1310.6116\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2989.8755 - val_loss: 1204.7818\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3007.2883 - val_loss: 1203.1591\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3033.1437 - val_loss: 1231.6636\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2939.6479 - val_loss: 1194.2488\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2969.2794 - val_loss: 1229.6754\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2905.8876 - val_loss: 1131.2868\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2952.5807 - val_loss: 1248.8382\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2975.2722 - val_loss: 1324.2872\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2930.4437 - val_loss: 1302.7981\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2997.2566 - val_loss: 1293.6677\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 2889.4241 - val_loss: 1244.2989\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 2904.3187 - val_loss: 1235.9290\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2870.4913 - val_loss: 1133.6829\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2890.8331 - val_loss: 1112.2922\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2916.4363 - val_loss: 1135.9324\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 2884.0281 - val_loss: 1032.7216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2857.3021 - val_loss: 1008.7034\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2860.6513 - val_loss: 1128.8852\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2906.7029 - val_loss: 1144.8188\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2810.6969 - val_loss: 1068.3798\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2838.6292 - val_loss: 1014.7039\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2838.3082 - val_loss: 979.4674\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2799.4595 - val_loss: 1012.6876\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2789.3747 - val_loss: 1178.8238\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2760.5818 - val_loss: 1036.5405\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2740.8285 - val_loss: 1046.5969\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2728.0775 - val_loss: 1053.3782\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2744.4532 - val_loss: 1052.1053\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2763.3555 - val_loss: 1095.7931\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2751.0345 - val_loss: 900.4039\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2760.6989 - val_loss: 948.4423\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2692.0582 - val_loss: 1088.8601\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2729.2718 - val_loss: 934.6659\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2706.8535 - val_loss: 988.7738\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2710.5130 - val_loss: 1029.8784\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2617.9024 - val_loss: 961.2167\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2707.8660 - val_loss: 1007.5734\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2662.5229 - val_loss: 989.6241\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2632.0894 - val_loss: 1095.7305\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2598.3043 - val_loss: 878.2767\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2637.0885 - val_loss: 789.7735\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2667.7722 - val_loss: 938.5631\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2625.9402 - val_loss: 956.3203\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2603.8872 - val_loss: 919.4265\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2607.7790 - val_loss: 929.1474\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2570.6652 - val_loss: 783.8581\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2542.1478 - val_loss: 895.7994\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2541.7430 - val_loss: 999.5935\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2530.2411 - val_loss: 905.3451\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2524.5554 - val_loss: 1088.1955\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2541.3544 - val_loss: 1008.0547\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2570.3470 - val_loss: 770.5398\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 2471.7048 - val_loss: 728.5990\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 2457.7821 - val_loss: 964.0223\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2490.5672 - val_loss: 920.5130\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2432.4121 - val_loss: 909.4998\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2447.6743 - val_loss: 800.4581\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2473.5150 - val_loss: 682.6069\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 2464.3408 - val_loss: 745.0192\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 2389.2020 - val_loss: 783.5885\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2442.9925 - val_loss: 741.1212\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2425.8375 - val_loss: 647.5724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [58:02<00:00, 386.90s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "\n",
    "scan_object = talos.Scan(X_train,\n",
    "                         y_train, \n",
    "                         params=p,\n",
    "                         model=LSTM_model,\n",
    "                         experiment_name='LSTM',\n",
    "                         fraction_limit=0.03,\n",
    "                         print_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug (link below) in the Talos library that means it sorts the columns and values from parameter dictionary incorrectly in the ```talos.Analze``` object. That is why each hyperparameter has unique values so I can easily understand the results.\n",
    "\n",
    "[https://github.com/autonomio/talos/issues/439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>LSTM_dropout</th>\n",
       "      <th>LSTM_n</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dense_dropout</th>\n",
       "      <th>dense_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>17.747945</td>\n",
       "      <td>17.063383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120</td>\n",
       "      <td>0.00</td>\n",
       "      <td>401</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>18.036368</td>\n",
       "      <td>12.276716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>401</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>118.781533</td>\n",
       "      <td>1501.891289</td>\n",
       "      <td>0.1</td>\n",
       "      <td>120</td>\n",
       "      <td>0.11</td>\n",
       "      <td>201</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>229.086226</td>\n",
       "      <td>249.266560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>401</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>344.550551</td>\n",
       "      <td>720.659121</td>\n",
       "      <td>0.1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.22</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>537.265257</td>\n",
       "      <td>1979.447426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.22</td>\n",
       "      <td>401</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>596.352055</td>\n",
       "      <td>471.765609</td>\n",
       "      <td>0.2</td>\n",
       "      <td>90</td>\n",
       "      <td>0.22</td>\n",
       "      <td>201</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59</td>\n",
       "      <td>610.984471</td>\n",
       "      <td>1305.146822</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>101</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>647.572393</td>\n",
       "      <td>2425.837471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120</td>\n",
       "      <td>0.22</td>\n",
       "      <td>101</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs    val_loss         loss  LSTM_dropout  LSTM_n  batch_size  \\\n",
       "6           100   17.747945    17.063383           0.0     120        0.00   \n",
       "3            70   18.036368    12.276716           0.0      90        0.00   \n",
       "7            41  118.781533  1501.891289           0.1     120        0.11   \n",
       "1           100  229.086226   249.266560           0.0      90        0.00   \n",
       "4            56  344.550551   720.659121           0.1      60        0.22   \n",
       "0            47  537.265257  1979.447426           0.0      60        0.22   \n",
       "2            49  596.352055   471.765609           0.2      90        0.22   \n",
       "5            59  610.984471  1305.146822           0.2      30        0.11   \n",
       "8           100  647.572393  2425.837471           0.0     120        0.22   \n",
       "\n",
       "   dense_dropout  dense_n  \n",
       "6            401      100  \n",
       "3            401      100  \n",
       "7            201      100  \n",
       "1            401      400  \n",
       "4            201      200  \n",
       "0            401      200  \n",
       "2            201      100  \n",
       "5            101      400  \n",
       "8            101      400  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_object = talos.Analyze(scan_object)\n",
    "df = analyze_object.data\n",
    "df.sort_values(by=['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "* I like the look of the first two results. The features include:\n",
    "  * LSTM neurons 100\n",
    "  * No dropout on LSTM layers\n",
    "  * Dense neurons of 400\n",
    "  * No dropout on Dense layers\n",
    "  * Higher batch sizes of 90 or 120"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
