{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS Optimization - FOUR\n",
    "\n",
    "An implementation of a hyperparameter grid search using the Talos library.\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check what hardware is available to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8240830078219759046\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8280149956530184838\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5031, 16)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    keep_col = list(range(1,18))\n",
    "\n",
    "    df_day = pd.read_csv('ECL_Clean_Day.csv', \n",
    "                     infer_datetime_format=True,\n",
    "                     parse_dates=['Timestamp'], \n",
    "                     index_col=['Timestamp'],\n",
    "                     usecols = keep_col,\n",
    "                     date_parser=lambda col: pd.to_datetime(col, utc=True).tz_convert('America/New_York'))\n",
    "\n",
    "    #df_min.dtypes\n",
    "    print (df_day.shape)\n",
    "    df_day.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 10 years of data from 2019 - 2010\n",
    "day_data = df_day[df_day.index >= '2010-01-01']\n",
    "\n",
    "#split 7 years train, 3 years test\n",
    "day_train = day_data[day_data.index <= '2017-01-01']\n",
    "day_test = day_data[day_data.index >= '2017-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = day_train['Close'] \n",
    "base_test = day_test['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(base_test)\n",
    "plt.plot(base_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Simple Vanilla Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1703, 30) (1703, 30) (695, 30) (695, 30)\n"
     ]
    }
   ],
   "source": [
    "# Multi-step data preparation\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# define input sequence\n",
    "#raw_seq = list(base_test[:100].values)\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 30, 30\n",
    "# split into samples\n",
    "X_train, y_train = split_sequence(base_train.values, n_steps_in, n_steps_out)\n",
    "X_test, y_test = split_sequence(base_test.values, n_steps_in, n_steps_out) #length must be > n_in + n_out\n",
    "# summarize the data\n",
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 43,830\n",
      "Trainable params: 43,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1703 samples, validate on 695 samples\n",
      "Epoch 1/20\n",
      "1703/1703 [==============================] - 3s 2ms/step - loss: 1765.6345 - val_loss: 819.9287\n",
      "Epoch 2/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 179.8529 - val_loss: 189.0207\n",
      "Epoch 3/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 28.0630 - val_loss: 58.6808\n",
      "Epoch 4/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 23.0308 - val_loss: 55.0501\n",
      "Epoch 5/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 22.5642 - val_loss: 58.3242\n",
      "Epoch 6/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 22.6404 - val_loss: 60.5912\n",
      "Epoch 7/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.4203 - val_loss: 55.2515\n",
      "Epoch 8/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.2762 - val_loss: 56.8508\n",
      "Epoch 9/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 20.9513 - val_loss: 58.1461\n",
      "Epoch 10/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.2728 - val_loss: 58.9907\n",
      "Epoch 11/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.0639 - val_loss: 50.5409\n",
      "Epoch 12/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.1934 - val_loss: 54.0625\n",
      "Epoch 13/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.5636 - val_loss: 46.4667\n",
      "Epoch 14/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.4746 - val_loss: 45.4928\n",
      "Epoch 15/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.7263 - val_loss: 47.3153\n",
      "Epoch 16/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 21.8367 - val_loss: 47.9984\n",
      "Epoch 17/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 23.7283 - val_loss: 81.8158\n",
      "Epoch 18/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 26.1776 - val_loss: 61.4108\n",
      "Epoch 19/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 22.6335 - val_loss: 48.1290\n",
      "Epoch 20/20\n",
      "1703/1703 [==============================] - 2s 1ms/step - loss: 22.8756 - val_loss: 59.4135\n"
     ]
    }
   ],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "#print (X_train.shape)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "#print (X_test.shape)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# fit model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for grid search\n",
    "\n",
    "p = {'LSTM_n' : [100, 200, 400],\n",
    "    'LSTM_dropout' : [0, 0.1, 0.2],\n",
    "    'batch_size' : [30, 60, 90, 120],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train, y_train, X_test, y_test, params):\n",
    "    model = Sequential()                            \n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(LSTM(params['LSTM_n'], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['LSTM_dropout']))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='Adam', \n",
    "                  loss='mse')\n",
    "    print (model.summary())\n",
    "    \n",
    "    #Early Stopping to avoid wasting time on bad hyperparams\n",
    "    es = EarlyStopping(monitor='val_loss', mode='auto', patience=20)\n",
    "\n",
    "    out = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    callbacks=[es],\n",
    "                    validation_data=[X_test, y_test])\n",
    "    \n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/36 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 100, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7807.2271 - val_loss: 3513.9797\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7509.1519 - val_loss: 6046.2112\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7157.8326 - val_loss: 6685.1399\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6772.9420 - val_loss: 5462.4627\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6379.8940 - val_loss: 6091.0211\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5979.5388 - val_loss: 5995.1099\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5524.6455 - val_loss: 4921.7009\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4976.2504 - val_loss: 5359.5500\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4387.8303 - val_loss: 4858.8241\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3768.4863 - val_loss: 4221.9448\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3133.9505 - val_loss: 3218.4109\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2535.1051 - val_loss: 2740.1957\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1984.6703 - val_loss: 2308.5271\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1488.9222 - val_loss: 1829.4620\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1090.7729 - val_loss: 1034.6464\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 765.9398 - val_loss: 1064.2366\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 520.9918 - val_loss: 636.0224\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 346.7856 - val_loss: 496.9038\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 218.8148 - val_loss: 268.7209\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 139.8055 - val_loss: 121.3435\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 91.8852 - val_loss: 60.4491\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 61.0598 - val_loss: 43.0582\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 61.0676 - val_loss: 49.7182\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 45.9683 - val_loss: 83.5862\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 52.0407 - val_loss: 136.3117\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 45.8328 - val_loss: 127.8510\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 42.3880 - val_loss: 67.1327\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 28.2180 - val_loss: 29.5860\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 42.8240 - val_loss: 34.6116\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 35.9852 - val_loss: 17.5904\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 47.5755 - val_loss: 28.2196\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 41.2465 - val_loss: 17.7593\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 41.9031 - val_loss: 4878.0229\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 46.6258 - val_loss: 1117.2280\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 43.1997 - val_loss: 66.1691\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 39.2172 - val_loss: 96.4044\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 71.0347 - val_loss: 42.8715\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 52.4215 - val_loss: 495.2895\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 52.6233 - val_loss: 27.4856\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 51.5300 - val_loss: 36.4264\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 40.5549 - val_loss: 142.5687\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 43.7458 - val_loss: 201.6503\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 45.6201 - val_loss: 490.4793\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 32.4958 - val_loss: 185.3381\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 42.8974 - val_loss: 98.9902\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 32.3346 - val_loss: 224.2546\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 33.8284 - val_loss: 155.2708\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 30.7726 - val_loss: 182.9716\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.2123 - val_loss: 180.4112\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.0755 - val_loss: 50.7881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 1/36 [03:57<2:18:17, 237.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 100, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7845.9385 - val_loss: 7582.6102\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7701.5995 - val_loss: 8028.7302\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7555.9958 - val_loss: 7991.6790\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7395.2600 - val_loss: 7754.8071\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7221.9864 - val_loss: 7515.6987\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7040.0784 - val_loss: 7475.5942\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6850.0126 - val_loss: 6969.9492\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6666.8879 - val_loss: 6721.7361\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6467.8391 - val_loss: 6419.5597\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6273.5512 - val_loss: 6086.5279\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6079.7754 - val_loss: 5816.6970\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5870.6846 - val_loss: 5772.1579\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5650.7057 - val_loss: 6531.6925\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5417.4762 - val_loss: 5525.9436\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5145.8838 - val_loss: 5278.5250\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4862.7039 - val_loss: 4843.3552\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4573.0692 - val_loss: 5193.0787\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4266.2964 - val_loss: 4087.3576\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 3963.2500 - val_loss: 3655.2722\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3648.7196 - val_loss: 3179.2041\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3328.9299 - val_loss: 2611.3557\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3024.3376 - val_loss: 2171.2729\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2723.2117 - val_loss: 1816.4234\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2428.0828 - val_loss: 1789.6495\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2157.9260 - val_loss: 1558.4305\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1880.5014 - val_loss: 1629.7121\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1641.4276 - val_loss: 1198.5346\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1420.8843 - val_loss: 1221.3128\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1204.2441 - val_loss: 845.6156\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1026.7920 - val_loss: 493.2682\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 853.3830 - val_loss: 722.8171\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 712.1833 - val_loss: 386.9205\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 592.9881 - val_loss: 374.5617\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 479.4579 - val_loss: 266.8812\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 391.6881 - val_loss: 216.4436\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 307.5952 - val_loss: 150.0441\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 252.2862 - val_loss: 500.7981\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 199.1728 - val_loss: 379.4609\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 161.5841 - val_loss: 349.6074\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 122.5678 - val_loss: 275.1191\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 99.2123 - val_loss: 172.8834\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 80.8891 - val_loss: 131.8754\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 63.9363 - val_loss: 71.4133\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 52.9228 - val_loss: 59.7316\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 40.9517 - val_loss: 50.0673\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 42.0251 - val_loss: 59.9561\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 30.4230 - val_loss: 35.4693\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 27.7046 - val_loss: 31.5509\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 28.1635 - val_loss: 70.5692\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 27.1340 - val_loss: 58.6355\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 28.8776 - val_loss: 26.1352\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 21.9864 - val_loss: 31.0518\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 23.3525 - val_loss: 19.3260\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 23.7928 - val_loss: 36.5146\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 25.7041 - val_loss: 18.9550\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 18.2425 - val_loss: 17.1177\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 22.8608 - val_loss: 33.5646\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 20.2741 - val_loss: 19.8380\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 18.2857 - val_loss: 12.1959\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 3s 3ms/step - loss: 25.5739 - val_loss: 16.2588\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 18.8726 - val_loss: 23.9660\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 27.6774 - val_loss: 18.0057\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 18.0480 - val_loss: 39.2423\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 27.7026 - val_loss: 54.1807\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 23.7843 - val_loss: 77.1913\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 20.1358 - val_loss: 17.2841\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 35.4809 - val_loss: 14.8392\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 30.1382 - val_loss: 19.2421\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 22.5022 - val_loss: 20.1337\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 21.2542 - val_loss: 19.9835\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 21.8189 - val_loss: 18.5421\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 27.3574 - val_loss: 12.6817\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 22.1415 - val_loss: 13.8443\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 18.2880 - val_loss: 26.3258\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 27.7063 - val_loss: 17.2923\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 23.5696 - val_loss: 43.4993\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 21.1416 - val_loss: 12.1471\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 22.1249 - val_loss: 14.1771\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 19.7333 - val_loss: 17.6753\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 20.0293 - val_loss: 12.3499\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 23.7465 - val_loss: 14.2317\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 17.0534 - val_loss: 11.8725\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 21.1496 - val_loss: 16.3678\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 26.4021 - val_loss: 30.0580\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 24.7181 - val_loss: 24.9768\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 25.3141 - val_loss: 21.5429\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 18.7170 - val_loss: 13.4452\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 19.6528 - val_loss: 15.6856\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 23.0252 - val_loss: 13.9822\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 22.5287 - val_loss: 20.2359\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 19.3935 - val_loss: 15.3683\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 15.7519 - val_loss: 17.7242\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 21.5530 - val_loss: 15.5599\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 26.7739 - val_loss: 22.7202\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 26.8067 - val_loss: 17.8437\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 19.3451 - val_loss: 12.3345\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 21.5203 - val_loss: 23.6462\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 26.9342 - val_loss: 19.8593\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 20.9460 - val_loss: 24.0173\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 19.6010 - val_loss: 10.8204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 2/36 [09:41<2:32:31, 269.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 100, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7857.4767 - val_loss: 7953.9957\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7763.0363 - val_loss: 8067.0853\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7670.0505 - val_loss: 7962.0291\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7569.5487 - val_loss: 7728.8854\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7460.9484 - val_loss: 7525.7468\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7345.2840 - val_loss: 7441.2848\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7224.2462 - val_loss: 7185.0823\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7099.1726 - val_loss: 7306.1671\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6970.5001 - val_loss: 7302.4716\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6843.3397 - val_loss: 6824.7999\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6709.0561 - val_loss: 6725.1226\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6570.2670 - val_loss: 6550.2349\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6435.6706 - val_loss: 7484.3612\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6301.1164 - val_loss: 6316.4108\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6160.6597 - val_loss: 7571.5289\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6024.7717 - val_loss: 5699.2075\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5882.6526 - val_loss: 5584.1869\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5727.2772 - val_loss: 6532.7054\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5583.7001 - val_loss: 5501.9865\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5397.8729 - val_loss: 3470.5226\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5207.9582 - val_loss: 3642.7416\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5024.7596 - val_loss: 3813.3577\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4828.1558 - val_loss: 3543.1428\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4628.5875 - val_loss: 2480.2416\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4414.1966 - val_loss: 3437.4246\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4202.8118 - val_loss: 3399.4323\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3987.9110 - val_loss: 3211.2478\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3762.1098 - val_loss: 4790.4890\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3545.4568 - val_loss: 3272.1809\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3323.5323 - val_loss: 3174.8106\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3105.5927 - val_loss: 2842.3767\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2893.7696 - val_loss: 2512.4072\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2683.2722 - val_loss: 2381.4990\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2476.6940 - val_loss: 2008.6587\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2285.2936 - val_loss: 1582.7124\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2098.8544 - val_loss: 1510.0967\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1904.0767 - val_loss: 1132.0672\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1735.9213 - val_loss: 1303.5762\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1555.4219 - val_loss: 1192.5002\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1404.9094 - val_loss: 1529.1033\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1262.7769 - val_loss: 1560.8072\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1120.3524 - val_loss: 1442.9762\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1000.4900 - val_loss: 931.9195\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 883.9116 - val_loss: 1059.8755\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 783.9799 - val_loss: 883.9785\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 681.1136 - val_loss: 968.2224\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 594.9195 - val_loss: 842.7537\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 514.4373 - val_loss: 890.7615\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 448.8711 - val_loss: 485.1856\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 380.3723 - val_loss: 448.3503\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 328.1450 - val_loss: 200.5082\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 280.7425 - val_loss: 178.3366\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 237.6330 - val_loss: 172.1013\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 203.8779 - val_loss: 149.3890\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 176.1782 - val_loss: 275.3845\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 147.9995 - val_loss: 255.8650\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 121.6246 - val_loss: 104.0418\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 107.5931 - val_loss: 24.5376\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 3s 2ms/step - loss: 88.0336 - val_loss: 29.7179\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 81.2206 - val_loss: 53.3021\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 67.5491 - val_loss: 59.2131\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 53.4146 - val_loss: 59.8660\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 48.1601 - val_loss: 77.9111\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 41.4303 - val_loss: 95.7342\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 38.6489 - val_loss: 129.1549\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 30.7954 - val_loss: 64.1120\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 40.9042 - val_loss: 19.2339\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 24.6201 - val_loss: 100.3355\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 24.3696 - val_loss: 94.5439\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 22.5087 - val_loss: 36.3436\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 23.1472 - val_loss: 76.0754\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 23.6857 - val_loss: 21.2489\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 15.9812 - val_loss: 53.2986\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 13.3087 - val_loss: 25.4073\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 18.5312 - val_loss: 16.7553\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 24.9808 - val_loss: 15.4092\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 18.8348 - val_loss: 12.4672\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 17.3225 - val_loss: 16.7774\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 21.1874 - val_loss: 19.3763\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 16.8206 - val_loss: 36.6339\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 14.8137 - val_loss: 26.8532\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 20.1112 - val_loss: 14.7809\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 25.1247 - val_loss: 14.5237\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 18.2772 - val_loss: 31.7379\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 24.8858 - val_loss: 12.0825\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 21.5394 - val_loss: 14.4529\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 16.1687 - val_loss: 13.2127\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 14.5460 - val_loss: 12.8227\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 27.2640 - val_loss: 18.8862\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 16.4855 - val_loss: 15.4682\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 15.9544 - val_loss: 11.1995\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 17.3305 - val_loss: 25.0169\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 20.7966 - val_loss: 15.1974\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 19.9913 - val_loss: 23.2358\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 14.6487 - val_loss: 21.4854\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 22.4444 - val_loss: 13.6457\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 16.9445 - val_loss: 32.8412\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 14.4494 - val_loss: 33.3412\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 18.7163 - val_loss: 34.9634\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 16.5026 - val_loss: 42.7848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 3/36 [14:27<2:30:54, 274.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 100, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7882.7492 - val_loss: 10483.1321\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7804.5392 - val_loss: 55015.9663\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7732.4324 - val_loss: 6230.0797\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7658.3167 - val_loss: 6544.9167\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7584.2799 - val_loss: 1956.5146\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7503.9317 - val_loss: 3813.2534\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7423.0218 - val_loss: 4526.5962\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7346.8583 - val_loss: 2621.2222\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7255.8755 - val_loss: 4048.7296\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7166.0956 - val_loss: 3579.6011\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7075.7693 - val_loss: 4122.6925\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6981.7569 - val_loss: 4341.3028\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6885.8289 - val_loss: 4577.3451\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6798.3939 - val_loss: 5037.6059\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6695.5231 - val_loss: 4227.5848\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6601.2887 - val_loss: 4633.7585\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6502.7071 - val_loss: 4913.3894\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6414.5783 - val_loss: 4619.6773\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6310.7718 - val_loss: 4940.5640\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6213.6802 - val_loss: 5843.3661\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6118.3565 - val_loss: 6459.5097\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6020.7634 - val_loss: 6772.7842\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5921.8032 - val_loss: 6586.0791\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5815.8436 - val_loss: 6180.3748\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5701.1616 - val_loss: 5359.1320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█         | 4/36 [15:32<1:52:45, 211.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 200, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 7663.0633 - val_loss: 3792.4850\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7194.9211 - val_loss: 3999.9206\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 6754.3195 - val_loss: 5194.7787\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 6338.1405 - val_loss: 4179.5851\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 5793.5418 - val_loss: 4968.4896\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 5077.4106 - val_loss: 5918.7281\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 4220.6074 - val_loss: 5289.1499\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 3312.4331 - val_loss: 4586.4989\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 2452.3269 - val_loss: 3158.5973\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 1689.8748 - val_loss: 723.8652\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1116.3989 - val_loss: 1410.0962\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 638.5290 - val_loss: 1892.0755\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 368.0785 - val_loss: 2115.4668\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 188.6712 - val_loss: 2038.8771\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 114.9214 - val_loss: 1927.3421\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 68.9691 - val_loss: 2020.4161\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 52.5286 - val_loss: 2075.3029\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 52.9489 - val_loss: 2098.0430\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 42.8070 - val_loss: 2350.0527\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 43.6219 - val_loss: 2247.9320\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 39.3451 - val_loss: 2023.2891\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 40.6619 - val_loss: 1817.0834\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 43.2773 - val_loss: 1073.3450\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 47.6389 - val_loss: 1001.3389\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 36.9307 - val_loss: 967.2802\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 50.7534 - val_loss: 1041.8365\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 57.5392 - val_loss: 15088095123.7887\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 47.5908 - val_loss: 911.0139\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 48.9093 - val_loss: 941.0116\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 47.3271 - val_loss: 948.3799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 5/36 [19:21<1:51:57, 216.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 200, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 7799.8775 - val_loss: 2293.7353\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7560.0962 - val_loss: 7515.0555\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7326.4639 - val_loss: 7910.3722\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7103.9731 - val_loss: 7393.5212\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 6880.9546 - val_loss: 7596.9607\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 6659.2401 - val_loss: 7134.5933\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 6438.8429 - val_loss: 7127.9959\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 6205.4510 - val_loss: 6753.4253\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 5923.9069 - val_loss: 6658.4378\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 5609.9827 - val_loss: 6344.3994\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 5255.3167 - val_loss: 5897.1553\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4861.7504 - val_loss: 5432.4843\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 4444.2228 - val_loss: 4879.2913\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 4017.1986 - val_loss: 4404.7761\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 3569.9491 - val_loss: 11421.4326\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 3114.8320 - val_loss: 4245.9625\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 2678.5155 - val_loss: 3962.2588\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 2439.5048 - val_loss: 3256.5519\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 2032.0574 - val_loss: 2993.0882\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1526.6783 - val_loss: 2109.9939\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1198.1187 - val_loss: 2036.5228\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 940.4472 - val_loss: 1965.4316\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 721.3633 - val_loss: 1858.0895\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 542.4125 - val_loss: 1832.3901\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 400.2783 - val_loss: 1694.7917\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 300.8934 - val_loss: 1558.3920\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 213.9545 - val_loss: 1438.5390\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 150.6237 - val_loss: 1325.7251\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 115.6329 - val_loss: 1225.7323\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 80.2094 - val_loss: 1173.0837\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 67.9604 - val_loss: 1115.8911\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 56.0795 - val_loss: 1018.7699\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 51.1135 - val_loss: 912.6074\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 41.0226 - val_loss: 954.3680\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33.6948 - val_loss: 922.3969\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 31.3624 - val_loss: 865.7004\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 36.4131 - val_loss: 900.5379\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.8394 - val_loss: 887.1503\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.8688 - val_loss: 844.7963\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 30.6520 - val_loss: 845.8956\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24.4119 - val_loss: 801.7463\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 30.3431 - val_loss: 770.0224\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29.7608 - val_loss: 764.7465\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29.2207 - val_loss: 710.5755\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 33.0630 - val_loss: 707.5411\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29.1797 - val_loss: 715.4450\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.3607 - val_loss: 653.2199\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.7302 - val_loss: 614.0985\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.0644 - val_loss: 631.9186\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 30.7753 - val_loss: 600.2623\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 25.3730 - val_loss: 562.8380\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 33.1415 - val_loss: 522.3433\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 28.1791 - val_loss: 415.8622\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.1259 - val_loss: 335.0579\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 34.5080 - val_loss: 330.8037\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 32.7216 - val_loss: 314.3884\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29.3440 - val_loss: 289.5450\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 28.4552 - val_loss: 268.5199\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.7683 - val_loss: 188.8203\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 5s 4ms/step - loss: 27.0593 - val_loss: 196.5800\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.1001 - val_loss: 202.8190\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 28.1577 - val_loss: 275.7207\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.8346 - val_loss: 170.6243\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 28.1021 - val_loss: 128.1307\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 32.1612 - val_loss: 162.9425\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 32.7052 - val_loss: 178.1358\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.1701 - val_loss: 193.3838\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 33.7741 - val_loss: 175.2941\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29.6928 - val_loss: 197.8653\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 33.2363 - val_loss: 190.7468\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 26.7523 - val_loss: 214.5354\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.9524 - val_loss: 279.0438\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.3990 - val_loss: 269.7996\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 28.1271 - val_loss: 296.2559\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 31.2392 - val_loss: 281.8219\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 25.6828 - val_loss: 295.4083\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 23.8669 - val_loss: 236.8662\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.9130 - val_loss: 155.4529\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 30.1731 - val_loss: 243.6165\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 33.8388 - val_loss: 264.0249\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 29.0546 - val_loss: 314.5816\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 39.8437 - val_loss: 371.8527\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 30.5205 - val_loss: 375.1815\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 29.0470 - val_loss: 368.4712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 6/36 [26:02<2:16:04, 272.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 200, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7796.6916 - val_loss: 6867.2271\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7631.2683 - val_loss: 6783.1983\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7476.7929 - val_loss: 7068.7810\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7317.6323 - val_loss: 6677.9633\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7159.6211 - val_loss: 6307.4689\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7015.3937 - val_loss: 6423.0797\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6854.8129 - val_loss: 6853.1031\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6710.0130 - val_loss: 6113.6353\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6572.5294 - val_loss: 5840.3026\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6424.0107 - val_loss: 6041.4410\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6270.7735 - val_loss: 6207.5079\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6103.5419 - val_loss: 5554.4530\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5901.3083 - val_loss: 5856.9004\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5682.8948 - val_loss: 5701.1688\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5440.6334 - val_loss: 5184.5584\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5182.9796 - val_loss: 4796.0269\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4906.0896 - val_loss: 4754.1341\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4623.8515 - val_loss: 3935.1971\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4320.3747 - val_loss: 3791.0202\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4001.8812 - val_loss: 3580.0292\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3690.0283 - val_loss: 3628.2449\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3372.2877 - val_loss: 3521.2778\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3060.7741 - val_loss: 2919.8600\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2746.0335 - val_loss: 2863.5517\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 2445.2542 - val_loss: 2222.2068\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 2172.7907 - val_loss: 1841.0097\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1895.6811 - val_loss: 1841.0126\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1648.4503 - val_loss: 1504.5087\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1414.9864 - val_loss: 1517.7087\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1210.0287 - val_loss: 1101.0735\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1024.2523 - val_loss: 1015.4368\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 853.8574 - val_loss: 736.4575\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 699.0090 - val_loss: 669.2988\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 572.9282 - val_loss: 541.6405\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 464.2371 - val_loss: 462.8788\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 374.6132 - val_loss: 371.5470\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 294.2759 - val_loss: 579.2381\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 241.7031 - val_loss: 439.3024\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 183.3413 - val_loss: 212.6184\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 144.5335 - val_loss: 222.1533\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 110.1142 - val_loss: 169.9479\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 87.9413 - val_loss: 119.0313\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 72.2634 - val_loss: 64.0318\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 52.3250 - val_loss: 30.8893\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 47.6395 - val_loss: 47.0609\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 50.6710 - val_loss: 52.7204\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 37.5415 - val_loss: 314.9939\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 40.4223 - val_loss: 75.5083\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 30.4431 - val_loss: 20.2572\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 24.3423 - val_loss: 19.2156\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.6849 - val_loss: 66.7861\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 23.8026 - val_loss: 19.4277\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.9703 - val_loss: 75.7223\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 19.0892 - val_loss: 328.2598\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 17.3038 - val_loss: 13.3363\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 22.0930 - val_loss: 21.5469\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 22.8235 - val_loss: 38.1141\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 24.1573 - val_loss: 30.1850\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.8457 - val_loss: 61.5099\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.5697 - val_loss: 20.3449\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 24.1607 - val_loss: 30.4686\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 15.2810 - val_loss: 40.6423\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.7919 - val_loss: 20.9610\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 15.3254 - val_loss: 50.5710\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 13.5279 - val_loss: 57.4740\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 13.4193 - val_loss: 12.3659\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 24.9513 - val_loss: 15.0489\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 22.0181 - val_loss: 23.8128\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 24.6814 - val_loss: 15.0711\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 21.9233 - val_loss: 23.3364\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.1333 - val_loss: 25.4302\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 15.0822 - val_loss: 145.7113\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 24.3285 - val_loss: 60.7616\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 18.7946 - val_loss: 43.3309\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.7421 - val_loss: 45.7932\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 17.7143 - val_loss: 19.9346\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 19.8383 - val_loss: 12.2867\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 23.9031 - val_loss: 47.2160\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.2448 - val_loss: 15.2245\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.2056 - val_loss: 19.5529\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 21.3427 - val_loss: 15.6361\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 18.6922 - val_loss: 12.7754\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 19.0826 - val_loss: 15.5620\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.7086 - val_loss: 21.6459\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 23.6446 - val_loss: 40.9978\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.1950 - val_loss: 11.7951\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 19.0702 - val_loss: 12.2017\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 18.9718 - val_loss: 12.1879\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 19.7544 - val_loss: 16.2323\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 23.9151 - val_loss: 12.9707\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 15.9487 - val_loss: 16.5154\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.1175 - val_loss: 12.8414\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 13.7842 - val_loss: 25.7530\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.7703 - val_loss: 22.2675\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 18.8784 - val_loss: 13.1503\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.8069 - val_loss: 27.1287\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 21.6407 - val_loss: 17.1694\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 17.3427 - val_loss: 15.3160\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 21.4010 - val_loss: 77.0606\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 18.7830 - val_loss: 68.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|█▉        | 7/36 [33:01<2:32:50, 316.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 200, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7853.9224 - val_loss: 5399.1971\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7724.6433 - val_loss: 5072.9609\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7601.4513 - val_loss: 5289.6472\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7488.1966 - val_loss: 5508.7810\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7373.0492 - val_loss: 6075.0881\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7250.2513 - val_loss: 5609.9871\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7136.5000 - val_loss: 6066.3879\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7014.6449 - val_loss: 6020.1427\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6906.5135 - val_loss: 6009.1068\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6794.9966 - val_loss: 5679.9490\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6686.6324 - val_loss: 5898.2020\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6590.5161 - val_loss: 5847.0633\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6488.5043 - val_loss: 5939.9364\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6383.2996 - val_loss: 6207.7841\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6270.2182 - val_loss: 6251.6423\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6139.9593 - val_loss: 4808.3875\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6005.0725 - val_loss: 4594.0872\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5859.3647 - val_loss: 4187.8533\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5701.2768 - val_loss: 5466.7167\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5533.5645 - val_loss: 5461.9620\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5351.0586 - val_loss: 4986.0993\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 5164.4941 - val_loss: 5167.8523\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4962.6097 - val_loss: 5428.2850\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4763.1837 - val_loss: 5491.1746\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4545.8493 - val_loss: 5339.4272\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4327.9382 - val_loss: 5443.3100\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 4108.4361 - val_loss: 4846.5890\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3878.0900 - val_loss: 5048.9192\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3654.6716 - val_loss: 4192.5492\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3429.3536 - val_loss: 3961.8416\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 3199.7810 - val_loss: 3855.7679\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2977.5639 - val_loss: 3467.2446\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2757.6357 - val_loss: 3716.9416\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2545.1508 - val_loss: 3678.2291\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2333.1205 - val_loss: 3336.9838\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 2128.2128 - val_loss: 2718.4768\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1938.9265 - val_loss: 2492.5712\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1757.0182 - val_loss: 2270.1172\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1582.7962 - val_loss: 2217.4034\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1419.5879 - val_loss: 1292.2730\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1265.8036 - val_loss: 1841.0857\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 1120.6538 - val_loss: 1866.7430\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 995.5897 - val_loss: 1140.0843\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 871.0768 - val_loss: 1464.7984\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 763.1338 - val_loss: 1119.4649\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 657.1082 - val_loss: 1203.9605\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 571.8913 - val_loss: 750.2338\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 495.7686 - val_loss: 840.0092\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 420.1242 - val_loss: 800.0422\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 357.0888 - val_loss: 309.5099\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 306.7100 - val_loss: 571.8346\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 263.3185 - val_loss: 623.5755\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 211.7369 - val_loss: 601.6384\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 177.1645 - val_loss: 290.1073\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 150.1279 - val_loss: 313.1253\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 129.1021 - val_loss: 407.0475\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 107.0377 - val_loss: 155.8833\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 87.1381 - val_loss: 90.2175\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 4s 3ms/step - loss: 79.3969 - val_loss: 263.2451\n",
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 60.4418 - val_loss: 246.6938\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 54.5532 - val_loss: 158.1124\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 45.5142 - val_loss: 80.9010\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 37.7563 - val_loss: 284.1270\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 36.1806 - val_loss: 17.9896\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 28.5597 - val_loss: 226.2708\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 24.7981 - val_loss: 100.9281\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 22.0844 - val_loss: 41.1699\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 23.1630 - val_loss: 222.1427\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 21.8694 - val_loss: 213.2216\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 18.8069 - val_loss: 243.1730\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 15.8084 - val_loss: 116.8079\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.8816 - val_loss: 177.5892\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 18.2648 - val_loss: 75.3702\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 15.0222 - val_loss: 87.3541\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.2069 - val_loss: 49.3499\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 18.2370 - val_loss: 42.2342\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.0599 - val_loss: 102.0920\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.9793 - val_loss: 14.3430\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.7539 - val_loss: 21.3696\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 13.6629 - val_loss: 17.6488\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 12.7503 - val_loss: 17.2465\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.2927 - val_loss: 46.3520\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 12.8833 - val_loss: 17.6368\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 18.4032 - val_loss: 15.0394\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.5501 - val_loss: 23.9230\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.0917 - val_loss: 34.3187\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 16.6585 - val_loss: 87.8943\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.9330 - val_loss: 136.3982\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.0705 - val_loss: 24.5846\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 13.7895 - val_loss: 30.9758\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 21.5702 - val_loss: 129.7727\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 13.9893 - val_loss: 331.7353\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 20.4267 - val_loss: 15.7589\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 17.9179 - val_loss: 18.8746\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.3619 - val_loss: 591.2679\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 13.2612 - val_loss: 154.6194\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.5362 - val_loss: 27.9530\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 14.4202 - val_loss: 15.7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 8/36 [39:39<2:39:00, 340.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 400, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 18s 15ms/step - loss: 7518.6212 - val_loss: 2841.2696\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 15s 12ms/step - loss: 6952.6323 - val_loss: 3691.3179\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 6480.9738 - val_loss: 3718.6618\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 5807.4528 - val_loss: 1541.7215\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 4689.4785 - val_loss: 2699.8945\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 3434.6199 - val_loss: 2530.3106\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 2200.9462 - val_loss: 2399.9811\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 1219.4196 - val_loss: 1213.4641\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 578.1096 - val_loss: 706.1525\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 253.4254 - val_loss: 98.6803\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 107.1501 - val_loss: 345.9539\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 63.2546 - val_loss: 250.0745\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 51.0718 - val_loss: 419.7270\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 46.6146 - val_loss: 491.2840\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 30.9237 - val_loss: 4031779.5827\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 42.0395 - val_loss: 528.7655\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 47.8724 - val_loss: 159.0649\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 46.1353 - val_loss: 10989945481.2681\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 46.8592 - val_loss: 488.3454\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 38.5548 - val_loss: 551.6499\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 11s 10ms/step - loss: 46.1777 - val_loss: 579.0281\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 55.0777 - val_loss: 258.1297\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 40.7361 - val_loss: 243.6368\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 43.0109 - val_loss: 398.6352\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 39.9697 - val_loss: 268.4986\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 46.0905 - val_loss: 117.6940\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 37.7371 - val_loss: 121.8490\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 48.9456 - val_loss: 240.0378\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 41.9189 - val_loss: 161.4384\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 12s 10ms/step - loss: 42.2283 - val_loss: 182.1441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 9/36 [46:06<2:39:32, 354.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 400, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 7724.7408 - val_loss: 47375.9838\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 7360.6329 - val_loss: 2253.3849\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 7081.2323 - val_loss: 6054.3989\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6830.8815 - val_loss: 6920.9782\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6578.7906 - val_loss: 7217.8643\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6306.1878 - val_loss: 7234.7073\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 5946.3066 - val_loss: 7487.4637\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 5463.3641 - val_loss: 6579.3043\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 4928.8519 - val_loss: 7063.9774\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 4333.3369 - val_loss: 4830.5363\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 3691.6479 - val_loss: 4479.1843\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 3046.2261 - val_loss: 3930.0989\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2428.9231 - val_loss: 3067.8810\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1896.8932 - val_loss: 80480072217.0489\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1419.0037 - val_loss: 3890.1943\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 968.1942 - val_loss: 179.0258\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 656.1799 - val_loss: 469.2692\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 426.1120 - val_loss: 201.0949\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 14s 11ms/step - loss: 277.1950 - val_loss: 200.2595\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 170.8504 - val_loss: 365.7948\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 102.7960 - val_loss: 433.4682\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 68.3374 - val_loss: 425.3317\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 57.0897 - val_loss: 454.6784\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 39.4281 - val_loss: 367.6262\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 31.3634 - val_loss: 331.3963\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 35.0582 - val_loss: 327.0288\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 40.0839 - val_loss: 324.3932\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 35.4795 - val_loss: 295.3605\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 33.2799 - val_loss: 288.3016\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 30.3272 - val_loss: 321.1729\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 32.4137 - val_loss: 306.2843\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 35.2221 - val_loss: 272.1075\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 29.4504 - val_loss: 255.8730\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 32.4257 - val_loss: 241.2790\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 29.2991 - val_loss: 227.8775\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 28.9115 - val_loss: 248.5364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 10/36 [52:44<2:39:18, 367.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 400, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 13s 11ms/step - loss: 7741.0141 - val_loss: 1894541.0201\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 7461.1675 - val_loss: 7455.2512\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 7230.4669 - val_loss: 2173.4837\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 7037.0622 - val_loss: 5013.1914\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6879.8670 - val_loss: 6752.2299\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6742.9689 - val_loss: 6785.2751\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 6583.5222 - val_loss: 6089.6746\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6399.6006 - val_loss: 7156.4734\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 6167.7065 - val_loss: 6770.5134\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 5896.7609 - val_loss: 7180.2097\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 5567.1287 - val_loss: 7772.7647\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 5208.8106 - val_loss: 11389.3434\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 4808.2451 - val_loss: 7819.5849\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 4372.9277 - val_loss: 7617.6391\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 3929.1432 - val_loss: 2783.8883\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 3488.7209 - val_loss: 4726.6205\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 3266.0280 - val_loss: 11377.0535\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2574.2552 - val_loss: 4128.3344\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 2203.0595 - val_loss: 10288804.5460\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1819.7072 - val_loss: 3027.8089\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1431.8879 - val_loss: 2420.7427\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 1112.6556 - val_loss: 1879.7026\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 854.6699 - val_loss: 1484.8144\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 642.8948 - val_loss: 1208.6445\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 473.1228 - val_loss: 1025.7349\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 347.1053 - val_loss: 892.1472\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 251.9399 - val_loss: 813.4568\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 181.2176 - val_loss: 762.6074\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 131.5068 - val_loss: 739.9397\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 91.1032 - val_loss: 735.4901\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 67.6582 - val_loss: 744.2567\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 54.9252 - val_loss: 756.8434\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 49.1690 - val_loss: 775.5324\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 38.3063 - val_loss: 792.8819\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 10s 8ms/step - loss: 41.0009 - val_loss: 812.3728\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 28.1169 - val_loss: 829.3818\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 31.1151 - val_loss: 839.1542\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 21.2559 - val_loss: 847.9169\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 24.0629 - val_loss: 858.6572\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 27.9490 - val_loss: 853.4988\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 26.5094 - val_loss: 840.6037\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 26.2731 - val_loss: 836.9134\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 27.2343 - val_loss: 839.5963\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 28.0098 - val_loss: 14000305.2916\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 25.8980 - val_loss: 826.0832\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 32.4878 - val_loss: 811.8307\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 27.7721 - val_loss: 795.7590\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 27.2011 - val_loss: 782.7543\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 30.8201 - val_loss: 770.6437\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 26.6629 - val_loss: 760.4770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|███       | 11/36 [1:00:38<2:46:26, 399.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 400, 'LSTM_dropout': 0}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7793.1642 - val_loss: 928555.4374\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7576.5782 - val_loss: 95561.7659\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7400.6706 - val_loss: 908.8762\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7244.1603 - val_loss: 4841.9605\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7112.8909 - val_loss: 6509.5514\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6982.8684 - val_loss: 5569.9245\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6865.0432 - val_loss: 5249.9397\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6759.8434 - val_loss: 6023.1493\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6641.0068 - val_loss: 5555.8177\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6529.1301 - val_loss: 5592.3789\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6380.6998 - val_loss: 5331.8596\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6219.7559 - val_loss: 5786.7138\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 6040.3727 - val_loss: 5832.0554\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5830.1957 - val_loss: 5806.2602\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 5613.6927 - val_loss: 5510.6196\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 5362.6271 - val_loss: 5185.0852\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 5096.0322 - val_loss: 5145.1547\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 4807.2607 - val_loss: 5085.8993\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 4512.3765 - val_loss: 4708.1055\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 4193.1224 - val_loss: 4370.8320\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 3869.6577 - val_loss: 4080.9842\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 3545.7056 - val_loss: 3755.4623\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3228.8227 - val_loss: 3315.4280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 12/36 [1:02:50<2:07:40, 319.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 100, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7819.8730 - val_loss: 7578.7521\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7556.6267 - val_loss: 7602.9945\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7233.7855 - val_loss: 7649.6610\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6852.6428 - val_loss: 7416.8024\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6429.6935 - val_loss: 6965.8442\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5978.1582 - val_loss: 6317.8124\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5492.7911 - val_loss: 4414.9160\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 4973.9687 - val_loss: 4922.8827\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 4381.4564 - val_loss: 3881.7438\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 3782.1002 - val_loss: 3651.0368\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 3154.4620 - val_loss: 3676.2520\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 2598.5210 - val_loss: 2181.7136\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 2037.9563 - val_loss: 743.2207\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1536.9401 - val_loss: 836.6499\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1135.9596 - val_loss: 646453.1402\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 826.5135 - val_loss: 496.5166\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 569.9671 - val_loss: 928.7760\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 385.7437 - val_loss: 177.6852\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 270.3837 - val_loss: 467.8325\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 179.3629 - val_loss: 290.0188\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 139.4398 - val_loss: 84.3382\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 96.3356 - val_loss: 40.6099\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 107.1652 - val_loss: 82420.4508\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 84.6360 - val_loss: 2425.3604\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 65.3520 - val_loss: 263.4941\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 74.4167 - val_loss: 139.0350\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 65.2009 - val_loss: 118.6466\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 63.2790 - val_loss: 309.5149\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 64.5839 - val_loss: 401.6593\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 56.2620 - val_loss: 400.0131\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 57.2748 - val_loss: 385.3932\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 55.2779 - val_loss: 187.3956\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 57.1001 - val_loss: 349.6386\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 64.5189 - val_loss: 393.8350\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 60.7928 - val_loss: 827.6413\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 56.3297 - val_loss: 547.4841\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 58.1103 - val_loss: 490.1336\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 48.1361 - val_loss: 386.7913\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 56.1037 - val_loss: 318.6673\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 59.2252 - val_loss: 316.1226\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 54.7433 - val_loss: 244.7396\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 58.0641 - val_loss: 226.2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▌      | 13/36 [1:05:15<1:42:21, 267.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 100, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7839.6558 - val_loss: 7200.5929\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7702.9335 - val_loss: 6778.5676\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7560.8990 - val_loss: 6226.7316\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7403.9642 - val_loss: 5641.5504\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7234.3694 - val_loss: 5633.3120\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7056.1906 - val_loss: 5588.6085\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6866.9095 - val_loss: 4462.3357\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6685.6458 - val_loss: 6406.7787\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6479.2637 - val_loss: 6422.6549\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6293.4843 - val_loss: 6359.9338\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6113.0421 - val_loss: 6570.8155\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5884.4305 - val_loss: 5725.9495\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5668.2652 - val_loss: 4310.5310\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5425.0647 - val_loss: 3145.8357\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5172.0323 - val_loss: 5766.6147\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4882.0592 - val_loss: 5396.2185\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4613.2928 - val_loss: 4999.2541\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4298.0178 - val_loss: 6985.0995\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3989.9983 - val_loss: 5764.2260\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3677.3882 - val_loss: 4854.0575\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3383.5279 - val_loss: 4322.0706\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 3067.6947 - val_loss: 3600.5099\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 2781.8543 - val_loss: 3762.0031\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2485.3092 - val_loss: 3428.0548\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2196.9912 - val_loss: 3290.6386\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1930.1595 - val_loss: 3220.2313\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1689.6989 - val_loss: 3220.9158\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1463.3794 - val_loss: 2811.6508\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1265.2457 - val_loss: 2520.3899\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1077.9978 - val_loss: 2215.5367\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 892.3366 - val_loss: 3754.8884\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 749.8622 - val_loss: 1155.9742\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 626.9191 - val_loss: 985.9721\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 531.8792 - val_loss: 1212.2199\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 425.4817 - val_loss: 1222.1024\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 344.0037 - val_loss: 889.2710\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 272.6676 - val_loss: 1296.1699\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 227.0259 - val_loss: 1665.0236\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 180.0370 - val_loss: 1521.5939\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 149.3224 - val_loss: 886.5043\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 115.6921 - val_loss: 941.9896\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 95.4128 - val_loss: 422.3985\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 83.8948 - val_loss: 392.1251\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 80.7960 - val_loss: 266.7290\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 58.6771 - val_loss: 202.5728\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 57.0228 - val_loss: 215.3857\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 48.1142 - val_loss: 202.4872\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 41.3535 - val_loss: 140.5030\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 47.2295 - val_loss: 113.6763\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 55.1659 - val_loss: 69.7128\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 44.1925 - val_loss: 79.9928\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 41.0213 - val_loss: 73.2000\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 34.4142 - val_loss: 76.2877\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 37.6488 - val_loss: 68.4061\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 39.2882 - val_loss: 63.9120\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 38.9390 - val_loss: 30.1675\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 46.4819 - val_loss: 40.1317\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 36.4151 - val_loss: 37.5098\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 37.5193 - val_loss: 73.8809\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 2s 2ms/step - loss: 43.5922 - val_loss: 34.0547\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 38.3728 - val_loss: 35.4340\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 42.0215 - val_loss: 26.9984\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 37.6465 - val_loss: 40.3000\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 34.4473 - val_loss: 55.9187\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 41.1509 - val_loss: 30.7003\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 40.3668 - val_loss: 31.3977\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 35.9800 - val_loss: 23.7135\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 38.9054 - val_loss: 30.0020\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 40.3103 - val_loss: 26.7337\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 39.1720 - val_loss: 20.4106\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 38.8945 - val_loss: 20.2580\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 33.5083 - val_loss: 20.7823\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 36.2498 - val_loss: 36.4621\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 39.5468 - val_loss: 26.5112\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 38.4968 - val_loss: 29.1346\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 41.8346 - val_loss: 37.8253\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 32.9157 - val_loss: 17.7415\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 42.2336 - val_loss: 32.4133\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 35.9968 - val_loss: 22.8783\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 33.9692 - val_loss: 29.1981\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 39.4025 - val_loss: 25.8565\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 33.5320 - val_loss: 31.3537\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 34.7377 - val_loss: 17.2764\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 34.6426 - val_loss: 44.6514\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 42.4422 - val_loss: 21.9819\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 38.1594 - val_loss: 30.6366\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 31.8380 - val_loss: 17.1435\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 40.6027 - val_loss: 23.8643\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 39.6720 - val_loss: 30.8908\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 42.0083 - val_loss: 20.0132\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 35.0798 - val_loss: 13.2052\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 43.8614 - val_loss: 17.5542\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 36.5954 - val_loss: 14.3101\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 35.1538 - val_loss: 15.7900\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 40.8820 - val_loss: 21.9411\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 35.7755 - val_loss: 12.8543\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 40.2384 - val_loss: 17.1888\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 31.2443 - val_loss: 17.3188\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 32.8246 - val_loss: 13.8555\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 37.1524 - val_loss: 12.8057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███▉      | 14/36 [1:08:43<1:31:26, 249.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 100, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7882.6277 - val_loss: 13869.8996\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7784.7777 - val_loss: 6230.5237\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7681.9759 - val_loss: 2756.6356\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7577.5170 - val_loss: 3750.1508\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7464.1126 - val_loss: 3438.5853\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7346.7739 - val_loss: 3075.0590\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7222.6154 - val_loss: 4623.6147\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7097.7465 - val_loss: 4513.1167\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6965.7926 - val_loss: 4550.9681\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6842.0596 - val_loss: 3997.7755\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6700.8697 - val_loss: 3636.9299\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6568.5625 - val_loss: 4229.9021\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6425.2801 - val_loss: 5094.6601\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6286.4547 - val_loss: 6414.3401\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6150.2213 - val_loss: 5998.8218\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6005.6450 - val_loss: 4703.2194\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5852.3871 - val_loss: 6901.6262\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5702.6192 - val_loss: 4985.3807\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5539.2925 - val_loss: 5518.5691\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5360.2752 - val_loss: 5661.3907\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5170.8904 - val_loss: 5176.9418\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 4992.4741 - val_loss: 5215.8881\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 4797.2947 - val_loss: 4743.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 15/36 [1:09:28<1:05:45, 187.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 100, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7885.9136 - val_loss: 6422.7989\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7809.8042 - val_loss: 6597.2240\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7744.8907 - val_loss: 6594.2815\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7676.9016 - val_loss: 6624.4984\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7607.6811 - val_loss: 7330.3569\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7530.3034 - val_loss: 6534.2784\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7451.2515 - val_loss: 6169.4222\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7369.8516 - val_loss: 6697.9597\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7281.4287 - val_loss: 6112.1662\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7190.1234 - val_loss: 6254.8970\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7103.8581 - val_loss: 6384.1608\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7008.6902 - val_loss: 5878.7997\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6913.3675 - val_loss: 3124.9077\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6813.3826 - val_loss: 3866.9316\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6725.9111 - val_loss: 1316.6181\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6623.4627 - val_loss: 987.7622\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6519.7185 - val_loss: 3889.4269\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6433.7536 - val_loss: 4908.9162\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6319.5660 - val_loss: 5397.2083\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6228.4520 - val_loss: 4869.2431\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6127.7094 - val_loss: 5086.1158\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6015.8044 - val_loss: 4982.9039\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5909.8409 - val_loss: 4580.5541\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5810.2065 - val_loss: 4611.0891\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5701.5968 - val_loss: 4258.2451\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5580.8571 - val_loss: 4302.1653\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5468.1617 - val_loss: 5589.4720\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5336.1276 - val_loss: 5809.8446\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5207.5743 - val_loss: 5574.3994\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5062.3899 - val_loss: 6035.5214\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 4927.7657 - val_loss: 4810.9853\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 4787.9485 - val_loss: 3657.5144\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 4631.9943 - val_loss: 3642.0360\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 4491.5517 - val_loss: 3793.3591\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 4347.2238 - val_loss: 4076.2834\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 4184.7891 - val_loss: 4107.5236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████▍     | 16/36 [1:10:23<49:21, 148.08s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 200, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 5ms/step - loss: 7689.8750 - val_loss: 5770.6168\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7259.6286 - val_loss: 7711.1470\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6822.1331 - val_loss: 7347.7588\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6371.3277 - val_loss: 8485.4577\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 5834.8099 - val_loss: 6684.2871\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5219.7552 - val_loss: 8961.5591\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 4674.1548 - val_loss: 9022.0683\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3475.8869 - val_loss: 6659.0275\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 2562.6281 - val_loss: 6041.6480\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1793.5549 - val_loss: 4788.0229\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1195.8699 - val_loss: 4124.2991\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 739.6969 - val_loss: 3168.8859\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 433.2976 - val_loss: 2463.4218\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 246.7903 - val_loss: 1900.7652\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 138.1768 - val_loss: 1462.8681\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 93.6658 - val_loss: 1289.9438\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 69.9156 - val_loss: 1035.9284\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 57.4533 - val_loss: 840.3552\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 53.7777 - val_loss: 745.9045\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 60.5604 - val_loss: 646.1129\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 51.8636 - val_loss: 600.6939\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 52.7915 - val_loss: 515.8527\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 57.0787 - val_loss: 421.6602\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 65.7020 - val_loss: 351.6584\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 54.1564 - val_loss: 319.9680\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 58.4930 - val_loss: 362.5327\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 60.9044 - val_loss: 332.8542\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 59.6121 - val_loss: 379.0285\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 61.7962 - val_loss: 383.7957\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 55.0511 - val_loss: 574.9115\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 58.3448 - val_loss: 8381.7578\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 57.8052 - val_loss: 213.5993\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 51.0579 - val_loss: 145.7590\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 53.1198 - val_loss: 131.8512\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 50.6485 - val_loss: 200.4168\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 55.5819 - val_loss: 139.5999\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 53.1104 - val_loss: 152.6883\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 65.2156 - val_loss: 138.7321\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 51.1444 - val_loss: 146.0579\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 49.8628 - val_loss: 164.4553\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 54.6185 - val_loss: 162.8900\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 68.6858 - val_loss: 139.3122\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 55.8264 - val_loss: 105.2754\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 65.5913 - val_loss: 119.9805\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 58.2717 - val_loss: 91.3135\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 56.3438 - val_loss: 93.2698\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 49.1115 - val_loss: 126.5452\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 65.5359 - val_loss: 59.6426\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 66.5446 - val_loss: 78.5358\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 45.3763 - val_loss: 72.0364\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 58.1834 - val_loss: 94.0601\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 52.1601 - val_loss: 47.2516\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 53.0698 - val_loss: 110.0703\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 53.3396 - val_loss: 99.5613\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 54.5996 - val_loss: 80.1088\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 59.5748 - val_loss: 157.4720\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 59.0412 - val_loss: 666.2557\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 48.9615 - val_loss: 85.5719\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 57.5133 - val_loss: 106.0097\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 4s 4ms/step - loss: 52.4199 - val_loss: 149.6903\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 56.9382 - val_loss: 40.6136\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 52.1313 - val_loss: 4492.4220\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 55.6916 - val_loss: 3353767035363.9106\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 66.7781 - val_loss: 5102.6776\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 59.8070 - val_loss: 1476.9045\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 58.2221 - val_loss: 111.3314\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 51.9182 - val_loss: 29.9910\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 49.8924 - val_loss: 78018696506119766016.0000\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 60.2383 - val_loss: 49.5759\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 54.8353 - val_loss: 63.7235\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 61.5298 - val_loss: 172.7718\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 66.0725 - val_loss: 487.9832\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 53.6948 - val_loss: 60.5107\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 50.6555 - val_loss: 65.7378\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 51.9296 - val_loss: 55.4373\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 48.8150 - val_loss: 420.7600\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 56.8230 - val_loss: 60.0552\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 55.2505 - val_loss: 1155.6714\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 41.9841 - val_loss: 249.8773\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 55.0316 - val_loss: 62.0545\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 50.2298 - val_loss: 41.5628\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 51.2926 - val_loss: 72.9575\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 56.6138 - val_loss: 81.7365\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 52.3814 - val_loss: 36.8567\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 68.3387 - val_loss: 75.6555\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 53.4652 - val_loss: 96.4751\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 62.5866 - val_loss: 51.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 17/36 [1:16:59<1:10:26, 222.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 200, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7809.8639 - val_loss: 44319.6743\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7586.4747 - val_loss: 7188.9824\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7368.3629 - val_loss: 6512.1170\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7148.1215 - val_loss: 6520.7149\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6926.2200 - val_loss: 7230.9704\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6708.6929 - val_loss: 6904.1809\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6459.5504 - val_loss: 7045.5452\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6240.4722 - val_loss: 6958.2933\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5938.8519 - val_loss: 6504.6784\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5599.0732 - val_loss: 6588.5478\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5245.8917 - val_loss: 13801832364124.1797\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4858.5844 - val_loss: 5882.7693\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4443.6639 - val_loss: 5586.3872\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4011.1765 - val_loss: 2622187642649790.5000\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3591.2902 - val_loss: 4459.9493\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3143.6100 - val_loss: 3851.3078\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2719.7338 - val_loss: 3516.2469\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2305.4153 - val_loss: 2856.7993\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1934.9423 - val_loss: 2354.0060\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1644.6613 - val_loss: 8753.5285\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1305.2569 - val_loss: 2090.0518\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1012.1945 - val_loss: 643.7305\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 781.4468 - val_loss: 511.0261\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 584.9458 - val_loss: 493.9845\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 440.4448 - val_loss: 496.2359\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 332.7757 - val_loss: 525.6728\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 240.6370 - val_loss: 625.7305\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 186.0067 - val_loss: 574.1744\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 139.9002 - val_loss: 1171.1444\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 102.3464 - val_loss: 1050.7831\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 77.4690 - val_loss: 847.7101\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 66.3858 - val_loss: 787.6029\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 58.5409 - val_loss: 727.4110\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 44.4630 - val_loss: 646.8382\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.0968 - val_loss: 656.3820\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.1489 - val_loss: 604.7606\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 48.8319 - val_loss: 672.9173\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 44.1263 - val_loss: 575.5454\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 42.7718 - val_loss: 547.6843\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.6520 - val_loss: 543.0129\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 40.4966 - val_loss: 540.5270\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 40.4548 - val_loss: 501.6955\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 39.2908 - val_loss: 472.0062\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 39.7424 - val_loss: 461.4977\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 41.0233 - val_loss: 559.9775\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 35.4800 - val_loss: 526.1343\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 36.1544 - val_loss: 495.8919\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 44.7925 - val_loss: 396.2833\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 39.7126 - val_loss: 356.4996\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 38.7713 - val_loss: 364.6989\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 36.8375 - val_loss: 354.1455\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 34.5607 - val_loss: 360.5347\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 33.1909 - val_loss: 337.9063\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 36.2136 - val_loss: 294.5892\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 38.5854 - val_loss: 311.4489\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 39.3900 - val_loss: 432.0171\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 38.0907 - val_loss: 522.1215\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 42.6562 - val_loss: 491.8782\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 37.2130 - val_loss: 459.6705\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 3s 2ms/step - loss: 34.5935 - val_loss: 459.8664\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 38.9556 - val_loss: 479.6463\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 36.6586 - val_loss: 431.8625\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.9723 - val_loss: 420.7406\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 38.0968 - val_loss: 403.1265\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 51.5050 - val_loss: 418.5983\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 39.1765 - val_loss: 388.4892\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 40.3017 - val_loss: 314.8029\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 35.8965 - val_loss: 274.0657\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 45.8301 - val_loss: 326.8701\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 40.6788 - val_loss: 291.3336\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 33.1064 - val_loss: 263.7486\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 40.0424 - val_loss: 133.0466\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.0824 - val_loss: 173.6842\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 39.5319 - val_loss: 548.7640\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 50.6557 - val_loss: 27009.9390\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 44.6324 - val_loss: 253.3183\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 44.6489 - val_loss: 204.4000\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 41.9976 - val_loss: 234.2495\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 48.3372 - val_loss: 263.4909\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 41.1027 - val_loss: 335.5354\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.3376 - val_loss: 283.9796\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 37.1137 - val_loss: 239.9336\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 53.0272 - val_loss: 231.3101\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 42.9335 - val_loss: 140.9628\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 42.4479 - val_loss: 603.8618\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 39.7185 - val_loss: 612.8554\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 42.5483 - val_loss: 252.1321\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.0959 - val_loss: 379.7820\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.4218 - val_loss: 442.2329\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 33.8732 - val_loss: 172.1332\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 38.3542 - val_loss: 159.2316\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 38.9973 - val_loss: 166.8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 18/36 [1:21:28<1:10:59, 236.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 200, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7832.3647 - val_loss: 8622.7094\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7662.8692 - val_loss: 2177.3420\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7503.2227 - val_loss: 5142.6945\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7348.7767 - val_loss: 5978.4679\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7192.4369 - val_loss: 6303.1211\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7036.1136 - val_loss: 6835.0089\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6876.9925 - val_loss: 6469.2725\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6719.3529 - val_loss: 5929.9356\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6584.5410 - val_loss: 6265.3990\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6422.7996 - val_loss: 5524.2697\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6257.6412 - val_loss: 5795.4765\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6083.5274 - val_loss: 6415.9879\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5882.7406 - val_loss: 6544.0477\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5653.8831 - val_loss: 6452.8871\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5406.2991 - val_loss: 5469.3860\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5151.5945 - val_loss: 5558.9005\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4889.7871 - val_loss: 5635.2422\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4591.5675 - val_loss: 5401.0262\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4298.4094 - val_loss: 4967.3219\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3991.0296 - val_loss: 3873.4819\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3679.4624 - val_loss: 3083.3420\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3367.9729 - val_loss: 4039.5724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 19/36 [1:22:29<52:06, 183.91s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 200, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7845.2295 - val_loss: 34649.8512\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7715.6033 - val_loss: 3009.9134\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7600.5865 - val_loss: 5266.9726\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7485.9839 - val_loss: 6498.2857\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7375.0545 - val_loss: 7082.2624\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7260.9606 - val_loss: 7046.0749\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7146.2159 - val_loss: 6899.3007\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7038.4718 - val_loss: 6818.6777\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6922.1279 - val_loss: 6148.9507\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6812.7346 - val_loss: 6533.3699\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6702.1454 - val_loss: 6469.0420\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6603.2788 - val_loss: 7147.3076\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6491.5390 - val_loss: 6954.7501\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6376.7704 - val_loss: 5395.9035\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6267.4858 - val_loss: 5767.5791\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6130.2606 - val_loss: 5870.8324\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5999.5820 - val_loss: 5174.3784\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5848.0564 - val_loss: 4748.4563\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5697.9050 - val_loss: 2926.6271\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5531.0375 - val_loss: 3875.8815\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5356.0916 - val_loss: 1526.5310\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5169.3514 - val_loss: 2663.4114\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4980.6430 - val_loss: 4115.4863\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4765.6916 - val_loss: 4738.0750\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4546.6664 - val_loss: 3787.6281\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4340.0547 - val_loss: 4516.1615\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4124.9995 - val_loss: 5195.0325\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3898.6133 - val_loss: 5595.2176\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3684.0300 - val_loss: 5317.6009\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3462.7713 - val_loss: 5029.6267\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3237.7815 - val_loss: 4519.5845\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3010.9817 - val_loss: 4053.9069\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2783.6857 - val_loss: 3950.8253\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2585.8132 - val_loss: 3545.4353\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2376.7141 - val_loss: 3518.0706\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2172.9878 - val_loss: 3305.6052\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1993.5848 - val_loss: 2881.6264\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1803.7172 - val_loss: 2693.3660\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1625.0062 - val_loss: 2585.7453\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1469.2539 - val_loss: 2294.9386\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1304.5640 - val_loss: 2077.8953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████▌    | 20/36 [1:24:12<42:33, 159.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 400, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 11s 9ms/step - loss: 7532.0356 - val_loss: 1477.8055\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 6973.5255 - val_loss: 4154.9421\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 6484.5931 - val_loss: 6576.1684\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 5801.3598 - val_loss: 5586.2613\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 4779.8372 - val_loss: 5999.7295\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3618.8043 - val_loss: 4930.6853\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2373.0008 - val_loss: 2791.2864\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 1339.8677 - val_loss: 1549.2782\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 658.5377 - val_loss: 912.9115\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 286.0366 - val_loss: 642.3692\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 138.9678 - val_loss: 547.5079\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 76.3273 - val_loss: 513.9446\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 54.6825 - val_loss: 506.8690\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 56.6065 - val_loss: 438.2629\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 53.4490 - val_loss: 397.1581\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 56.5653 - val_loss: 370.5182\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 46.5052 - val_loss: 344.5481\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 53.4330 - val_loss: 303.6566\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 51.9804 - val_loss: 319.8397\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 48.6689 - val_loss: 227.1887\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 49.4935 - val_loss: 152.7913\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 74.1429 - val_loss: 181.2935\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 45.0718 - val_loss: 198.7791\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 48.0735 - val_loss: 223.7243\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 58.5141 - val_loss: 195.2664\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 43.7205 - val_loss: 197.1075\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 54.9466 - val_loss: 206.1723\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 41.2577 - val_loss: 183.9211\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 49.4180 - val_loss: 106.1177\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 47.6912 - val_loss: 104.9822\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 53.4044 - val_loss: 104.9558\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 45.1408 - val_loss: 126.2456\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 61.0984 - val_loss: 145.5704\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 46.0203 - val_loss: 97.0986\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 51.9997 - val_loss: 105.1336\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 46.6755 - val_loss: 95.2001\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 43.3981 - val_loss: 102.2984\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 44.8402 - val_loss: 108.6782\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 58.9982 - val_loss: 92.6550\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 67.1566 - val_loss: 116.4468\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 45.4104 - val_loss: 53.5205\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 46.8993 - val_loss: 63.3700\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 61.4050 - val_loss: 75.2131\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 47.6351 - val_loss: 93.8446\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 45.9167 - val_loss: 105.7395\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 56.8282 - val_loss: 74.7762\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 51.1532 - val_loss: 98.7025\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 43.6514 - val_loss: 100.5373\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 49.1492 - val_loss: 117.3959\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 44.9988 - val_loss: 64.8306\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 55.0664 - val_loss: 808695471.9061\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 104.1466 - val_loss: 416.2758\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 55.2979 - val_loss: 352.4163\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 58.7614 - val_loss: 425.9944\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 51.4851 - val_loss: 411.1379\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 48.3781 - val_loss: 367.0265\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 57.5387 - val_loss: 382.1749\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 55.9428 - val_loss: 378.7203\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 52.4208 - val_loss: 372.3445\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 8s 7ms/step - loss: 49.5934 - val_loss: 349.6043\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 50.1581 - val_loss: 329.6520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 21/36 [1:32:58<1:07:21, 269.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 400, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 7692.9823 - val_loss: 1385.4732\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7337.6441 - val_loss: 2696.5331\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7055.3545 - val_loss: 28085.9666\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6848.5635 - val_loss: 7024.3966\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6607.5574 - val_loss: 7025.6670\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6329.8393 - val_loss: 7400.3863\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 5953.2924 - val_loss: 6957.0946\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 5634.0059 - val_loss: 8814.9800\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 5138.5994 - val_loss: 753980.2510\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 4389.8841 - val_loss: 5301.3698\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 3733.6444 - val_loss: 104155.9812\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 3103.0946 - val_loss: 4634.0928\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 2486.4384 - val_loss: 4203.9980\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1928.3929 - val_loss: 3160.0998\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1459.2045 - val_loss: 3026.2804\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1049.3266 - val_loss: 1879.1807\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 734.8310 - val_loss: 1203.6622\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 481.5101 - val_loss: 1169.2511\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 308.3634 - val_loss: 1253.3513\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 200.0341 - val_loss: 1113.0705\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 130.3860 - val_loss: 1142.5590\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 93.1288 - val_loss: 1103.5645\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 64.5302 - val_loss: 1040.2036\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 50.3050 - val_loss: 993.8640\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 43.7639 - val_loss: 956.4674\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 37.4104 - val_loss: 1309.2696\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 42.1172 - val_loss: 237.3237\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.5950 - val_loss: 587.0994\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 40.7337 - val_loss: 422.6448\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 42.3591 - val_loss: 304.8269\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 40.7724 - val_loss: 421.3398\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 34.8969 - val_loss: 463.8523\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 40.7610 - val_loss: 477.1072\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 42.5578 - val_loss: 537.4963\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 39.5922 - val_loss: 557.1481\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 32.9489 - val_loss: 514.8573\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33.7974 - val_loss: 534.8939\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 38.3703 - val_loss: 185.1932\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33.5565 - val_loss: 277.6877\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 44.8057 - val_loss: 285.2340\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 32.7987 - val_loss: 291.6363\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.7713 - val_loss: 275.5322\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.4263 - val_loss: 302.3525\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 37.4426 - val_loss: 303.0917\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 30.2313 - val_loss: 280.7817\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 37.5403 - val_loss: 236.6314\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 31.9724 - val_loss: 232.7467\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 38.0354 - val_loss: 235.3799\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33.4212 - val_loss: 236.5144\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33.4440 - val_loss: 429.3302\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 39.7423 - val_loss: 441.7715\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.3064 - val_loss: 396.0646\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 33.3350 - val_loss: 379.5143\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 30.4732 - val_loss: 335.5703\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 40.0035 - val_loss: 319.3948\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 39.9383 - val_loss: 293.5477\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 39.5730 - val_loss: 247.4153\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.8004 - val_loss: 263.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|██████    | 22/36 [1:38:58<1:09:11, 296.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 400, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7755.7407 - val_loss: 707294.5515\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7468.6450 - val_loss: 86087.1395\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7244.1874 - val_loss: 8108.4911\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7055.3473 - val_loss: 3450.5695\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6891.6537 - val_loss: 2171.3954\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6754.9358 - val_loss: 1988.0432\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6591.7347 - val_loss: 2101.4020\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6402.7568 - val_loss: 1359.6255\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6215.9718 - val_loss: 10013.2874\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5912.3458 - val_loss: 420.6839\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 5591.6453 - val_loss: 7189.8470\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5246.8036 - val_loss: 8151.3949\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4834.0346 - val_loss: 4895.3996\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4418.0970 - val_loss: 8602.7114\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3982.4306 - val_loss: 8339.1834\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3537.5311 - val_loss: 7955.6245\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3076.7743 - val_loss: 7621.3739\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2649.2361 - val_loss: 7152.4760\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 2237.3005 - val_loss: 6645.2411\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 1845.3843 - val_loss: 6153.8019\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1499.9709 - val_loss: 5772.2848\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1190.4142 - val_loss: 5351.9717\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 929.5273 - val_loss: 4922.7450\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 710.4382 - val_loss: 4631.4923\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 533.0976 - val_loss: 4336.0802\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 381.9930 - val_loss: 4027.0196\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 279.7436 - val_loss: 3619.1571\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 206.6753 - val_loss: 3258.4904\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 144.9687 - val_loss: 3109.0360\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 105.5305 - val_loss: 3499.7391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|██████▍   | 23/36 [1:41:41<55:33, 256.44s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 400, 'LSTM_dropout': 0.1}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7795.1271 - val_loss: 105380.0529\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7589.7919 - val_loss: 1973.2076\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7415.2787 - val_loss: 6075.7768\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7262.8625 - val_loss: 7103.6654\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7117.5077 - val_loss: 7187.8862\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7011.5476 - val_loss: 7585.9495\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6885.7239 - val_loss: 7315.1489\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 6775.4382 - val_loss: 7443.1637\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6685.9824 - val_loss: 7448.6529\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 6521.3263 - val_loss: 7099.7558\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6359.9630 - val_loss: 7010.0819\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 6199.4824 - val_loss: 6928.8957\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 5983.5259 - val_loss: 6383.7290\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5771.2827 - val_loss: 6064.2351\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5550.5395 - val_loss: 8243.4892\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5281.6229 - val_loss: 5993.4367\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4996.7437 - val_loss: 5370.3944\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4781.8384 - val_loss: 3231.5259\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4449.9438 - val_loss: 4170.9799\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4264.9223 - val_loss: 4620.2189\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3861.6501 - val_loss: 4718.6814\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3577.6587 - val_loss: 4050.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 24/36 [1:43:30<42:29, 212.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 100, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7834.5849 - val_loss: 3998.5029\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7558.8163 - val_loss: 6443.3847\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7217.5903 - val_loss: 5917.9222\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6843.2227 - val_loss: 5911.9591\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6436.0265 - val_loss: 6769.7600\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6061.7822 - val_loss: 5300.7509\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6324.3437 - val_loss: 6362.6868\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5454.5802 - val_loss: 6319.3875\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 4613.3443 - val_loss: 6186.9479\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 3922.9086 - val_loss: 5841.2961\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 3261.8437 - val_loss: 5319.7320\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 2662.5053 - val_loss: 78051946026539.0781\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 2126.8639 - val_loss: 3653.1447\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1637.4669 - val_loss: 3289.5791\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1261.3514 - val_loss: 993.6393\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 933.6858 - val_loss: 1267.9992\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 640.4054 - val_loss: 1042.2454\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 470.0054 - val_loss: 851.9308\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 333.3471 - val_loss: 686.8861\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 240.6846 - val_loss: 533.0525\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 178.3475 - val_loss: 388.9322\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 138.5529 - val_loss: 284.1050\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 109.6421 - val_loss: 285.5869\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 94.2254 - val_loss: 189.9395\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 100.9751 - val_loss: 214.5729\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 81.7368 - val_loss: 102.2135\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 97.9873 - val_loss: 208.5355\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 89.9932 - val_loss: 852.8699\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 97.2910 - val_loss: 61.7485\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 84.2915 - val_loss: 58.0954\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 92.3038 - val_loss: 131.0561\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 72.8727 - val_loss: 89.0765\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 72.0930 - val_loss: 100.2394\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 75.6661 - val_loss: 80.0009\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 79.1871 - val_loss: 122.5850\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 85.5329 - val_loss: 131.3877\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 80.3189 - val_loss: 117.4714\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 76.3865 - val_loss: 133.4088\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 84.9156 - val_loss: 133.1750\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 89.4900 - val_loss: 80.9480\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 77.1536 - val_loss: 101.5238\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 80.6452 - val_loss: 97.8086\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 80.8743 - val_loss: 107.2674\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 71.8469 - val_loss: 106.7488\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 76.3820 - val_loss: 112.9253\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 81.9260 - val_loss: 134.0300\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 84.7301 - val_loss: 121.2553\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 73.9904 - val_loss: 99.7854\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 73.2192 - val_loss: 76.4551\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 80.1643 - val_loss: 151.5245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|██████▉   | 25/36 [1:46:13<36:11, 197.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 100, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7863.6601 - val_loss: 5356.6654\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7738.7005 - val_loss: 6432.3291\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7587.2030 - val_loss: 5091.4602\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7431.5538 - val_loss: 5626.3048\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7257.6816 - val_loss: 5728.5112\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7078.8165 - val_loss: 6037.1877\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6887.9626 - val_loss: 5816.4732\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6693.9162 - val_loss: 5953.6435\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6493.0161 - val_loss: 5443.3883\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6292.5641 - val_loss: 4692.1529\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6092.8723 - val_loss: 4085.4982\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5882.5942 - val_loss: 5149.2255\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5653.6562 - val_loss: 5263.4391\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5414.2248 - val_loss: 5170.2501\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5148.2980 - val_loss: 4916.8591\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4892.0251 - val_loss: 4872.2721\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4610.9764 - val_loss: 4834.2551\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4314.0258 - val_loss: 4256.1939\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3991.1293 - val_loss: 3914.9520\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3720.1020 - val_loss: 2586.1500\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3371.4287 - val_loss: 3652.4085\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3059.6424 - val_loss: 3168.4257\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2787.4230 - val_loss: 2700.2881\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2538.6800 - val_loss: 4999.4006\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2247.9225 - val_loss: 3324.0454\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2035.0856 - val_loss: 4759.3278\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1779.0582 - val_loss: 4828.0800\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1568.1216 - val_loss: 10905.0946\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1267.9989 - val_loss: 3936.2562\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1087.5240 - val_loss: 2494.6565\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 906.5601 - val_loss: 1922.2820\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 755.9754 - val_loss: 1482.1935\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 652.9394 - val_loss: 1193.8641\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 542.0157 - val_loss: 1013.2647\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 452.7445 - val_loss: 842.9009\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 358.1533 - val_loss: 705.2790\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 309.0529 - val_loss: 588.9185\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 252.7589 - val_loss: 591.7091\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 224.2755 - val_loss: 532.0260\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 181.6648 - val_loss: 460.9531\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 146.8209 - val_loss: 434.3637\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 134.9724 - val_loss: 406.9774\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 124.7210 - val_loss: 392.0329\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 110.8733 - val_loss: 365.4721\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 94.2686 - val_loss: 338.5399\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 83.6057 - val_loss: 323.6962\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 95.2114 - val_loss: 521.7969\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 65.1014 - val_loss: 351.6863\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 77.4949 - val_loss: 289.9759\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 66.0307 - val_loss: 267.0838\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 74.4588 - val_loss: 287.4751\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 73.2068 - val_loss: 255.2134\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 66.0508 - val_loss: 265.9059\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 59.2620 - val_loss: 973.7452\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 70.7025 - val_loss: 241.6215\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 71.5486 - val_loss: 188.8126\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 63.5989 - val_loss: 215.4031\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 66.2752 - val_loss: 203.5794\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 68.5621 - val_loss: 165.8579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 78.8650 - val_loss: 150.2365\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 74.0324 - val_loss: 270.5418\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 62.8353 - val_loss: 110.8421\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 65.3666 - val_loss: 105.4942\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 67.5841 - val_loss: 104.2201\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 69.0065 - val_loss: 99.0920\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 66.5694 - val_loss: 90.3650\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 67.9857 - val_loss: 94.1236\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 69.9498 - val_loss: 93.7895\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 84.8473 - val_loss: 1016.5929\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 95.4289 - val_loss: 114.1307\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 91.1597 - val_loss: 157.2267\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 72.3742 - val_loss: 117.9318\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 74.6039 - val_loss: 13926476.4709\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 74.9228 - val_loss: 173.2346\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 78.3145 - val_loss: 262.7939\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 93.5932 - val_loss: 292.7125\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 77.1868 - val_loss: 308.0423\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 84.9436 - val_loss: 287.2647\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 78.8200 - val_loss: 277.0577\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 71.7594 - val_loss: 130.0351\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 79.9379 - val_loss: 307.9267\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 70.0241 - val_loss: 302.0266\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 63.1170 - val_loss: 143.6649\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 72.9857 - val_loss: 117.7266\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 63.4389 - val_loss: 103.9178\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 66.4183 - val_loss: 93.9389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████▏  | 26/36 [1:49:24<32:35, 195.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 100, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7862.6744 - val_loss: 8075.5402\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7762.6138 - val_loss: 8096.2447\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7671.0922 - val_loss: 8035.8999\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7575.9518 - val_loss: 7741.6754\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7468.8678 - val_loss: 7901.9450\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7359.6077 - val_loss: 7672.6648\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7238.5389 - val_loss: 7583.7967\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 7113.1687 - val_loss: 7518.5746\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6982.9095 - val_loss: 7247.9082\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6850.9263 - val_loss: 6885.3963\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6720.5949 - val_loss: 7191.4434\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6578.8179 - val_loss: 6200.9553\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6454.2171 - val_loss: 6533.7356\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6308.9253 - val_loss: 6127.8663\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6171.9501 - val_loss: 6271.0918\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6027.8037 - val_loss: 7693.2690\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5885.1144 - val_loss: 5094.2474\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5734.8137 - val_loss: 5774.4525\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5566.7542 - val_loss: 5456.1922\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5395.6215 - val_loss: 5705.8253\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5220.0002 - val_loss: 5208.1801\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 5029.8980 - val_loss: 2069.3157\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 4856.1420 - val_loss: 3484.6532\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 4646.2270 - val_loss: 1015.9794\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 4456.4086 - val_loss: 2469.4990\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 4228.2361 - val_loss: 4885.6952\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 4034.3162 - val_loss: 5067.9657\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 3823.5649 - val_loss: 3709.4336\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3630.4030 - val_loss: 787.7803\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3391.2832 - val_loss: 1166.3094\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 3237.4210 - val_loss: 3031.9279\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2991.1658 - val_loss: 9581.4177\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2753.1658 - val_loss: 2018.2871\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 2580.1937 - val_loss: 13161.6896\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 2346.0315 - val_loss: 7684.0429\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 2266.7490 - val_loss: 7909.8233\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 2026.0308 - val_loss: 5127.1165\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 1838.3557 - val_loss: 3945.5045\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 1652.9486 - val_loss: 2263.8574\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 1502.4541 - val_loss: 2029.4304\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 1357.6952 - val_loss: 1856.6284\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 1221.0583 - val_loss: 1660.3997\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 1092.1500 - val_loss: 1532.7780\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 985.9791 - val_loss: 1363.6942\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 861.9563 - val_loss: 51272.4591\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 771.9793 - val_loss: 1392.8596\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 679.0413 - val_loss: 1326.0201\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 607.5172 - val_loss: 1397.8769\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 533.2287 - val_loss: 1551.7886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▌  | 27/36 [1:50:58<24:46, 165.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 100, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 100)           40800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 205,830\n",
      "Trainable params: 205,230\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 7892.0961 - val_loss: 7339.9059\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7820.7590 - val_loss: 5432.9846\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7753.3909 - val_loss: 3596.7878\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7683.0830 - val_loss: 2741.5370\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7611.8385 - val_loss: 3306.3200\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7529.1476 - val_loss: 3625.9213\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7448.6803 - val_loss: 5594.8034\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7366.1895 - val_loss: 6636.6318\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7282.8963 - val_loss: 6014.6798\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7209.3015 - val_loss: 7761.3982\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7118.1675 - val_loss: 8438.1344\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 7033.1965 - val_loss: 8660.8220\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6948.2440 - val_loss: 8626.8755\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 1ms/step - loss: 6833.5664 - val_loss: 7850.0524\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6731.9198 - val_loss: 7522.8407\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6632.4294 - val_loss: 7589.1538\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6528.2984 - val_loss: 7671.6278\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6431.9553 - val_loss: 7659.9936\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6323.1884 - val_loss: 7356.1551\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6222.2603 - val_loss: 7217.0388\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6120.1567 - val_loss: 7023.0100\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 6026.7736 - val_loss: 6666.7026\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5916.0385 - val_loss: 6647.7850\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 1s 1ms/step - loss: 5805.8919 - val_loss: 6495.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 28/36 [1:51:41<17:08, 128.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 200, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 7699.0352 - val_loss: 2898.2394\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7264.9899 - val_loss: 4525.3332\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6833.1937 - val_loss: 5468.3341\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6402.3095 - val_loss: 3972.2812\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5895.4542 - val_loss: 9531.8588\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5314.0276 - val_loss: 2193.1549\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4450.0833 - val_loss: 4151.7788\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3583.2490 - val_loss: 3976.4996\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2732.8986 - val_loss: 17380470601030158.0000\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1979.8620 - val_loss: 3082.5920\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1344.2928 - val_loss: 2677.2915\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 866.5280 - val_loss: 2084.3023\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 526.6370 - val_loss: 1579.4349\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 291.2055 - val_loss: 1315.1139\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 175.5991 - val_loss: 1266.2975\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 119.3651 - val_loss: 930.2906\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 83.7491 - val_loss: 771.6109\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 74.2587 - val_loss: 678.4244\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 74.9085 - val_loss: 612.2454\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 64.2001 - val_loss: 543.1771\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 60.8048 - val_loss: 465.9683\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 69.3553 - val_loss: 366.9230\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 79.8067 - val_loss: 442.7929\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 61.0300 - val_loss: 382.9698\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 69.9805 - val_loss: 379.8838\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 76.3151 - val_loss: 282.2430\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 65.8340 - val_loss: 376.1435\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 67.6400 - val_loss: 328.9574\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 57.0090 - val_loss: 224.5893\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 59.5599 - val_loss: 235.1206\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 71.2490 - val_loss: 211.5636\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 71.5270 - val_loss: 183.6360\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 63.8096 - val_loss: 186.8163\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 72.5219 - val_loss: 179.4743\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 80.1078 - val_loss: 117.8057\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 66.8497 - val_loss: 125.3460\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 70.0698 - val_loss: 134.8919\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 67.2148 - val_loss: 170.2785\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 73.2359 - val_loss: 176.7973\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 63.8320 - val_loss: 40.8140\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 64.2813 - val_loss: 54.5441\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 56.8741 - val_loss: 74.4537\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 80.7382 - val_loss: 85.1489\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 63.6637 - val_loss: 49.4739\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 58.8893 - val_loss: 56.9199\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 62.7736 - val_loss: 105.7240\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 68.2024 - val_loss: 104.6604\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 83.4230 - val_loss: 108.6628\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 69.1799 - val_loss: 86.6142\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 62.2833 - val_loss: 82.1811\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 66.7898 - val_loss: 28.4971\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 66.5084 - val_loss: 51.1132\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 56.6895 - val_loss: 39.3267\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 65.3168 - val_loss: 39.5996\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 61.7291 - val_loss: 51.7718\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 64.2608 - val_loss: 45.6292\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 62.5602 - val_loss: 41.6667\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 50.6792 - val_loss: 42.2224\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 69.7285 - val_loss: 71.3182\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 5s 4ms/step - loss: 65.7785 - val_loss: 50.2084\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 61.2764 - val_loss: 42.8058\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 67.2363 - val_loss: 53.7207\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 61.7729 - val_loss: 62.7725\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 70.1685 - val_loss: 25.2472\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 49.3655 - val_loss: 51.7182\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 58.9318 - val_loss: 34.1667\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 54.9490 - val_loss: 54.6826\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 57.4617 - val_loss: 44.6226\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 67.3158 - val_loss: 56.7172\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 70.2889 - val_loss: 55.5813\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 66.1695 - val_loss: 48.9946\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 74.1474 - val_loss: 48.3044\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 67.2760 - val_loss: 27.7851\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 62.9072 - val_loss: 29.7984\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 64.1033 - val_loss: 50.6857\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 56.1837 - val_loss: 40.8424\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 56.6639 - val_loss: 38.2285\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 62.0622 - val_loss: 29.2425\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 55.1534 - val_loss: 30.9043\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 55.5903 - val_loss: 28.9963\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 58.0006 - val_loss: 61.7656\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 58.4957 - val_loss: 67.4373\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 4s 3ms/step - loss: 56.7721 - val_loss: 74.8898\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 66.2051 - val_loss: 60.3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|████████  | 29/36 [1:58:42<25:13, 216.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 200, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7841.3265 - val_loss: 33084.8352\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7654.3321 - val_loss: 3956.0853\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7411.0119 - val_loss: 6132.4316\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 7170.9514 - val_loss: 6534.9127\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6941.3723 - val_loss: 7817.2461\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6710.9935 - val_loss: 8644.7818\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6467.7444 - val_loss: 50942.6736\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6241.5934 - val_loss: 7889.0263\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 6033.4791 - val_loss: 6706.2597\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 5664.5255 - val_loss: 5806.0868\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5254.1586 - val_loss: 4798.6483\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 4826.8675 - val_loss: 4008.6280\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 4418.7848 - val_loss: 3717.7361\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 4012.2748 - val_loss: 1271.2079\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 3584.9636 - val_loss: 1026.1337\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3145.6196 - val_loss: 1398.9805\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2722.0798 - val_loss: 2293.1898\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2304.6418 - val_loss: 2712.9641\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 1932.3825 - val_loss: 2202.3971\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1587.2216 - val_loss: 1442.6503\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1289.3627 - val_loss: 761.2716\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1025.5396 - val_loss: 1388.9362\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 794.7350 - val_loss: 1212.8913\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 618.5179 - val_loss: 1218.1762\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 463.3367 - val_loss: 698.9897\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 346.6822 - val_loss: 641.1644\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 259.4201 - val_loss: 220.2684\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 197.3673 - val_loss: 178.0653\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 156.5101 - val_loss: 163.7532\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 109.6029 - val_loss: 253.4683\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 86.2279 - val_loss: 308.3058\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 72.0546 - val_loss: 364.8170\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 78.3763 - val_loss: 464.0722\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 59.8498 - val_loss: 401.5912\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 56.1877 - val_loss: 263.6265\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 66.6773 - val_loss: 322.9352\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 52.8268 - val_loss: 343.8472\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 43.5660 - val_loss: 341.8946\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 44.0274 - val_loss: 321.0727\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 52.6067 - val_loss: 316.9940\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 46.5341 - val_loss: 311.2833\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.8912 - val_loss: 277.1491\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 45.2260 - val_loss: 165.2092\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 49.4284 - val_loss: 126.7657\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.0038 - val_loss: 128.5183\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 52.8683 - val_loss: 148.7383\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.7996 - val_loss: 153.2654\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 48.9153 - val_loss: 152.3329\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 50.7193 - val_loss: 144.9092\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 54.0251 - val_loss: 167.1759\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 49.4239 - val_loss: 176.4697\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.3352 - val_loss: 131.9289\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 44.2261 - val_loss: 143.5918\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 54.1669 - val_loss: 144.0352\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 39.4265 - val_loss: 110.8360\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 48.9726 - val_loss: 120.1876\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 52.3882 - val_loss: 116.3106\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 45.5419 - val_loss: 97.2013\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 41.8207 - val_loss: 51.9413\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 3s 3ms/step - loss: 49.1768 - val_loss: 126.8539\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 51.5893 - val_loss: 142.2939\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 44.2152 - val_loss: 129.3274\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 51.5837 - val_loss: 121.1037\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 51.4955 - val_loss: 153.1312\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 45.1630 - val_loss: 138.6106\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.2815 - val_loss: 136.1491\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 41.8545 - val_loss: 116.1814\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 54.8326 - val_loss: 99.8791\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 50.8679 - val_loss: 94.1836\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 49.0394 - val_loss: 81.7178\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.5988 - val_loss: 77.2046\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 49.8673 - val_loss: 75.7989\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 44.5745 - val_loss: 77.4719\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.8927 - val_loss: 49.0587\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 48.5169 - val_loss: 36.3309\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 45.5369 - val_loss: 39.2095\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.3955 - val_loss: 48.2389\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.0936 - val_loss: 36.3374\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.3350 - val_loss: 37.8654\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 45.2133 - val_loss: 51.3822\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 50.8765 - val_loss: 60.8161\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.5860 - val_loss: 65.8950\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 44.8169 - val_loss: 60.0484\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.3857 - val_loss: 29.3782\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.8467 - val_loss: 25.6023\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 47.5249 - val_loss: 29.7330\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 45.3519 - val_loss: 31.4826\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 42.6059 - val_loss: 39.8266\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 49.5507 - val_loss: 35.0870\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 53.1498 - val_loss: 48.3247\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.2026 - val_loss: 48.6694\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 45.7491 - val_loss: 52.1862\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 3s 3ms/step - loss: 49.3144 - val_loss: 56.7375\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 60.2814 - val_loss: 57.4413\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.8185 - val_loss: 46.5804\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 41.5305 - val_loss: 52.3394\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 43.2419 - val_loss: 60.4663\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 46.3938 - val_loss: 45.1507\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 40.1665 - val_loss: 38.5417\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 41.1530 - val_loss: 34.6568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████▎ | 30/36 [2:03:48<24:18, 243.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 200, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7845.6636 - val_loss: 6314.9382\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7672.6019 - val_loss: 2151.9821\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7515.3594 - val_loss: 4104.6730\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7359.9675 - val_loss: 5493.3688\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7201.8400 - val_loss: 5741.4021\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 7042.5104 - val_loss: 6014.2713\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6885.0506 - val_loss: 5623.8069\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6730.7554 - val_loss: 5246.6677\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6584.9719 - val_loss: 5259.5581\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6418.8308 - val_loss: 5977.3719\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6251.7980 - val_loss: 6131.3792\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 6100.4717 - val_loss: 6113.4951\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5924.1517 - val_loss: 3400202.0342\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5719.0649 - val_loss: 5209.0387\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5458.8148 - val_loss: 5236.1667\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 5208.1752 - val_loss: 6226.5763\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4934.3195 - val_loss: 5985.5191\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4640.2195 - val_loss: 6117.3116\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4334.4808 - val_loss: 508412.2025\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 4346.7219 - val_loss: 7053731266.8806\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3796.9399 - val_loss: 1008.9464\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3438.2770 - val_loss: 236.9543\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 3098.8184 - val_loss: 609987742489261.3750\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2793.2234 - val_loss: 882.6751\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2493.3716 - val_loss: 1256.5267\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 2210.3407 - val_loss: 1500.6963\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1964.6758 - val_loss: 1554.0164\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1710.8435 - val_loss: 1664.1168\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1490.2620 - val_loss: 1522.1001\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 1269.7702 - val_loss: 1469.4826\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 1088.6092 - val_loss: 1405.5500\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 913.8209 - val_loss: 1282.9600\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 781.2667 - val_loss: 1193.2210\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 639.2947 - val_loss: 1082.1124\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 534.3537 - val_loss: 274159291964109.4062\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 431.4392 - val_loss: 804.7384\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 3s 2ms/step - loss: 359.1470 - val_loss: 767.7546\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 288.0710 - val_loss: 752.7650\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 246.5041 - val_loss: 669.7357\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 191.4116 - val_loss: 615.6563\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 153.3945 - val_loss: 568.1446\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 133.8892 - val_loss: 523.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████▌ | 31/36 [2:05:41<17:00, 204.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 200, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 200)           161600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 811,630\n",
      "Trainable params: 810,430\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 4s 4ms/step - loss: 7864.7948 - val_loss: 742837.8549\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7745.8787 - val_loss: 19531.6879\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7642.9111 - val_loss: 5809.4110\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7541.3710 - val_loss: 7129.7694\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7436.0591 - val_loss: 6574.3541\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7335.5941 - val_loss: 6905.4257\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7257.4748 - val_loss: 7789.0229\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7167.1432 - val_loss: 6918.9292\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 7116.6701 - val_loss: 7023.2542\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6966.7119 - val_loss: 6510.2359\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6864.5969 - val_loss: 7406.7820\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6843.2990 - val_loss: 6990.1367\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6816.6297 - val_loss: 7209.2551\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6682.9257 - val_loss: 7097.3161\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6538.1349 - val_loss: 7315.4349\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6353.5269 - val_loss: 7347.0840\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 6151.9371 - val_loss: 7398.7044\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5967.0712 - val_loss: 7394.0143\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5757.4544 - val_loss: 7575.0422\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5557.3740 - val_loss: 7477.5278\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5360.8826 - val_loss: 7419.8930\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 5120.9452 - val_loss: 7293.0667\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 2s 2ms/step - loss: 4920.8005 - val_loss: 7163.4659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%|████████▉ | 32/36 [2:06:40<10:42, 160.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 30, 'LSTM_n': 400, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 10s 9ms/step - loss: 7582.4490 - val_loss: 6071.6346\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 7026.2884 - val_loss: 8108.0300\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 6514.2995 - val_loss: 8146.1980\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 5866.3538 - val_loss: 7523.3218\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 4926.1008 - val_loss: 3453370940.3366\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 3883.9084 - val_loss: 58679.9598\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 2531.6237 - val_loss: 3535.1323\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 1453.1435 - val_loss: 1882.7344\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 744.2534 - val_loss: 1048.0822\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 343.8701 - val_loss: 720.1643\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 146.6786 - val_loss: 533.6601\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 79.5623 - val_loss: 433.4605\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 55.7607 - val_loss: 429.9546\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 48.8305 - val_loss: 418.2161\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 50.1212 - val_loss: 369.0726\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 45.7937 - val_loss: 344.4506\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 49.8689 - val_loss: 304.6079\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 56.5349 - val_loss: 253.9051\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 54.9765 - val_loss: 188.4238\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 56.7057 - val_loss: 193.5590\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 62.6472 - val_loss: 170.7938\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 52.9128 - val_loss: 96.2479\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 8s 7ms/step - loss: 47.8462 - val_loss: 141.9635\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 54.1511 - val_loss: 119.8385\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 57.1313 - val_loss: 122.9439\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 57.3629 - val_loss: 106.0184\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 63.5000 - val_loss: 129.8006\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 47.5832 - val_loss: 138.9863\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 52.4866 - val_loss: 108.5715\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 61.7974 - val_loss: 72.4995\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 51.0916 - val_loss: 584.5534\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 49.2777 - val_loss: 436.4239\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 51.9147 - val_loss: 127.9280\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 60.2506 - val_loss: 172.2460\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 60.7959 - val_loss: 86.8771\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 61.9191 - val_loss: 136.3615\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 57.9022 - val_loss: 92.4139\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 54.1215 - val_loss: 144.4594\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 56.8348 - val_loss: 148.8015\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 55.6091 - val_loss: 165.2092\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 53.5758 - val_loss: 141.8836\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 56.3370 - val_loss: 107.4362\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 63.4955 - val_loss: 129.5052\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 58.2547 - val_loss: 134.4123\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 54.3163 - val_loss: 152.3843\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 50.5596 - val_loss: 133.0980\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 54.0082 - val_loss: 218.0486\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 58.9153 - val_loss: 233.0054\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 49.3781 - val_loss: 229.6436\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 9s 7ms/step - loss: 57.7236 - val_loss: 304.0243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████▏| 33/36 [2:13:34<11:49, 236.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 60, 'LSTM_n': 400, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 9s 8ms/step - loss: 7741.8350 - val_loss: 29662.1472\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7394.9591 - val_loss: 5845.9959\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 7179.4269 - val_loss: 6849.7947\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6896.2098 - val_loss: 7490.7187\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6591.1111 - val_loss: 7472.3131\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 6287.1307 - val_loss: 7182.6809\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 5918.2283 - val_loss: 6688.3224\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 5551.3704 - val_loss: 1810394.0514\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 4993.3168 - val_loss: 5368.0642\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 4409.1142 - val_loss: 4962.6323\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 3790.5260 - val_loss: 69692.3383\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 3323.2629 - val_loss: 7676.1888\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 2580.3780 - val_loss: 3147.3585\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1995.0111 - val_loss: 2964.6774\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1506.2019 - val_loss: 2130.1964\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 1104.5592 - val_loss: 1617.4135\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 758.3491 - val_loss: 1239.0231\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 529.4245 - val_loss: 954.4985\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 342.1915 - val_loss: 795.8958\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 215.6017 - val_loss: 735.5371\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 138.8422 - val_loss: 733.6317\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 98.4686 - val_loss: 758.0396\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 82.0333 - val_loss: 795.1844\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 59.6801 - val_loss: 832.1194\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 55.3573 - val_loss: 859.1771\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 47.8109 - val_loss: 881.6775\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 39.0953 - val_loss: 908.6986\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 40.7917 - val_loss: 924.9586\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 42.6085 - val_loss: 943.1380\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 44.2843 - val_loss: 951.2441\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 52.7018 - val_loss: 959.8496\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 48.8874 - val_loss: 943.7061\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 41.9198 - val_loss: 935.2135\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 39.3783 - val_loss: 926.7620\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 40.3160 - val_loss: 925.6393\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 48.0672 - val_loss: 912.3727\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 49.2927 - val_loss: 899.9191\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 36.8437 - val_loss: 877.9160\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 41.5917 - val_loss: 880.6022\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 47.3330 - val_loss: 867.7140\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 6s 5ms/step - loss: 41.8535 - val_loss: 853.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████▍| 34/36 [2:17:54<08:07, 243.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 90, 'LSTM_n': 400, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 8s 6ms/step - loss: 7767.9384 - val_loss: 21479.9380\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7531.6279 - val_loss: 7039.8461\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7470.8561 - val_loss: 8372.0235\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7453.2466 - val_loss: 12105.1106\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7279.6022 - val_loss: 7630.7149\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7041.2135 - val_loss: 7347.7181\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6801.6586 - val_loss: 7008.5612\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6519.5081 - val_loss: 6546.4536\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6224.4841 - val_loss: 6041.3103\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5908.4189 - val_loss: 5542.0757\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5553.9610 - val_loss: 5044.2351\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5185.6491 - val_loss: 4485.2796\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4801.3652 - val_loss: 3923.0971\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4383.9249 - val_loss: 3456.2730\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3985.3176 - val_loss: 3179.4828\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3547.8741 - val_loss: 2697.9665\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3125.9062 - val_loss: 2200.4953\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2716.3470 - val_loss: 1867.6683\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 2300.5644 - val_loss: 1556.2608\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1930.6652 - val_loss: 1256.4431\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1586.5191 - val_loss: 1044.0583\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1277.8148 - val_loss: 890.0269\n",
      "Epoch 23/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 1004.1737 - val_loss: 783.8168\n",
      "Epoch 24/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 770.9489 - val_loss: 694.1111\n",
      "Epoch 25/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 587.3736 - val_loss: 630.9248\n",
      "Epoch 26/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 447.4758 - val_loss: 594.8872\n",
      "Epoch 27/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 338.9435 - val_loss: 580.7915\n",
      "Epoch 28/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 239.4799 - val_loss: 627.0310\n",
      "Epoch 29/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 178.1299 - val_loss: 658.5996\n",
      "Epoch 30/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 131.7119 - val_loss: 684.0552\n",
      "Epoch 31/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 102.6637 - val_loss: 739.9744\n",
      "Epoch 32/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 80.7416 - val_loss: 717.9771\n",
      "Epoch 33/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 58.1117 - val_loss: 739.0252\n",
      "Epoch 34/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 47.9863 - val_loss: 749.0645\n",
      "Epoch 35/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 44.5895 - val_loss: 844.0979\n",
      "Epoch 36/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 54.3706 - val_loss: 838.4197\n",
      "Epoch 37/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 45.2102 - val_loss: 752.7636\n",
      "Epoch 38/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.5046 - val_loss: 737.0175\n",
      "Epoch 39/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36.9171 - val_loss: 761.4425\n",
      "Epoch 40/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 41.0565 - val_loss: 767.1110\n",
      "Epoch 41/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 40.5199 - val_loss: 734.9074\n",
      "Epoch 42/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.8445 - val_loss: 706.5949\n",
      "Epoch 43/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.5655 - val_loss: 685.5207\n",
      "Epoch 44/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.6840 - val_loss: 659.7877\n",
      "Epoch 45/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.9150 - val_loss: 658.3049\n",
      "Epoch 46/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.3511 - val_loss: 606.9656\n",
      "Epoch 47/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.2527 - val_loss: 569.0302\n",
      "Epoch 48/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 40.8251 - val_loss: 510.6119\n",
      "Epoch 49/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 34.1952 - val_loss: 486.9178\n",
      "Epoch 50/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 44.7526 - val_loss: 453.2336\n",
      "Epoch 51/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.2767 - val_loss: 369.1248\n",
      "Epoch 52/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.6932 - val_loss: 365.6105\n",
      "Epoch 53/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.9079 - val_loss: 377.9212\n",
      "Epoch 54/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 40.3107 - val_loss: 329.5062\n",
      "Epoch 55/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.0884 - val_loss: 324.8734\n",
      "Epoch 56/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 39.3240 - val_loss: 308.0819\n",
      "Epoch 57/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 33.6097 - val_loss: 319.0829\n",
      "Epoch 58/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 31.7670 - val_loss: 294.9165\n",
      "Epoch 59/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 39.0300 - val_loss: 272.0340\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 5s 4ms/step - loss: 46.5937 - val_loss: 244.9909\n",
      "Epoch 61/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.7130 - val_loss: 293.2435\n",
      "Epoch 62/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.4342 - val_loss: 290.3194\n",
      "Epoch 63/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.4312 - val_loss: 262.2355\n",
      "Epoch 64/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 40.0371 - val_loss: 241.0228\n",
      "Epoch 65/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.5532 - val_loss: 242.6647\n",
      "Epoch 66/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36.8070 - val_loss: 261.6396\n",
      "Epoch 67/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.0628 - val_loss: 287.4156\n",
      "Epoch 68/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 41.0763 - val_loss: 258.8451\n",
      "Epoch 69/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.9387 - val_loss: 242.2773\n",
      "Epoch 70/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36.7743 - val_loss: 225.7948\n",
      "Epoch 71/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.9994 - val_loss: 236.4836\n",
      "Epoch 72/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36.2228 - val_loss: 218.2336\n",
      "Epoch 73/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.0132 - val_loss: 206.9123\n",
      "Epoch 74/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 34.7254 - val_loss: 279.8750\n",
      "Epoch 75/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.6064 - val_loss: 233.5686\n",
      "Epoch 76/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 43.4178 - val_loss: 224.0856\n",
      "Epoch 77/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.4725 - val_loss: 264.7192\n",
      "Epoch 78/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 40.4645 - val_loss: 181.0826\n",
      "Epoch 79/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.7861 - val_loss: 159.3057\n",
      "Epoch 80/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36.6953 - val_loss: 121.7556\n",
      "Epoch 81/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 42.4451 - val_loss: 80.6598\n",
      "Epoch 82/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36.4259 - val_loss: 80.5283\n",
      "Epoch 83/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 48.0781 - val_loss: 84.9558\n",
      "Epoch 84/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.8730 - val_loss: 41.6876\n",
      "Epoch 85/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 39.6848 - val_loss: 64.6499\n",
      "Epoch 86/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 36.2145 - val_loss: 62.6632\n",
      "Epoch 87/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 31.6862 - val_loss: 106.1238\n",
      "Epoch 88/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 45.4841 - val_loss: 112.4838\n",
      "Epoch 89/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.1584 - val_loss: 107.7530\n",
      "Epoch 90/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.7675 - val_loss: 124.0616\n",
      "Epoch 91/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.7724 - val_loss: 116.3891\n",
      "Epoch 92/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.3422 - val_loss: 109.5759\n",
      "Epoch 93/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 34.6903 - val_loss: 115.6161\n",
      "Epoch 94/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.1646 - val_loss: 97.4123\n",
      "Epoch 95/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 33.2763 - val_loss: 101.1698\n",
      "Epoch 96/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 37.6043 - val_loss: 113.0974\n",
      "Epoch 97/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 35.5819 - val_loss: 113.9186\n",
      "Epoch 98/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 41.1527 - val_loss: 129.9043\n",
      "Epoch 99/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 38.3089 - val_loss: 139.3793\n",
      "Epoch 100/100\n",
      "1192/1192 [==============================] - 5s 5ms/step - loss: 37.0403 - val_loss: 125.9525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|█████████▋| 35/36 [2:26:39<05:28, 328.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 120, 'LSTM_n': 400, 'LSTM_dropout': 0.2}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 400)           643200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 400)           1600      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                12030     \n",
      "=================================================================\n",
      "Total params: 3,223,230\n",
      "Trainable params: 3,220,830\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1192 samples, validate on 511 samples\n",
      "Epoch 1/100\n",
      "1192/1192 [==============================] - 7s 6ms/step - loss: 7810.1651 - val_loss: 21932342.9119\n",
      "Epoch 2/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7605.9657 - val_loss: 1242.8314\n",
      "Epoch 3/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7426.1951 - val_loss: 5240.3596\n",
      "Epoch 4/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7274.8711 - val_loss: 4292.0127\n",
      "Epoch 5/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7146.9519 - val_loss: 5036.5937\n",
      "Epoch 6/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7026.1090 - val_loss: 7847.7712\n",
      "Epoch 7/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 7060.6155 - val_loss: 7233.5770\n",
      "Epoch 8/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6893.0354 - val_loss: 7403.4208\n",
      "Epoch 9/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6725.7007 - val_loss: 7500.8617\n",
      "Epoch 10/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6552.8420 - val_loss: 7820.6727\n",
      "Epoch 11/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6362.7170 - val_loss: 7930.6655\n",
      "Epoch 12/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 6159.0633 - val_loss: 6838.1477\n",
      "Epoch 13/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5948.5122 - val_loss: 5311.3201\n",
      "Epoch 14/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5731.2256 - val_loss: 23763311.9609\n",
      "Epoch 15/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5499.0973 - val_loss: 4945.7635\n",
      "Epoch 16/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 5250.3889 - val_loss: 4124.0070\n",
      "Epoch 17/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4988.8531 - val_loss: 4545.7924\n",
      "Epoch 18/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4708.9832 - val_loss: 3913.0158\n",
      "Epoch 19/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4424.8505 - val_loss: 4305.6556\n",
      "Epoch 20/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 4134.3556 - val_loss: 3196.4326\n",
      "Epoch 21/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3830.4137 - val_loss: 4377.4401\n",
      "Epoch 22/100\n",
      "1192/1192 [==============================] - 5s 4ms/step - loss: 3527.2978 - val_loss: 4672.5151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 36/36 [2:28:30<00:00, 247.52s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "\n",
    "scan_object = talos.Scan(X_train,\n",
    "                         y_train, \n",
    "                         params=p,\n",
    "                         model=LSTM_model,\n",
    "                         experiment_name='LSTM',\n",
    "                         print_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug (link below) in the Talos library that means it sorts the columns and values from parameter dictionary incorrectly in the ```talos.Analze``` object. That is why each hyperparameter has unique values so I can easily understand the results.\n",
    "\n",
    "[https://github.com/autonomio/talos/issues/439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>LSTM_dropout</th>\n",
       "      <th>LSTM_n</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>10.820394</td>\n",
       "      <td>19.600999</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100</td>\n",
       "      <td>12.805731</td>\n",
       "      <td>37.152377</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>98</td>\n",
       "      <td>15.786570</td>\n",
       "      <td>14.420242</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100</td>\n",
       "      <td>34.656778</td>\n",
       "      <td>41.152965</td>\n",
       "      <td>60</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>42.784797</td>\n",
       "      <td>16.502601</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>50.788145</td>\n",
       "      <td>35.075523</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>87</td>\n",
       "      <td>51.667780</td>\n",
       "      <td>62.586626</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>84</td>\n",
       "      <td>60.307169</td>\n",
       "      <td>66.205071</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>68.828456</td>\n",
       "      <td>18.782989</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>86</td>\n",
       "      <td>93.938887</td>\n",
       "      <td>66.418342</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100</td>\n",
       "      <td>125.952476</td>\n",
       "      <td>37.040327</td>\n",
       "      <td>90</td>\n",
       "      <td>400</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50</td>\n",
       "      <td>151.524533</td>\n",
       "      <td>80.164311</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>92</td>\n",
       "      <td>166.883685</td>\n",
       "      <td>38.997273</td>\n",
       "      <td>60</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>182.144088</td>\n",
       "      <td>42.228314</td>\n",
       "      <td>30</td>\n",
       "      <td>400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>42</td>\n",
       "      <td>226.292798</td>\n",
       "      <td>58.064071</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36</td>\n",
       "      <td>248.536357</td>\n",
       "      <td>28.911466</td>\n",
       "      <td>60</td>\n",
       "      <td>400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>58</td>\n",
       "      <td>263.759974</td>\n",
       "      <td>36.800374</td>\n",
       "      <td>60</td>\n",
       "      <td>400</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50</td>\n",
       "      <td>304.024293</td>\n",
       "      <td>57.723601</td>\n",
       "      <td>30</td>\n",
       "      <td>400</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>61</td>\n",
       "      <td>329.651958</td>\n",
       "      <td>50.158120</td>\n",
       "      <td>30</td>\n",
       "      <td>400</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>84</td>\n",
       "      <td>368.471200</td>\n",
       "      <td>29.046989</td>\n",
       "      <td>60</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>42</td>\n",
       "      <td>523.052478</td>\n",
       "      <td>133.889195</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>760.476990</td>\n",
       "      <td>26.662885</td>\n",
       "      <td>90</td>\n",
       "      <td>400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>41</td>\n",
       "      <td>853.843337</td>\n",
       "      <td>41.853523</td>\n",
       "      <td>60</td>\n",
       "      <td>400</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>948.379948</td>\n",
       "      <td>47.327118</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>49</td>\n",
       "      <td>1551.788603</td>\n",
       "      <td>533.228746</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>41</td>\n",
       "      <td>2077.895289</td>\n",
       "      <td>1304.564003</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23</td>\n",
       "      <td>3315.428006</td>\n",
       "      <td>3228.822711</td>\n",
       "      <td>120</td>\n",
       "      <td>400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>30</td>\n",
       "      <td>3499.739071</td>\n",
       "      <td>105.530500</td>\n",
       "      <td>90</td>\n",
       "      <td>400</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22</td>\n",
       "      <td>4039.572376</td>\n",
       "      <td>3367.972873</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>4050.001853</td>\n",
       "      <td>3577.658665</td>\n",
       "      <td>120</td>\n",
       "      <td>400</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36</td>\n",
       "      <td>4107.523556</td>\n",
       "      <td>4184.789074</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>22</td>\n",
       "      <td>4672.515149</td>\n",
       "      <td>3527.297801</td>\n",
       "      <td>120</td>\n",
       "      <td>400</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>23</td>\n",
       "      <td>4743.949617</td>\n",
       "      <td>4797.294667</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>5359.132035</td>\n",
       "      <td>5701.161595</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24</td>\n",
       "      <td>6495.013576</td>\n",
       "      <td>5805.891880</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23</td>\n",
       "      <td>7163.465898</td>\n",
       "      <td>4920.800506</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs     val_loss         loss  LSTM_dropout  LSTM_n  batch_size\n",
       "1            100    10.820394    19.600999            60     100         0.0\n",
       "13           100    12.805731    37.152377            60     100         0.1\n",
       "7             98    15.786570    14.420242           120     200         0.0\n",
       "29           100    34.656778    41.152965            60     200         0.2\n",
       "2            100    42.784797    16.502601            90     100         0.0\n",
       "0             50    50.788145    35.075523            30     100         0.0\n",
       "16            87    51.667780    62.586626            30     200         0.1\n",
       "28            84    60.307169    66.205071            30     200         0.2\n",
       "6            100    68.828456    18.782989            90     200         0.0\n",
       "25            86    93.938887    66.418342            60     100         0.2\n",
       "34           100   125.952476    37.040327            90     400         0.2\n",
       "24            50   151.524533    80.164311            30     100         0.2\n",
       "17            92   166.883685    38.997273            60     200         0.1\n",
       "8             30   182.144088    42.228314            30     400         0.0\n",
       "12            42   226.292798    58.064071            30     100         0.1\n",
       "9             36   248.536357    28.911466            60     400         0.0\n",
       "21            58   263.759974    36.800374            60     400         0.1\n",
       "32            50   304.024293    57.723601            30     400         0.2\n",
       "20            61   329.651958    50.158120            30     400         0.1\n",
       "5             84   368.471200    29.046989            60     200         0.0\n",
       "30            42   523.052478   133.889195            90     200         0.2\n",
       "10            50   760.476990    26.662885            90     400         0.0\n",
       "33            41   853.843337    41.853523            60     400         0.2\n",
       "4             30   948.379948    47.327118            30     200         0.0\n",
       "26            49  1551.788603   533.228746            90     100         0.2\n",
       "19            41  2077.895289  1304.564003           120     200         0.1\n",
       "11            23  3315.428006  3228.822711           120     400         0.0\n",
       "22            30  3499.739071   105.530500            90     400         0.1\n",
       "18            22  4039.572376  3367.972873            90     200         0.1\n",
       "23            22  4050.001853  3577.658665           120     400         0.1\n",
       "15            36  4107.523556  4184.789074           120     100         0.1\n",
       "35            22  4672.515149  3527.297801           120     400         0.2\n",
       "14            23  4743.949617  4797.294667            90     100         0.1\n",
       "3             25  5359.132035  5701.161595           120     100         0.0\n",
       "27            24  6495.013576  5805.891880           120     100         0.2\n",
       "31            23  7163.465898  4920.800506           120     200         0.2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_object = talos.Analyze(scan_object)\n",
    "df = analyze_object.data\n",
    "df.sort_values(by=['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "* The top 10 results are interesting. They all feature:\n",
    "  * 100 or 200 as LSTM layer size\n",
    "  * "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
